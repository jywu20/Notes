\documentclass[hyperref, a4paper]{article}

\usepackage{geometry}
\usepackage{titling}
\usepackage{titlesec}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{footnote}
\usepackage{enumerate}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathtools}
\usepackage{bbm}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{physics}
\usepackage{tensor}
\usepackage{siunitx}
\usepackage[version=4]{mhchem}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{autobreak}
\usepackage[ruled, vlined, linesnumbered]{algorithm2e}
\usepackage{nameref,zref-xr}
\zxrsetup{toltxlabel}
\usepackage[colorlinks,unicode]{hyperref} % , linkcolor=black, anchorcolor=black, citecolor=black, urlcolor=black, filecolor=black
\usepackage[most]{tcolorbox}
\usepackage{prettyref}

% Page style
\geometry{left=3.18cm,right=3.18cm,top=2.54cm,bottom=2.54cm}
\titlespacing{\paragraph}{0pt}{1pt}{10pt}[20pt]
\setlength{\droptitle}{-5em}

% More compact lists 
\setlist[itemize]{
    itemindent=17pt, 
    leftmargin=1pt,
    listparindent=\parindent,
    parsep=0pt,
}

% Math operators
\DeclareMathOperator{\timeorder}{\mathcal{T}}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\legpoly}{P}
\DeclareMathOperator{\primevalue}{P}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\res}{Res}
\newcommand*{\ii}{\mathrm{i}}
\newcommand*{\ee}{\mathrm{e}}
\newcommand*{\const}{\mathrm{const}}
\newcommand*{\suchthat}{\quad \text{s.t.} \quad}
\newcommand*{\argmin}{\arg\min}
\newcommand*{\argmax}{\arg\max}
\newcommand*{\normalorder}[1]{: #1 :}
\newcommand*{\pair}[1]{\langle #1 \rangle}
\newcommand*{\fd}[1]{\mathcal{D} #1}
\DeclareMathOperator{\bigO}{\mathcal{O}}

% TikZ setting
\usetikzlibrary{arrows,shapes,positioning}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{decorations.markings}
\tikzstyle arrowstyle=[scale=1]
\tikzstyle directed=[postaction={decorate,decoration={markings,
    mark=at position .5 with {\arrow[arrowstyle]{stealth}}}}]
\tikzstyle ray=[directed, thick]
\tikzstyle dot=[anchor=base,fill,circle,inner sep=1pt]

% Algorithm setting
% Julia-style code
\SetKwIF{If}{ElseIf}{Else}{if}{}{elseif}{else}{end}
\SetKwFor{For}{for}{}{end}
\SetKwFor{While}{while}{}{end}
\SetKwProg{Function}{function}{}{end}
\SetArgSty{textnormal}

\newcommand*{\concept}[1]{{\textbf{#1}}}

% Embedded codes
\lstset{basicstyle=\ttfamily,
  showstringspaces=false,
  commentstyle=\color{gray},
  keywordstyle=\color{blue}
}

% Reference formatting
\newrefformat{fig}{Figure~\ref{#1}}
\newrefformat{tbl}{Table~\ref{#1}}

% Color boxes
\tcbuselibrary{skins, breakable, theorems}
\newtcbtheorem[number within=section]{warning}{Warning}%
  {colback=orange!5,colframe=orange!65,fonttitle=\bfseries, breakable}{warn}
\newtcbtheorem[number within=section]{note}{Note}%
  {colback=green!5,colframe=green!65,fonttitle=\bfseries, breakable}{note}
\newtcbtheorem[number within=section]{info}{Info}%
  {colback=blue!5,colframe=blue!65,fonttitle=\bfseries, breakable}{info}

\newenvironment{shelldisplay}{\begin{lstlisting}}{\end{lstlisting}}

\title{Homework 4}
\author{Jinyuan Wu}

\begin{document}

\maketitle

\paragraph{Exercise 3 in lecture 12} 

\paragraph{Solution} The normalization factor in the time domain is 
\begin{equation}
    \int_{-\infty}^\infty \abs{g(t)}^2 \dd{t} = \sigma \sqrt{\pi}.
\end{equation}
The expectation values are 
\begin{equation}
    \expval{t^2} = \frac{\int_{-\infty}^\infty \dd{t} \abs{g(t)}^2 t^2 }{\int_{-\infty}^\infty \dd{t} \abs{g(t)}^2}
    = t_0^2 + \frac{1}{2} \sigma^2,
\end{equation}
and 
\begin{equation}
    \expval{t} = \frac{\int_{-\infty}^\infty \dd{t} \abs{g(t)}^2 t }{\int_{-\infty}^\infty \dd{t} \abs{g(t)}^2}
    = t_0.
\end{equation}
The frequency domain version of $g$ is 
\begin{equation}
    g[\omega] = \int_{-\infty}^\infty \ee^{\ii \omega t} g(t) 
    = \sigma \sqrt{2\pi} \ee^{\ii (\omega - \omega_0) t - \frac{1}{2} \sigma^2 (\omega - \omega_0)^2}.
\end{equation}
The normalization factor is 
\begin{equation}
    \int_{-\infty}^\infty \abs{g[\omega]}^2 \dd{\omega} 
    = 2 \pi \sigma^2 \cdot \sqrt{\frac{\pi}{\sigma^2}} = 2 \pi^{3/2} \sigma.
\end{equation}
The expectation values are 
\begin{equation}
    \expval{\omega^2} = \frac{
        \int_{-\infty}^\infty \dd{\omega} \abs{g[\omega]}^2 \omega^2
    }{
        \int_{-\infty}^\infty \dd{\omega} \abs{g[\omega]}^2
    } = \frac{1}{2 \sigma^2} + \omega_0^2,
\end{equation}
and 
\begin{equation}
    \expval{\omega} = \omega_0.
\end{equation}
So 
\begin{equation}
    \begin{aligned}
        \sigma_t \sigma_\omega &= 
        \sqrt{ \expval{t^2} - \expval{t}^2 } \sqrt{ \expval{\omega^2} - \expval{\omega}^2 } \\
        &= \sqrt{ \frac{1}{2} \sigma^2 \cdot \frac{1}{2 \sigma^2} } = \frac{1}{2}.
    \end{aligned}
\end{equation}
So indeed for $g(t)$,
$\sigma_t \sigma_\omega$ reaches its minimum value.

\paragraph{Exercise 1 in lecture 13} We have the following transmission model: the sent signal $X$ has 3 possible values $x=0, \pm 1$ with $p_0=p_1=p_{-1}=1 / 3$. The noise $\Xi$ has two possible values $\zeta=\pm 1$. We suppose the channel function $F$ has the following expression $Y=\operatorname{Im}\left[\mathrm{e}^{i 2 \pi(X+\Xi) / }\right]$. What are all the possible values of $Y$ ? Give the expression of the distributions $w(x), w(y), w(x, y)$ and $w(y \mid x)$.

\paragraph{Solution} The possible results are listed in \prettyref{tbl:prob-y}.
The possible values of $Y$ are $0$ and $\pm \sqrt{3}/ 2$.
From \prettyref{tbl:prob-y} we have
\begin{equation}
    w(x) = \begin{cases}
        \frac{1}{3}, \quad x = 0, \\
        \frac{1}{3}, \quad x = 1, \\
        \frac{1}{3}, \quad x = -1,
    \end{cases}
\end{equation}
\begin{equation}
    w(y) = \begin{cases}
        \frac{1}{3}, \quad y = 0, \\
        \frac{1}{3}, \quad y = \sqrt{3} / 2, \\
        \frac{1}{3}, \quad y = - \sqrt{3} / 2,
    \end{cases}
\end{equation}
and 
\begin{equation}
    w(x , y) = \begin{cases}
        \frac{1}{6}, \quad (x, y) = (0, \sqrt{3} /2), \\
        \frac{1}{6}, \quad (x, y) = (0, - \sqrt{3} / 2), \\
        \frac{1}{6}, \quad (x, y) = (1, - \sqrt{3} / 2), \\
        \frac{1}{6}, \quad (x, y) = (1, 0), \\
        \frac{1}{6}, \quad (x, y) = (-1, 0), \\
        \frac{1}{6}, \quad (x, y) = (-1, \sqrt{3}/2), \\
        0, \quad \text{otherwise}.
    \end{cases}
\end{equation}
So 
\begin{equation}
    w(y | x) = \frac{w(x, y)}{w(x)} = 
    \begin{cases}
        \frac{1}{2}, \quad (x, y) = (0, \sqrt{3} /2), \\
        \frac{1}{2}, \quad (x, y) = (0, - \sqrt{3} / 2), \\
        \frac{1}{2}, \quad (x, y) = (1, - \sqrt{3} / 2), \\
        \frac{1}{2}, \quad (x, y) = (1, 0), \\
        \frac{1}{2}, \quad (x, y) = (-1, 0), \\
        \frac{1}{2}, \quad (x, y) = (-1, \sqrt{3}/2), \\
        0, \quad \text{otherwise}.
    \end{cases}
\end{equation}

\begin{table}
    \centering
    \caption{Probabilistic distribution of $Y$; the probability of each row is $1/6$.}
    \label{tbl:prob-y}

    \begin{tabular}{ccc}
    \toprule
    $X$                 & $\Xi$ & $Y = \Im \ee^{\ii 2 \pi(X+\Xi) / 3}$ \\ \midrule
    \multirow{2}{*}{0}  & 1     & $\frac{\sqrt{3}}{2}$                 \\
                        & -1    & $-\frac{\sqrt{3}}{2}$                \\ \midrule
    \multirow{2}{*}{1}  & 1     & $-\frac{\sqrt{3}}{2}$                \\
                        & -1    & $0$                                  \\ \midrule
    \multirow{2}{*}{-1} & 1     & $0$                                  \\
                        & -1    & $\frac{\sqrt{3}}{2}$                 \\ \bottomrule 
    \end{tabular}
\end{table}

\paragraph{Exercise 2 in lecture 14}

\paragraph{Solution} Suppose $p$ is the probability of $X = -1$,
and we have 
\begin{equation}
    m = \expval{X} = - p + (1 - p) = 1 - 2p, \quad -1 \leq m \leq 1,
\end{equation}
and therefore 
\begin{equation}
    p = \frac{1 - m}{2}, 
\end{equation}
and 
\begin{equation}
    \begin{aligned}
        H_\alpha(X) &= \frac{1}{1 - \alpha} \log_2(p^{\alpha} + (1 - p)^\alpha) \\
        &= \frac{1}{1 - \alpha} \log_2\left(
            \left( \frac{1 - m}{2} \right)^{\alpha}
            + \left( \frac{1 + m}{2} \right)^{\alpha}
        \right).
    \end{aligned}
\end{equation}
The plots can be found in \prettyref{fig:entropy-plot}.

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{plots/ising-entroy-alpha.pdf}
    \caption{Plots of $H_\alpha(X)$ with different $\alpha$'s}
    \label{fig:entropy-plot}
\end{figure}

\paragraph{Exercise 4 in lecture 14}

\paragraph{Solution} We have 
\begin{equation}
    \begin{aligned}
        H(Y | X) &= \sum_x H(Y, X = x) p(x) \\
        &= - \sum_x p(x) \sum_y p(y | x) \log_2 p(y | x) \\
        &= - \sum_{x, y} p(x, y) \log_{2} p(y | x),
    \end{aligned}
\end{equation}
and a direct verification gives 
\begin{equation}
    \begin{aligned}
        H(X, Y) &= - \sum_{x, y} p(x, y) \log_{2} \underbrace{p(x , y)}_{p(x) p(y | x)} \\
        &= - \sum_{x, y} p(x, y) (\log_2 p(x) + \log_2 p(y | x)) \\
        &= - \sum_{x} p(x) \log_2 p(x) - \sum_{x, y} p(x, y) \log_2 p(y | x) \\
        &= H(X) + H(Y | X).
    \end{aligned}
\end{equation}

\paragraph{Problem} Differences between the classical and quantum entropies 

\paragraph{Solution} \begin{itemize}
\item[(a)] This can be done by Schmidt decomposition. 
Suppose $\dim \mathcal{H}_{\text{A}} = m$, 
$\dim \mathcal{H}_{\text{B}} = n$,
and without loss of generality $m \geq n$,
then we can already find a set of basis, in which
\begin{equation}
    \ket{\psi_{\text{AB}}} = \sum_{i = 1}^m c_{i} \ket*{u^i_{\text{A}}} \otimes \ket*{v^i_{\text{B}}}.
    \label{eq:schmidt}
\end{equation}
We have 
\begin{equation}
    \begin{aligned}
        \rho_\text{A} &= \sum_{j=1}^m 
        \braket*{v_\text{B}^j}{\psi_{\text{AB}}}
        \braket*{\psi_{\text{AB}}}{v_\text{B}^j} \\
        &= \sum_{j=1}^m c_{j} \ket*{u^j_{\text{A}}} c_{j}^* \bra*{u^{j}_{\text{A}}} \\
        &= \sum_{j=1}^m \abs{c_j}^2 \dyad*{u^j_{\text{A}}}{u^{j}_{\text{A}}},
    \end{aligned}
\end{equation}
and similarly (note that $\{\ket*{v^j_{\text{B}}}\}_{j=1}^m$ 
appearing in the expansion of $\ket{\psi_{\text{AB}}}$ is over-complete,
and the form of $\rho_{\text{B}}$ is exactly the same as $\rho_{\text{A}}$)
\begin{equation}
    \rho_\text{B} = \sum_{i=1}^m \abs{c_{i}}^2 \dyad*{v_{\text{B}}^i}{v_{\text{B}}^{i}}.
\end{equation}
So we find 
\begin{equation}
    S(\rho_\text{A}) = S(\rho_{\text{B}}) = - \sum_{j=1}^m \abs{c_{i}}^2 \ln \abs{c_{i}}^2.
\end{equation}
Note that here 
\begin{equation}
    S(\rho_{\text{AB}}) = 0,
\end{equation}
because we are dealing with a pure state.
So here the entropy of a system is less than any of its parts,
and this is not possible in the classical case.

\item[(b)] Let 
\[
    \rho = \rho_{\text{AB}}, \quad \sigma = \rho_{\text{A}} \otimes \rho_{\text{B}},
\]
and from 
\[
    \trace (\rho \ln \rho ) - \trace (\rho \ln \sigma) \geq 0,
\]
we have 
\[
    \begin{aligned}
        0 &\leq \trace (\rho_{\text{AB}} \ln \rho_{\text{AB}} ) 
        - \trace (\rho_{\text{AB}} 
        \underbrace{\ln \rho_{\text{A}} \otimes \rho_{\text{B}}}_{\ln \rho_{\text{A}} + \ln \rho_{\text{B}}}) \\
        &= - S(\rho_{\text{AB}})
        - \trace_{\text{A}} \trace_{\text{B}} \rho_{\text{AB}} \ln \rho_{\text{A}}
        - \trace_{\text{B}} \trace_{\text{A}} \rho_{\text{AB}} \ln \rho_{\text{B}} \\
        &= - S(\rho_{\text{AB}}) + S(\rho_{\text{A}}) + S(\rho_{\text{B}}),
    \end{aligned}
\]
and thus 
\begin{equation}
    S(\rho_{\text{A}}) + S(\rho_{\text{B}}) - S(\rho_{\text{AB}}) \geq 0.
    \label{eq:ineq1}
\end{equation}

\item[(c)] What we need to prove 
\begin{equation}
    S(\rho_{\text{AB}}) \geq S(\rho_{\text{B}}) - S(\rho_{\text{A}}).
\end{equation}
Assume the density matrix $\rho_{\text{AB}}$ is actually 
the partial trace of the pure state $\ket{\Psi_{\text{ABC}}}$.
\eqref{eq:ineq1} means 
\[
    S(\rho_{\text{B}}) + S(\rho_{\text{C}}) \geq S(\rho_{\text{BC}}) ,
\]
and from (a), we have
\[
    S(\rho_{\text{C}}) = S(\rho_{\text{AB}}), \quad 
    S(\rho_{\text{BC}}) = S(\rho_{\text{A}}),
\]
and therefore 
\[
    S(\rho_{\text{B}}) + S(\rho_{\text{AB}}) \geq S(\rho_{\text{A}}),
\]
\begin{equation}
    S(\rho_{\text{AB}}) \geq S(\rho_{\text{A}}) - S(\rho_{\text{B}}).
\end{equation}
Similarly 
\begin{equation}
    S(\rho_{\text{AB}}) \geq S(\rho_{\text{B}}) - S(\rho_{\text{A}}).
\end{equation}

\item[(d)] We have
\begin{equation}
    p_j = \expval{\rho_{\text{B}}}{j},
\end{equation}
and therefore 
\begin{equation}
    \rho_{\text{AB}|\Pi^j_{\text{B}}} = 
    \frac{1}{p_j} \trace_{\text{B}} \dyad{j} \expval{\rho_{\text{AB}}}{j}
    = \frac{1}{p_j} \expval{\rho_{\text{AB}}}{j}.
\end{equation}
So 
\begin{equation}
    \begin{aligned}
        \sum_j p_j S\left( \rho_{\text{AB}|\Pi^j_{\text{B}}} \right)
        &= - \trace_{\text{A}} \sum_j \expval{\rho_{\text{AB}}}{j} \ln \frac{\expval{\rho_{\text{AB}}}{j}}{p_j} \\
        &= - \trace_{\text{A}} \sum_{j} \expval{\rho_{\text{AB}}}{j} \ln \expval{\rho_{\text{AB}}}{j} 
        + \sum_j \expval{\rho_{\text{B}}}{j} \ln \expval{\rho_{\text{B}}}{j} \\
        &= S(\rho_{\text{AB}}^{\text{D}}) - S(\rho_{\text{B}}^{\text{D}}).
    \end{aligned}
    \label{eq:ineq-2}
\end{equation}
The inequality to be proven is equivalent to 
\[
    S(\rho_{\text{A}}) - S(\rho_{\text{A}} | \rho_{\text{B}}) \leq S(\rho_{\text{A}}) - H(\rho_{\text{A}} | \rho_{\text{B}}), 
\]
which further is equivalent to 
\begin{equation}
    S(\rho_{\text{A}} | \rho_{\text{B}}) 
    = \min_{\{\Pi^j_{\text{B}}\}} \sum_j p_j S\left( \rho_{\text{AB}|\Pi^j_{\text{B}}} \right)
    \geq H(\rho_{\text{A}} | \rho_{\text{B}}).
\end{equation}
\eqref{eq:ineq-2}  and 
\begin{equation}
    S(\rho_{\text{AB}}^{\text{D}}) - S(\rho_{\text{B}}^{\text{D}}) \geq H(\rho_{\text{A}} | \rho_{\text{B}})
\end{equation}
show that for any $\{\Pi^j_{\text{B}}\}$,
the inequality is correct,
so we have already finished the proof.

\item[(e)] The disagreements in the definition of conditional entropy 
intuitively arises from the entanglement between the two systems,
so we expect when 
\begin{equation}
    \rho_{\text{AB}} = \rho_{\text{A}} \otimes \rho_{\text{B}},
    \label{eq:no-entanglement}
\end{equation}
there is no inconsistency in the definition of conditional entropy,
which is the classical case.
This can be verified directly, because when \eqref{eq:no-entanglement} is correct,
we have 
\[
    \rho^\text{D}_{\text{AB}} = \rho_{\text{AB}}, \quad 
    \rho^\text{D}_{\text{B}} = \rho_{\text{B}},
\]
and therefore according to \eqref{eq:ineq-2}, 
we have 
\begin{equation}
    S(\rho_{\text{A}} | \rho_{\text{B}}) = S(\rho_{\text{AB}}) - S(\rho_{\text{B}}),
\end{equation}
and 
\begin{equation}
    J(\rho_{\text{A}} : \rho_{\text{B}}) = 
    S(\rho_{\text{A}}) + S(\rho_{\text{B}}) - S(\rho_{\text{AB}}) 
    \eqqcolon I(\rho_{\text{A}} : \rho_{\text{B}}).
\end{equation}

\item[(f)] In a pure state, we have 
\begin{equation}
    I(\rho_{\text{A}} | \rho_{\text{B}}) = S(\rho_{\text{A}}) + S_{\rho_{\text{B}}} - S_{\rho_{\text{AB}}}
    = 2 S(\rho_{\text{A}}).
\end{equation}
On the other hand, we have 
\begin{equation}
    \rho_{\text{AB} | \Pi^j_{\text{B}}} = \frac{1}{p_j} \braket*{j}{\psi_{\text{AB}}} \braket*{\psi_{\text{AB}}}{j}.
\end{equation}
So now we can use Schmidt decomposition again:
We have \eqref{eq:schmidt},
and therefore (here $\ket*{j}$ means a basis vector for B, 
and $\ket*{j}_{\text{A}}$ is used to refer to a basis vector for A)
\[
    \braket*{j}{\psi_{\text{AB}}} = c_i \ket*{j}_{\text{A}},
\]
so $\rho_{\text{AB} | \Pi_\text{B}^j}$ is actually a pure state, 
and therefore we have $S(\rho_{\text{A}} | \rho_{\text{B}}) = 0$,
because each term in it is zero.
So 
\begin{equation}
    J(\rho_{\text{A}} : \rho_{\text{B}}) = S(\rho_{\text{A}}),
\end{equation}
and thus 
\begin{equation}
    \delta(\rho_{\text{A}} : \rho_{\text{B}}) = 2 S(\rho_{\text{A}}) - S(\rho_{\text{A}}) = 
    S(\rho_{\text{A}}) = S(\rho_{\text{B}}).
\end{equation}

\end{itemize}

\end{document}