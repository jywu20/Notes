\documentclass[hyperref, a4paper]{article}

\usepackage{geometry}
\usepackage{titling}
\usepackage{titlesec}
% No longer needed, since we will use enumitem package
% \usepackage{paralist}
\usepackage{enumitem}
\usepackage{footnote}
\usepackage{enumerate}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathtools}
\usepackage{bbm}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{physics}
\usepackage{tensor}
\usepackage{siunitx}
\usepackage[version=4]{mhchem}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{autobreak}
\usepackage[ruled, vlined, linesnumbered]{algorithm2e}
\usepackage{nameref,zref-xr}
\zxrsetup{toltxlabel}
\usepackage[colorlinks,unicode]{hyperref} % , linkcolor=black, anchorcolor=black, citecolor=black, urlcolor=black, filecolor=black
\usepackage[most]{tcolorbox}
\usepackage{prettyref}

% Page style
\geometry{left=3.18cm,right=3.18cm,top=2.54cm,bottom=2.54cm}
\titlespacing{\paragraph}{0pt}{1pt}{10pt}[20pt]
\setlength{\droptitle}{-5em}
\preauthor{\vspace{-10pt}\begin{center}}
\postauthor{\par\end{center}}

% More compact lists 
\setlist[itemize]{
    itemindent=17pt, 
    leftmargin=1pt,
    listparindent=\parindent,
    parsep=0pt,
}

% Math operators
\DeclareMathOperator{\timeorder}{\mathcal{T}}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\legpoly}{P}
\DeclareMathOperator{\primevalue}{P}
\DeclareMathOperator{\sgn}{sgn}
\newcommand*{\ii}{\mathrm{i}}
\newcommand*{\ee}{\mathrm{e}}
\newcommand*{\const}{\mathrm{const}}
\newcommand*{\suchthat}{\quad \text{s.t.} \quad}
\newcommand*{\argmin}{\arg\min}
\newcommand*{\argmax}{\arg\max}
\newcommand*{\normalorder}[1]{: #1 :}
\newcommand*{\pair}[1]{\langle #1 \rangle}
\newcommand*{\fd}[1]{\mathcal{D} #1}
\DeclareMathOperator{\bigO}{\mathcal{O}}

% TikZ setting
\usetikzlibrary{arrows,shapes,positioning}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{decorations.markings}
\tikzstyle arrowstyle=[scale=1]
\tikzstyle directed=[postaction={decorate,decoration={markings,
    mark=at position .5 with {\arrow[arrowstyle]{stealth}}}}]
\tikzstyle ray=[directed, thick]
\tikzstyle dot=[anchor=base,fill,circle,inner sep=1pt]

% Algorithm setting
% Julia-style code
\SetKwIF{If}{ElseIf}{Else}{if}{}{elseif}{else}{end}
\SetKwFor{For}{for}{}{end}
\SetKwFor{While}{while}{}{end}
\SetKwProg{Function}{function}{}{end}
\SetArgSty{textnormal}

\newcommand*{\concept}[1]{{\textbf{#1}}}

% Embedded codes
\lstset{basicstyle=\ttfamily,
  showstringspaces=false,
  commentstyle=\color{gray},
  keywordstyle=\color{blue}
}

% Reference formatting
\newrefformat{fig}{Figure~\ref{#1}}

% Color boxes
\tcbuselibrary{skins, breakable, theorems}
\newtcbtheorem[number within=section]{warning}{Warning}%
  {colback=orange!5,colframe=orange!65,fonttitle=\bfseries, breakable}{warn}
\newtcbtheorem[number within=section]{note}{Note}%
  {colback=green!5,colframe=green!65,fonttitle=\bfseries, breakable}{note}
\newtcbtheorem[number within=section]{info}{Info}%
  {colback=blue!5,colframe=blue!65,fonttitle=\bfseries, breakable}{info}

\newenvironment{shelldisplay}{\begin{lstlisting}}{\end{lstlisting}}

\title{Homework 1}
\author{Jinyuan Wu}

\begin{document}

\maketitle

\paragraph{Exercise 9 in chapter 1} (**) Let us show now one simple way to produce the realizations of $B_m$ knowing the realizations of one example of $B_{1 / 2}$. From the set of the realizations of $B_{1 / 2}$, which we can view as a real number $r \in[0,1[$ by forming the binary digit number $0.\beta^{(1)} \beta^{(2)} \beta^{(3)} \beta^{(4)} \beta^{(5)} \ldots$ (example: $0.011010110001111 \ldots$ ), we can obtain the real numbers $r^{(1)}, r^{(2)}, r^{(3)}$ etc... from the formula:
$$
r^{(n)}=\left(2^n r\right) \bmod 1
$$
The realisations $\beta_m^{(n)}$ will be obtained from $r^{(n)}$ by the expression
$$
\begin{aligned}
&\beta_m^{(n)}=0 \text { if } r^{(n)} \geqslant m \\
&\beta_m^{(n)}=1 \text { if } r^{(n)}<m
\end{aligned}
$$
Prove that this last expression yields the expected properties for $B_m$.

\paragraph{Solution} Since 
\[
    r^{(n)} = \beta^{(1)} \beta^{(2)} \cdots \beta^{(n)} . \beta^{(n+1)} \beta^{(n+2)} \cdots,
\]
we know 
\begin{equation}
    r^{(n)} = 0. \beta^{(n + 1)} \beta^{(n+2)} \cdots.
\end{equation}
Each digit of $r^{(n)}$ is 0 or 1, 
and thus the possible range of $r^{(n)}$ is [0, 1].%
\footnote{
    It's actually possible to have $r^{(n)} = 1$,
    because the binary $0.11111\cdots$ is actually $1$,
    in the same way $0.9999\cdots = 1$ in the decimal case.
    But the probability to have such a $r^{(n)}$ is $1/2 \times 1/2 \times \cdots = 0$.
    That is, the event that $r^{(n)} = 1$ is possible but is a null set.
} 
Suppose 
\[
    x = 0.x^{(1)} x^{(2)} \cdots \in [0, 1],
\]
we have 
\[
    \begin{aligned}
        P(r^{(n)} < x) &= P(\beta^{(n+1)} < x^{(1)}) + P(\beta^{(n+1)} = x^{(1)}) P(\beta^{(n+2)} < x^{(2)}) + \cdots \\
        &= \frac{1}{2} \delta_{x^{(1)}, 1} + \frac{1}{2} \times \frac{1}{2} \delta_{x^{(2)}, 1} + \cdots \\
        &= 0.x^{(1)} x^{(2)} \cdots = x,
    \end{aligned}
\]
so $r^{(n)}$ has a uniform probabilistic distribution on $[0, 1]$.
So the probability of $r^{(n)} < m$ i.e. $\beta^{(n)}_m = 1$ is exactly $m$,
and therefore $\beta^{(n)}_m$ is a realization of $B_m$, regardless of what $n$ is.

\paragraph{Exercise 14 in chapter 1} (**) Explain the link between the binomial distribution and the expansion of $(a+b)^N$.

\paragraph{Solution} The binomial distribution can be derived by an intermediate step used
to derive the expansion of $(a + b)^N$.

The binomial coefficient $\binom{N}{n}$ 
gives the number of ways to pick $n$ points in $N$ different points.
Without invoking the commutative property of multiplication,
there are $2^N$ terms in the expansion of $(a + b)^N$,
each of which is like 
\[
    aabbabba \cdots.
\]
Now by the definition of the binomial coefficient,
there are $\binom{N}{n}$ terms that have $n$ $a$'s and $(N-n)$ $b$'s.

From this conclusion we can derive the expansion of $(a + b)^N$:
there are $\binom{N}{n}$ terms in the total $2^N$ terms which has $n$ $a$'s and $(N-n)$ $b$'s,
and we have 
\begin{equation}
    (a + b)^N = \sum_{n=0}^N \binom{N}{n} a^n b^{N-n}.
\end{equation}
Similarly, if we consider the probabilistic distribution of 
\begin{equation}
    X_{m, N} = \sum_{k=1}^N B_{m, k},
\end{equation}
we will find the probability of the event that $X_{m, N} = x$ is the sum of the probability of 
all outputs of $\{B_{m, k}\}$ in which there are $x$ 1 outputs and $N - x$ 0 outputs,
and for each possible output, 
the probability is 
\[
    p(1)^x p(0)^{N-x} = m^x (1 - m)^{N-x},
\]
and we have 
\begin{equation}
    p_{m, N} (x) = \binom{N}{x} m^x (1 - m)^{N-x}.
    \label{eq:binominal}
\end{equation}

So the relation between the binomial distribution and the $(a + b)^N$ expansion is 
they both involve the notion of ``picking $x$ points from $N$ points''.
Indeed, by considering the normalization condition of \eqref{eq:binominal},
which is 
\begin{equation}
    1 = \sum_{x} p_{m, N}(x) = \sum_{x = 0}^N \binom{N}{x} m^x (1 - m)^{N-x},
\end{equation}
we rediscover the expansion of $(a + b)^N$,
where we set $a = m$ and $b = 1 - m$.

\paragraph{Exercise 3 in chapter 2} (**)  (a) Show that the above expression (2.15) for $w(x, t)$ with $t>0$ satisfies this equation. (b) By using a double Fourier transform in $x$ and t show that the Green's function of the Smoluchowsky equation (2.26) is indeed the above expression (2.15) for $w(x, t)$ with $t \geq 0$.

\paragraph{Solution} \begin{itemize}
\item[(a)] From (2.15) we have 
\[
    \begin{aligned}
        \pdv{t} w(x, t) &= 
        - \frac{1}{2} \sqrt{\frac{1}{4 \pi D t^3}} \ee^{- \frac{(x - v_d t)^2}{4 D t}} 
        - \sqrt{\frac{1}{4 \pi D t}} \ee^{- \frac{(x - v_d t)^2}{4 D t}} \frac{1}{4 D t^2} 
        (2 v_d (v_d t - x) t - (x- v_d t)^2 ) \\
        &= - \sqrt{\frac{1}{4 \pi D t}} \ee^{- \frac{(x - v_d t)^2}{4 D t}} 
        \left( \frac{1}{2 t} + \frac{(v_d t - x) (v_d t + x)}{4 D t^2} \right),
    \end{aligned}
\]
\[
    \begin{aligned}
        \pdv{x}w(x, t) &= - \sqrt{\frac{1}{4 \pi D t}} \ee^{- \frac{(x - v_d t)^2}{4 D t}} 
        \frac{x - v_d t}{2 D t},
    \end{aligned}
\]
and 
\[
    \begin{aligned}
        \pdv[2]{x}w(x, t) &= - \sqrt{\frac{1}{4 \pi D t}} \ee^{- \frac{(x - v_d t)^2}{4 D t}} 
        \left(
            \frac{1}{2 D t} - \left( \frac{x - v_d t}{2 D t} \right)^2
        \right),
    \end{aligned}
\]
The RHS of the Smoluchowski equation is 
\[
    \begin{aligned}
        D \pdv[2]{w}{x} - v_d \pdv{w}{x} &= - \sqrt{\frac{1}{4 \pi D t}} \ee^{- \frac{(x - v_d t)^2}{4 D t}} 
        \left(
            \frac{1}{2 t} - \frac{(x - v_d t)^2}{4 D t^2} - v_d \frac{x - v_d t}{2 D t}
        \right) \\
        &= - \sqrt{\frac{1}{4 \pi D t}} \ee^{- \frac{(x - v_d t)^2}{4 D t}} 
        \left(
            \frac{1}{2 t} - \frac{x^2 - v_d^2 t^2}{4 D t^2}
        \right),
    \end{aligned}
\]
so we have 
\[
    \pdv{x}w(x, t) = D \pdv[2]{w}{x} - v_d \pdv{w}{x} .
\]

\item[(b)] The initial condition is 
\[
    \lim_{t \to 0} w = \delta(x),
\]
which can be imposed to (2.26) by adding an ``impact'':
\begin{equation}
    \pdv{w}{t} = D \pdv[2]{w}{x} - v_d \pdv{w}{x} + \delta(x) \delta(t).
\end{equation}
Now by Fourier transformation we have 
\[
    \begin{aligned}
        w(x, t) &= \int \frac{\dd{k} \dd{\omega}}{(2\pi)^2} \ee^{- \ii (\omega t - k x)} \tilde{w}(k, \omega),  \\
        - \ii \omega \tilde{w} &= D (\ii k)^2 \tilde{w} - \ii k v_d \tilde{w}  + 1.
    \end{aligned}
\]
We find 
\[
    \tilde{w} = \frac{1}{- \ii \omega + k^2 D + \ii k v_d},
\]
and thus 
\[
    w(x, t) = \int \frac{\dd{k} \dd{\omega}}{(2\pi)^2} \ee^{- \ii (\omega t - k x)}  \frac{1}{- \ii \omega + k^2 D + \ii k v_d}.
\]
We first complete the integral over $\omega$, with the following contour:
\[
    \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
        %uncomment if require: \path (0,300); %set diagram left start at 0, and has height of 300
        
        %Straight Lines [id:da9297518667832358] 
        \draw    (139.5,107) -- (397.5,107) ;
        \draw [shift={(399.5,107)}, rotate = 180] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (12,-3) -- (0,0) -- (12,3) -- cycle    ;
        %Straight Lines [id:da9150191168439286] 
        \draw    (206.75,217.85) -- (206.75,80.43) ;
        \draw [shift={(206.75,78.43)}, rotate = 90] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (12,-3) -- (0,0) -- (12,3) -- cycle    ;
        %Straight Lines [id:da9914740440795959] 
        \draw    (241,147) ;
        \draw [shift={(241,147)}, rotate = 0] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]      (0, 0) circle [x radius= 3.35, y radius= 3.35]   ;
        %Straight Lines [id:da5103977712607115] 
        \draw  [dash pattern={on 4.5pt off 4.5pt}]  (142,117) -- (393.5,117) ;
        %Shape: Arc [id:dp04340597399658641] 
        \draw  [draw opacity=0][dash pattern={on 4.5pt off 4.5pt}] (393.46,114.38) .. controls (393.49,115.38) and (393.51,116.39) .. (393.5,117.4) .. controls (393.01,171.72) and (336.32,215.25) .. (266.87,214.63) .. controls (198.23,214.02) and (142.81,170.51) .. (142,117.05) -- (267.75,116.28) -- cycle ; \draw  [dash pattern={on 4.5pt off 4.5pt}] (393.46,114.38) .. controls (393.49,115.38) and (393.51,116.39) .. (393.5,117.4) .. controls (393.01,171.72) and (336.32,215.25) .. (266.87,214.63) .. controls (198.23,214.02) and (142.81,170.51) .. (142,117.05) ;  
        
        % Text Node
        \draw (250,144.4) node [anchor=north west][inner sep=0.75pt]    {$-\ii k^{2} D+kv_{d}$};
        
        
        \end{tikzpicture}    
\]
\[
    \begin{aligned}
        \int \dd{\omega} \ee^{- \ii (\omega t - k x)} \frac{1}{\omega + \ii D k^2 - k v_d} = - 2\pi \ii \ee^{- \ii ( - \ii k^2 D t + k v_d t - kx )}.
    \end{aligned}
\]
Thus 
\[
    \begin{aligned}
        w(x, t) &= \frac{\ii}{(2\pi)^2} \int \dd{k} (- 2\pi \ii) \ee^{- \ii ( - \ii k^2 D t + k v_d t - kx )} \\
        &= \frac{1}{2\pi} \int \dd{k} \ee^{- k^2 D t - \ii k (v_d t - x)} \\
        &= \frac{1}{2\pi} \cdot \sqrt{\frac{2\pi}{2 D t}} \ee^{\frac{1}{2} \frac{1}{2 D t} (- \ii (v_d t - x))^2} \\
        &= \sqrt{\frac{1}{4 \pi D t}} \ee^{- \frac{(x - v_d t)^2}{4 D t}} .
    \end{aligned}
\]
This is exactly (2.15).

\end{itemize}

\paragraph{Exercise 5 in chapter 2} (**) Explain in detail how, by measuring for the first time the position diffusion constant of a small Brownian sphere immerged in water, the physicist Jean Perrin, using the Einstein relation, was able to measure Avogadro's Number $N_A$, thereby confirming the existence of atoms (Jean Perrin received the Nobel prize for this work in 1926, see his Nobel lecture on the Nobel website). Use Stokes' law stating that a sphere of radius $R$ moving at a velocity $V$ feels in a fluid with viscosity $\eta$ a frictional force
$$
F=6 \pi R \eta V
$$
Remember that Avogadro's Number $N_A$ is involved in the ideal gas constant, defined by the relation
$$
\frac{\text { pressure } \text { volume }}{\text { temperature }}=n R_{\mathrm{ig}}
$$
where $n$ is the number of moles of the volume of gas considered. In the kinetic theory of gases, $R_{\mathrm{ig}}$ is given by
$$
R_{\mathrm{ig}}=N_A k_B
$$

\paragraph{Solution} The Stokes' law 
\begin{equation}
    F = 6 \pi R \eta v 
\end{equation}
connects two physical quantities arising from the same dissipation process in the fluid:
the viscosity $\mu$ and the response coefficient
\begin{equation}
    \mu = \frac{v}{F}.
\end{equation}
The relation between the two is imposed by the Navier-Stokes equation.
Since we also have 
\begin{equation}
    \mu = \frac{D}{k_{\text{B}} T},
\end{equation}
we have 
\begin{equation}
    \frac{1}{6 \pi R \eta} = \frac{D}{k_{\text{B}} T}.
\end{equation}
This equation can be used to measure $k_{\text{B}}$:
each quantities involved in the equation can be measured separately.
The viscosity $\eta$ can be measured by standard fluid dynamic methods.
The radius $R$ can be measured by letting the particles fall in the fluid 
and recording its terminal velocity,
and then we have 
\begin{equation}
    R = \frac{mg}{6 \pi \eta v_{\text{terminal}}}.
\end{equation}
The diffusion coefficient $D$ can be measured by looking at the trajectory of a Brownian particle.
The temperature is measured by a thermometer.
Now we find $k_{\text{B}}$,
and by the ideal gas equation 
\begin{equation}
    p V = n R_{\text{ig}} T
\end{equation}
we can measure $R_{\text{ig}}$,
so finally, by 
\begin{equation}
    R_{\text{ig}} = N_A k_{\text{B}},
\end{equation}
the Avogadro constant is found.

\paragraph{Exercise 4 in lecture 3} Treat the case of the Shrapnell process in dimension 2.

\paragraph{Solution} Now the damage is 
\begin{equation}
    X = \frac{\Omega}{r},
\end{equation}
and the probability per unit surface is 
\[
    n = \frac{1}{\pi R^2}.
\]
The condition $X < x$ is equivalent to 
\begin{equation}
    r > \frac{\Omega}{x}.
\end{equation}
We have 
\begin{equation}
    p(r > \Omega / x) = \frac{R^2 - (\Omega / x)^2}{R^2},
\end{equation}
so the probability density is 
\begin{equation}
    w(x) = \dv{p(r > \Omega / x)}{x} = \frac{2 \Omega^2}{R^2 x^3}.
\end{equation}
There is a minimum of $X$: it's $\Omega / R$,
because explosion doesn't happen outside the circle.
Now the first momentum is 
\begin{equation}
    \expval{X} = \int^\infty_{\Omega / R} \dd{x} x \frac{2 \Omega^2}{R^2 x^3} = \frac{2 \Omega}{R} < \infty,
\end{equation}
and the second momentum is 
\begin{equation}
    \expval{X^2} = \int^\infty_{\Omega / R} \dd{x} x^2 \frac{2 \Omega^2}{R^2 x^3} = \infty.
\end{equation}
So still the high order momenta of the variable diverges,
and thus the central limit theorem fails.

\paragraph{Problem 2} 

\paragraph{Solution} \begin{itemize}
\item[(a)] For a single bit we have 
\begin{equation}
    H(\mathrm{B}_{1/2}) = - \frac{1}{2} \log_2 \frac{1}{2} - \frac{1}{2} \log_2 \frac{1}{2} = 1.
\end{equation}
For $N$ independent bits,
we have 
\begin{equation}
    H(\otimes_{j=1}^N \mathrm{B}_{1/2, j}) = - \sum_{i} \left(\frac{1}{2}\right)^N \log_2 \left(\frac{1}{2}\right)^N = - 2^N \times \frac{1}{2^N} \times (- N) = N.
\end{equation}

\item[(b)] We have 
\[
    S = - \pdv{F}{T}, \quad F = - k _{\text{B}} T \ln Z.
\]
Now with the definition of the partition function 
\begin{equation}
    Z = \sum_i \ee^{- E_i / k_{\text{B}} T},
\end{equation}
we have 
\[
    \begin{aligned}
        \pdv{T} T \ln Z &= \ln Z + \frac{T}{Z} \pdv{Z}{T} \\
        &= \ln Z + \frac{T}{Z} \sum_i \frac{E_i}{k_{\text{B}} T^2} \ee^{- E_i / k_{\text{B}} T} \\
        &= \ln Z + \frac{1}{k_{\text{B}} T} \sum_i p_i E_i.
    \end{aligned}
\]
Thus 
\begin{equation}
    S = - k_{\text{B}} \ln Z - \frac{1}{T} \sum_i p_i E_i = - k_{\text{B}} \ln Z - \frac{\expval*{E}}{T}.
\end{equation}
On the other hand, we have 
\[
    \begin{aligned}
        \sum_i p_i \ln p_i &= 
        \sum_i p_i
        \underbrace{\left( - \frac{1}{k_{\text{B}} T} \sum_i E_i - \ln Z \right)}_{\ln p_i} \\
        &= - \frac{1}{k_{\text{B}} T} \expval{E} - \ln Z,
    \end{aligned}
\]
so we get 
\begin{equation}
    S = - k_{\text{B}} \sum_i p_i \ln p_i = k_{\text{B}} \ln 2 \times H.
    \label{eq:shannon-canonical}
\end{equation}

\item[(c)] In the high temperature limit $E_i / k_{\text{B}} T \to 0$ for every $E_{i}$,
so energy is no longer important in determining the probabilistic distribution 
and each configuration has the same probability.
The energy of $N$ indistinguishable random bits, in this case, is therefore 
\begin{equation}
    E = \sum_{j=1}^N \mathrm{B}_{1/2, j}.
\end{equation}
The probability of $E = \epsilon$ is 
\begin{equation}
    p(E = \epsilon) = \binom{N}{\epsilon} \times \frac{1}{2^N},
\end{equation}
which reaches its peak when $\epsilon = N / 2$,
so 
\begin{equation}
    \mathcal{E} = N / 2.
\end{equation}

There are $\binom{N}{\epsilon}$ microstates in the macrostate $(N ,\mathcal{C})$,
so 
\[
    \begin{aligned}
        \lim_{N \to \infty} \frac{1}{N} S(N, \mathcal{E}) &= k_{\text{B}} \frac{1}{N} \ln \binom{N}{N / 2} \\
        &= \frac{k_{\text{B}}}{N} (\ln N! - 2 \ln (N/2)!) \\
        &\approx \frac{k_{\text{B}}}{N} (N \ln N - 2 (N / 2) \ln N/2 ) \\
        &= k_{\text{B}} \ln 2.
    \end{aligned}
\]
Thus 
\begin{equation}
    \lim_{N \to \infty} \frac{1}{N} S(N, \mathcal{E}) = k_{\text{B}} \ln 2.
\end{equation}

\item[(d)] An output of $E$ -- we denote it as $\epsilon$ -- may be decomposed into
\begin{equation}
    \epsilon = \sum_i n_i \xi_i,
\end{equation}
so its probability is 
\begin{equation}
    p(\epsilon) = \Omega p_1^{n_1} p_2^{n_2} \cdots p_M^{n_M},
\end{equation}
where 
\begin{equation}
    \sum_{i} n_i = N,
    \label{eq:total-event-number}
\end{equation}
and 
\begin{equation}
    \begin{aligned}
        \Omega &= \binom{N}{n_1} \binom{N - n_1}{N - n_1 - n_2} \cdots \binom{N - n_1 - \cdots - n_{M-1}}{n_M} \\
        &= \frac{N!}{(N-n_1)! n_1!} \frac{(N - n_1)!}{(N - n_1 - n_2)! n_2!} 
        \cdots \frac{(N - n_1 - \cdots - n_{M-1})!}{n_{M}! 0!} \\
        &= \frac{N!}{n_1! n_2! \cdots n_M!}.
    \end{aligned}
\end{equation}
Since $N$ is large, we view $\{n_i\}$ as continuous variables.
Then, to maximize $p(\epsilon)$, we can maximize
\begin{equation}
    \ln p(\epsilon) = \ln \Omega + \sum_{i} n_i \ln p_i
\end{equation}
under the constraint of \eqref{eq:total-event-number}.
The expression of $\ln \Omega$ is given by the Stirling approximation:
\begin{equation}
    \begin{aligned}
        \ln \Omega &= \ln N! - \sum_i \ln n_i! \\
        &\approx N \ln N - N - \sum_i (n_i \ln n_i - n_i) \\
        &= N \ln N - \sum_i n_i \ln n_i.
    \end{aligned}
\end{equation}
By the method of Lagrange multiplier, the minimum is given by the equations 
\[
    \begin{aligned}
        0 &= \pdv{n_j} \left( \ln \Omega + \sum_i n_i \ln p_i - \lambda \left( \sum_i n_i - N \right)  \right) \\
        &= - \ln n_j - 1 + \ln p_j - \lambda.
    \end{aligned}
\] 
This means there is a constant $C$ such that for all $i$, we have 
\[
    n_i = C p_i.
\]
By \eqref{eq:total-event-number}, we know 
\begin{equation}
    n_i = N p_i.
\end{equation}
So 
\begin{equation}
    \begin{aligned}
        \ln \Omega(N, \mathcal{E}) &= N \ln N - \sum_i N p_i \ln (N p_i) \\
        &= N \ln N - N \sum_i p_i \ln p_i - \sum_i N p_i \ln N = - N \sum_i p_i \ln p_i.
    \end{aligned}
    \label{eq:omega-max}
\end{equation}
So in the $N \to \infty$ limit, under $\log_2$, \eqref{eq:omega-max} means  
\begin{equation}
    \lim_{N \to \infty } \frac{1}{N} \log_2 \Omega(N, \mathcal{E}) = - \sum_i p_i \log p_i \eqqcolon H(X).
\end{equation}

The Boltzmann entropy of $(N, \mathcal{E})$ is 
\begin{equation}
    S(N, \mathcal{E}) = k_{\text{B}} \ln \Omega(N, \mathcal{E}) = - k_{\text{B}} N \sum_i p_i \ln p_i,
\end{equation}
and thus 
\begin{equation}
    \lim_{N \to \infty} \frac{S(N, \mathcal{E})}{N} = - k_{\text{B}} \sum_{i} p_i \ln p_i 
    = k_{\text{B}} \ln 2 \times H.
    \label{eq:shannon-micro}
\end{equation}
Thus if we put a large amount of identical copies of the same system together 
and calculate its Boltzmann entropy when the total energy is in its most possible value,
then the average Boltzmann entropy only differs with the Shannon entropy with a prefactor $k_{\text{B}} \ln 2$.

\item[(e)] To summarize:
\begin{itemize}
    \item In the canonical ensemble, the thermodynamic entropy 
    only differs with the Shannon entropy of the system configuration 
    with a prefactor $k_{\text{B}} \ln 2$ -- see \eqref{eq:shannon-canonical}.
    \item In the microcanonical ensemble, when a large amount of a certain kind of degree of freedom 
    (for example particles) are put together 
    and their total energy is in the most possible value, 
    then the average Boltzmann entropy only differs with the Shannon entropy of the system configuration 
    with a prefactor $k_{\text{B}} \ln 2$ -- see \eqref{eq:shannon-micro}. 
    \item So in the $N \to \infty$ limit, 
    the average Boltzmann entropy of the microcanonical ensemble 
    in the most probable configuration, 
    the thermodynamic entropy of the canonical ensemble,
    and the Shannon entropy 
    are all ``equivalent'' in some sense.
\end{itemize}

\item[(f)] The terms in the RHS are
\[
    H(p_1 + p_2, \ldots, p_M) = - (p_1 + p_2) \log_2 (p_1 + p_2) - \sum_{i \geq 3} p_i \log_2 p_i,
\]
and 
\[
    \begin{aligned}
        &\quad (p_1 + p_2) H\left( \frac{p_1}{p_1 + p_2}, \frac{p_2}{p_1 + p_2} \right) \\
        &= - (p_1 + p_2) \left( \frac{p_1}{p_1 + p_2} \log_2 \frac{p_1}{p_1 + p_2}  
        + \frac{p_2}{p_1 + p_2} \log \frac{p_2}{p_1 + p_2} \right) \\
        &= - p_1 \log_2 p_1 + p_1 \log (p_1 + p_2) - p_2 \log p_2 + p_2 \log (p_1 + p_2),
    \end{aligned}
\]
so the RHS is 
\[
    - p_1 \log_2 p_1 - p_2 \log_2 p_2 - \sum_{i \geq 3} p_i \log_2 p_i,
\]
which is just the LHS. So 
\begin{equation}
    H(p_1, p_2, \ldots, p_M) = H(p_1 + p_2, \ldots, p_M) + (p_1 + p_2) H\left( \frac{p_1}{p_1 + p_2}, \frac{p_2}{p_1 + p_2} \right).
    \label{eq:entropy-plus}
\end{equation}

For example we have 
\begin{equation}
    H(1/2, 1/4, 1/4) = H(1/2, 1/2) + \frac{1}{2} H(1/2, 1/2) = 1 + \frac{1}{2} = 1.5.
\end{equation}

\eqref{eq:entropy-plus} means the Shannon entropy is additive to some extent:
if we first decide to ignore the difference between $\xi_1$ and $\xi_1$,
then the resulting entropy is $H(p_1 + p_2, \ldots, p_M)$.
To recover the original entropy,
we just need to calculate the ``inner'' entropy of the $\xi_1$-or-$\xi_2$ possibility -- 
that is, to calculate $H(p_1 / (p_1 + p_2), p_2 / (p_1 + p_2))$ --
and then multiply a $(p_1 + p_2)$ weight to it,
and after putting the two parts of entropies together, 
we get the original entropy corresponding to the full amount of information.

\item[(g)] We need to take the $\alpha \to 1$ limit of 
\begin{equation}
    H_\alpha(X) = \frac{1}{1 - \alpha} \log_2 \left( \sum_i p_i^\alpha \right) .
\end{equation}
When $\alpha = 1$, both the numerator ($\log_2 1 = 0$) and the denominator are zero,
so we can use the L'Hospital's rule:
\[
    \lim_{\alpha \to 1} H_{\alpha} = \lim_{\alpha \to 1} \frac{\frac{\sum_{i} \ln p_i p_i^\alpha}{\ln 2 \sum_{i} p_i^\alpha}}{-1}
    = - \sum_i p_i \log p_i = H(X).
\]

\item[(h)] The eigenvalues of $\rho$ are $(1 \pm \abs{\vb*{a}}) / 2$.
Thus 
\begin{equation}
    \begin{aligned}
        H_1(\rho) &= - \rho_1 \log_2 \rho_1 - \rho_2 \log_2 \rho_2 \\
        &= 1 - \frac{1 + \abs{\vb*{a}}}{2} \log_2 (1 + \abs{\vb*{a}})
        - \frac{1 - \abs{\vb*{a}}}{2} \log_2 (1 - \abs{\vb*{a}}),
    \end{aligned}
\end{equation}
and 
\begin{equation}
    \begin{aligned}
        H_2 (\rho) &= - \log_2 \left( \frac{(1 + \abs{\vb*{a}})^2}{4} + \frac{(1 - \abs{\vb*{a}})^2}{4} \right) \\
        &= - \log_2 \frac{1 + \abs{\vb*{a}}^2}{2}.
    \end{aligned}
\end{equation}
The maximum $1$ is reached when $\abs{\vb*{a}} = 0$,
and the minimum $0$ is reached when $\abs{\vb*{a}} = 1$.
When $\abs{\vb*{a}} = 0$, 
$\rho$ is essentially a classical $\SI{50}{\percent}$-$\SI{50}{\percent}$ probabilistic distribution,
so the entropy is the same as the entropy of a random bit, which is 1.
This is the most ``noisy'' case and indeed we get a maximal entropy here.
When $\abs{\vb*{a}} = 1$, $\rho$ is a pure state,
and there is nothing uncertain about it, so its entropy is 0, which is the minimum.

\end{itemize}

\end{document}