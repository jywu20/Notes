\documentclass[hyperref, a4paper]{article}

\usepackage{geometry}
\usepackage{titling}
\usepackage{titlesec}
% No longer needed, since we will use enumitem package
% \usepackage{paralist}
\usepackage{enumitem}
\usepackage{footnote}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathtools}
\usepackage{bbm}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{soulutf8}
\usepackage{physics}
\usepackage{tensor}
\usepackage{siunitx}
\usepackage[version=4]{mhchem}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{autobreak}
\usepackage[ruled, vlined, linesnumbered]{algorithm2e}
\usepackage{nameref,zref-xr}
\zxrsetup{toltxlabel}
\usepackage[backend=bibtex]{biblatex}
\addbibresource{elasticity.bib}
\usepackage[colorlinks,unicode]{hyperref} % , linkcolor=black, anchorcolor=black, citecolor=black, urlcolor=black, filecolor=black
\usepackage[most]{tcolorbox}
\usepackage{prettyref}

% Page style
\geometry{left=3.18cm,right=3.18cm,top=2.54cm,bottom=2.54cm}
\titlespacing{\paragraph}{0pt}{1pt}{10pt}[20pt]
\setlength{\droptitle}{-5em}

% More compact lists 
\setlist[itemize]{
    itemindent=17pt, 
    leftmargin=1pt,
    listparindent=\parindent,
    parsep=0pt,
}

% Math operators
\DeclareMathOperator{\timeorder}{\mathcal{T}}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\legpoly}{P}
\DeclareMathOperator{\primevalue}{P}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\res}{Res}
\DeclareMathOperator{\sinc}{sinc}
\newcommand*{\ii}{\mathrm{i}}
\newcommand*{\ee}{\mathrm{e}}
\newcommand*{\const}{\mathrm{const}}
\newcommand*{\suchthat}{\quad \text{s.t.} \quad}
\newcommand*{\argmin}{\arg\min}
\newcommand*{\argmax}{\arg\max}
\newcommand*{\normalorder}[1]{: #1 :}
\newcommand*{\pair}[1]{\langle #1 \rangle}
\newcommand*{\fd}[1]{\mathcal{D} #1}
\DeclareMathOperator{\bigO}{\mathcal{O}}

% TikZ setting
\usetikzlibrary{arrows,shapes,positioning}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{calc}
\tikzstyle arrowstyle=[scale=1]
\tikzstyle directed=[postaction={decorate,decoration={markings,
    mark=at position .5 with {\arrow[arrowstyle]{stealth}}}}]
\tikzstyle ray=[directed, thick]
\tikzstyle dot=[anchor=base,fill,circle,inner sep=1pt]

% Algorithm setting
% Julia-style code
\SetKwIF{If}{ElseIf}{Else}{if}{}{elseif}{else}{end}
\SetKwFor{For}{for}{}{end}
\SetKwFor{While}{while}{}{end}
\SetKwProg{Function}{function}{}{end}
\SetArgSty{textnormal}

\newcommand*{\concept}[1]{{\textbf{#1}}}

% Embedded codes
\lstset{basicstyle=\ttfamily,
  showstringspaces=false,
  commentstyle=\color{gray},
  keywordstyle=\color{blue}
}

% Reference formatting
\newcommand*{\citesec}[1]{\S~{#1}}
\newcommand*{\citechap}[1]{chap.~{#1}}
\newcommand*{\citefig}[1]{Fig.~{#1}}
\newcommand*{\citetable}[1]{Table~{#1}}
\newcommand*{\citepage}[1]{pp.~{#1}}
\newrefformat{fig}{Fig.~\ref{#1}}
\newcommand*{\term}[1]{\textit{#1}}

% Color boxes
\tcbuselibrary{skins, breakable, theorems}

\newtcbtheorem{infobox}{Box}{
    enhanced,
    boxrule=0pt,
    colback=blue!5,
    colframe=blue!5,
    coltitle=blue!50,
    borderline west={4pt}{0pt}{blue!65},
    sharp corners,
    fonttitle=\bfseries, 
    breakable,
    before upper={\parindent15pt\noindent}}{box}
\newtcbtheorem[use counter from=infobox]{theorybox}{Box}{
    enhanced,
    boxrule=0pt,
    colback=orange!5, 
    colframe=orange!5, 
    coltitle=orange!50,
    borderline west={4pt}{0pt}{orange!65},
    sharp corners,
    fonttitle=\bfseries, 
    breakable,
    before upper={\parindent15pt\noindent}}{box}
\newtcbtheorem[use counter from=infobox]{learnbox}{Box}{
    enhanced,
    boxrule=0pt,
    colback=green!5,
    colframe=green!5,
    coltitle=green!50,
    borderline west={4pt}{0pt}{green!65},
    sharp corners,
    fonttitle=\bfseries, 
    breakable,
    before upper={\parindent15pt\noindent}}{box}


\newenvironment{shelldisplay}{\begin{lstlisting}}{\end{lstlisting}}

\newcommand*{\kB}{k_{\text{B}}}
\newcommand*{\muB}{\mu_{\text{B}}}
\newcommand*{\efermi}{E_{\text{F}}}
\newcommand*{\pfermi}{p_{\text{F}}}
\newcommand*{\vfermi}{v_{\text{F}}}
\newcommand*{\sA}{\text{A}}
\newcommand*{\sB}{\text{B}}
\newcommand*{\Tc}{T_{\text{c}}}
\newcommand*{\hethree}{$^3$He}
\newcommand*{\hefour}{$^4$He}
\newcommand{\epsr}{\epsilon_{\text{r}}}
\newcommand{\chie}{\chi_{\text{e}}}
\newcommand*{\Gammae}{\Gamma_{\text{e}}}
\newcommand*{\Gammag}{\Gamma_{\text{g}}}
\newcommand*{\omegae}{\omega_{\text{e}}}
\newcommand*{\omegag}{\omega_{\text{g}}}
\newcommand*{\omegaeg}{\omega_{\text{eg}}}
\newcommand*{\ptwfc}[2]{\psi^{(#2)}_{#1}}
\newcommand*{\mueg}{\mu_{\text{eg}}}
\newcommand*{\muge}{\mu_{\text{ge}}}
\newcommand*{\Ezzero}{E_{z0}}
\newcommand*{\kete}{\ket*{\text{e}}}
\newcommand*{\ketg}{\ket*{\text{g}}}
\newcommand*{\coeffe}{c_{\text{e}}}
\newcommand*{\coeffg}{c_{\text{g}}}
\newcommand*{\pope}{p_{\text{e}}}
\newcommand*{\popg}{p_{\text{g}}}
\newcommand*{\ptwo}{P^{(2)}}
\newcommand*{\vp}{v_{\text{p}}}
\newcommand*{\vg}{v_{\text{g}}}
\newcommand*{\chitwo}{\chi^{(2)}}
\newcommand*{\chithree}{\chi^{(3)}}
\newcommand*{\omegap}{\omega_{\text{p}}}
\newcommand*{\Si}{S_{\text{i}}}
\newcommand*{\So}{S_{\text{o}}}
\newcommand*{\bi}{b_{\text{i}}}
\newcommand*{\bo}{b_{\text{o}}}
\newcommand*{\Tre}{\mathcal{T}}

\title{Topics in optics and quantum electronics}
\author{Jinyuan Wu}

\begin{document}

\maketitle

\section{Mode description of electromagnetic fields}

In this section we mainly focus on the mode description of classical electrodynamics.
Modes are seen in resonators, waveguides, photonic crystals, and more.
Loss can then be included perturbatively and we get leaky modes.

In the case of electrodynamics in vacuum, in the Coulomb gauge (i.e. $\varphi = 0$), we have 
\begin{equation}
    \curl\curl{\vb*{A}} = \left(\frac{\omega}{c}\right)^2 \vb*{A}, 
\end{equation}
and under the inner product definition 
\begin{equation}
    \braket*{\vb*{A}}{\vb*{B}} \coloneqq \int \dd{\vb*{r}} \vb*{A}^* \cdot \vb*{B},
\end{equation}
the LHS of the equation on $\vb*{A}$ is Hermitian, guaranteeing that $\omega$ is real.

From the well-known relation $\comm*{a}{a^\dagger} = 1$, 
it follows that the classical counterparts of creation and annihilation operators 
are complex variables which satisfies the relation 
\begin{equation}
    \poissonbracket{a}{a^*} = - \ii.
    \label{eq:a-astar-relation}
\end{equation}
This means if the Poisson brackets are to be defined in terms of $a$ and $a^*$, it should be defined as 
\begin{equation}
    \poissonbracket{A}{B} = \frac{1}{\ii} \left(\pdv{A}{a} \pdv{B}{a^*} - \pdv{A}{a^*} \pdv{B}{a}\right),
    \label{eq:poisson-ab}
\end{equation}
where $a$ and $a^*$ are to be regarded as independent variables.
The general relation between $a, a^*$ and $x, p$ is 
\begin{equation}
    a = \alpha q + \ii \beta p, \quad 
    q = \frac{a + a^*}{2 \alpha}, \quad p = \frac{a - a^*}{2 \ii \beta}, \quad 2 \alpha \beta = 1.
    \label{eq:a-astar-x-p-relation}
\end{equation}
If $q$ and $p$ follow the canonical commutation relation,
then $a$ and $a^*$ follow the commutation relation between 
a creation operator and a annihilation operator, and vice versa. 

We can also justify \eqref{eq:a-astar-relation} by Hamiltonian's equations.
From \eqref{eq:a-astar-x-p-relation} and the cian rule, we find 
\begin{equation}
    \begin{aligned}
    \dot{a} &= \pdv{a}{q} \dot{q} + \pdv{a}{p} \dot{p}
    = \pdv{a}{q} \pdv{H}{p} - \pdv{a}{p} \pdv{H}{q} \\
    &= \pdv{a}{q} \left(\pdv{H}{a} \pdv{a}{p} + \pdv{H}{a^*} \pdv{a^*}{p}\right)
    - \pdv{a}{p}  \left(\pdv{H}{a} \pdv{a}{q} + \pdv{H}{a^*} \pdv{a^*}{q}\right) \\
    &= - 2 \ii \alpha \beta \pdv{H}{a^*},
    \end{aligned}
\end{equation}
and similarly 
\begin{equation}
    \dot{a}^* = 2 \ii \alpha \beta \pdv{H}{a}.
\end{equation}
Now that means we have to define the Poisson bracket as  \eqref{eq:poisson-ab}
to keep the form 
\begin{equation}
    \dv{A}{t} = \poissonbracket{A}{H}.
\end{equation}

The $2 \alpha \beta = 1$ constraint can be explained by conservation of area in phase space: 
roughly speaking the area inside the trajectory of $a(t)$ or $(q(t), p(t))$ 
is the ``strength of oscillation'', like the number of photons.

Now going back to the problem of electromagnetic fields.
Consider the one dimensional periodic boundary condition problem.
We have (in phaser form)
\begin{equation}
    \tilde{E}_\pm(x, t) = \tilde{E}_\pm^0 \ee^{\ii k x - \ii \omega t}, 
\end{equation}
and we consider $A$ to be the coordinate and hence $-E = \pdv*{A}{t}$ is the momentum.
To decompose $A$ into modes, we write $A$ as static, spatially varying factors weighted by time-dependent real values,
and thus $A = q(t) A_0 \ee^{\ii k z}$, 

\section{Perturbation theory}

Stationary electrodynamics in terms of $\vb*{E}$ is not a Hermitian problem, 
but a generalized Hermitian one:
the problem is 
\begin{equation}
    \curl{\curl{\vb*{E}}} = \left(\frac{\omega}{c}\right)^2 \epsr(\vb*{r}) \vb*{E}(\vb*{r}),
\end{equation}
and although the operators on both LHS and RHS are Hermitian, 
the LHS multiplied with the inverse of the RHS is not.
The time-independent perturbation theory for this problem is therefore slightly more complicated, 
and the ``energy'' being solved is 
\begin{equation}
    \alpha \coloneqq \frac{\omega^2}{c^2},
\end{equation}
instead of the frequency. 
\begin{equation}
    \Delta \omega^{(1)}_n = - \frac{\omega_n^{(0)}}{2} \mel*{\tilde{\vb*{E}}_n}{\epsilon_0 \Delta \epsr}{\tilde{\vb*{E}}_n}.
    \label{eq:first-order-delta-eps}
\end{equation}
The minus sign can be understood using the following line of argumentation:
if a piece of dielectric is inserted into an isolated system, 
the total energy is conserved but some energy is stored 
within the internal degree of freedom dielectric, 
so $\omega (a^\dagger a + 1/2)$, the vacuum part of the Hamiltonian has to decrease.
One important caveat is that the leading order of energy correction 
\emph{can't} be evaluated by $\mel*{\vb*{E}_n}{\Delta \epsr}{\vb*{E}_n}$:
since the mapping from the electric field to the magnetic field 
involves at least one time derivative, 
the electromagnetic Hamiltonian in the form of $\vb*{E}^2 + \vb*{B}^2$
can't be seen as an ordinary Hamiltonian whose corresponding wave functions are electric fields, 
and therefore ordinary perturbation techniques fail.
One way to work around this is doing everything in terms of modes.
This is done in homework 2. 

\section{Loss as perturbation}

Now we study the influence of $\epsilon_2$, the imaginary part of $\epsr$, as a perturbation.
We can just inject $\Delta \epsr = \ii \epsilon_2$ into \eqref{eq:first-order-delta-eps}, 
and we get 
\begin{equation}
    \Delta \omega_n^{(1)} = - \ii \frac{1}{\tau_n}, \quad 
    \frac{1}{\tau_n} = \frac{\omega_n}{2} \mel*{\tilde{\vb*{E}}_n}{\epsilon_0 \epsilon_2}{\tilde{\vb*{E}}_n}.
\end{equation}
This means the time evolution of the corresponding mode $a_n$ contains a $\ee^{- t / \tau_n}$ factor, 
and the EOM seems to be 
\begin{equation}
    \dot{a} = - \ii \omega a - \frac{1}{\tau} a.
    \label{eq:eom-a-0}
\end{equation}
We can add a driving field to the RHS of this equation of motion; 
without considering any nonlinear effects,%
\footnote{
    Damping always comes from coupling of the system with a much larger, external bath.
    The first order correction from the bath 
    adds a finite imaginary part to the self energy of the system; 
    the interaction with the bath of course also modifies the 
    coupling between the bath and the system, 
    but this is not first-ordered, 
    and can be ignored when damping is small.
}
the lifetime remains the same regardless of the possible external driving.
The question is whether \eqref{eq:eom-a-0} is truly the correct equation 
for a damped system without external driving.
It is of course possible to have a different kind of damping:
if we choose to not start from the imaginary part of $\epsr$ 
and instead insert a $- k v$ type of damping into the EOM of $p$ and $q$, 
then the damping term for $a$ should assumes the form of 
\begin{equation}
    \dot{a} + \ii \omega_0 a = - \frac{a}{\tau} + \frac{a^*}{\tau}.
    \label{eq:eom-a-0-coupled}
\end{equation}
Interestingly, from an equally intuitive starting point, 
now we get a set of coupled EOMs for $a$ and $a^*$, 
while in \eqref{eq:eom-a-0}, the EOMs for $a, a^*$ are not coupled.

The ``real'' form of the damping term depends on how the system couples to the bath; 
if we are studying a harmonic oscillator then \eqref{eq:eom-a-0-coupled} is exact, 
while \eqref{eq:eom-a-0} is not; 
the most natural way to include loss in Maxwell's equations also leads to 
something similar to \eqref{eq:eom-a-0-coupled}.
So indeed, although \eqref{eq:eom-a-0} seems quite natural, 
it's not exact, and the question is how it's related to the exact equations.

The answer is rotating wave approximation. 
From the usual procedure we will find the coupled terms in the loss are related to fast oscillation terms, 
which can be ignored for the low frequency part of $a, a^*$. 
This is valid as long as there is separation between the time scales of damping and of oscillation.
This is the condition of the validity of RWA of an external driving field, 
which, in the latter case, means the frequency of driving should be close to $\omega_0$
while the driving \emph{amplitude} should be small.
Since the damping term is not oscillating, 
the validity condition of RWA for damping is simpler.

TODO: does rotating wave approximation change the way the system couple to external fields?
(c.f. general relativity and the $f(R)$ theory)

\section{Mode coupling}

Consider a ring resonator. 
We have a right mover mode and a left mover mode, 
which we refer to as 
\begin{equation}
    \tilde{E}_k = a_k E_k^0 \ee^{\ii k z}
\end{equation}
and 
\begin{equation}
    \tilde{E}_{-k} = a_{-k} E_{-k}^0 \ee^{- \ii k z},
\end{equation}
respectively. 

Now suppose we have such a field configuration:
\begin{equation}
    E = a_k \tilde{E}_k + \text{c.c.} + a_{-k} \tilde{E}_{-k},
\end{equation}
and there is a $\Delta \epsr$ perturbation in the system.
The change of the frequency is therefore given by  
\begin{equation}
    \Delta \omega^{(1)} = - \frac{\omega^{(0)}}{2} \expval{\Delta \epsr}
    \propto (a_k^* + a_{-k}) (a_k^* + a_{-k}) + \text{c.c.},
\end{equation}
and therefore the contribution of $\Delta \epsr$ to the energy of the system looks like
\begin{equation}
    \Delta E = - \tilde{\gamma} (a_k^* + a_{-k}) (a_k^* + a_{-k}) + \text{c.c.} 
\end{equation}
The EOM of $a_k$ therefore is (TODO: formalism)
\begin{equation}
    \dot{a}_k = - \ii \omega_k a_k + 2 \ii \gamma (a_k^* + a_{-k}).
\end{equation}
Again we can apply RWA and remove the ``fast'' $a_k^*$ term, 
so eventually the EOMs of the two modes are (TODO: RWA without external driving)
\begin{equation}
    \dot{a}_k = - \ii \omega_0 a_k + \ii \tilde{g} a_{-k}, \quad 
    \dot{a}_{-k} = - \ii \omega_0 a_{-k} + \ii \tilde{g}^* a_{k}, \quad 
    \tilde{g} = 2 \gamma.
\end{equation}
The EOMs are still within the Hamiltonian dynamics framework, 
so we can diagonalize them and find that when 
\begin{equation}
    \Delta \epsr = \Delta \epsilon \cos(2kz),
\end{equation}
the new mode 
$a_{k} + a_{-k}$ has a lower frequency $\omega_0 - \abs*{\tilde{g}}$
while the new mode $a_{k} - a_{-k}$ has a higher frequency $\omega_0 + \abs*{\tilde{g}}$.
But the above formalism is not restricted to this perturbation: 
whatever $\Delta \epsr$ is, the way it affects the two modes we consider here 
is by influencing $\tilde{g}$.
Specifically, $\abs*{\tilde{g}}$ can be used to estimate the roughness of a ring resonator.

Of course, we can also perturb the system by modulation in \emph{time}.
Consider a classic $LC$-circuit, for example:
it cam be verified that an oscillating \(C\) results in 
\begin{equation}
    H = \omega_0 a^* a + \frac{\gamma(t)}{2} (a + a^*)^2,
\end{equation}
because there is no \(L^2\) term that cancels the \(a a^*\) terms.
The resulting EOMs are 
\begin{equation}
    \dot{a} = - \ii \omega_0 a - \ii \gamma(t) (a + a^*), 
\end{equation}
and the perturbation $\gamma(t)$ usually takes the form like 
\begin{equation}
    \gamma(t) = \frac{\tilde{\gamma}_0}{2} \ee^{- \ii \Omega t} + \text{c.c.}
\end{equation}
where $\Omega = 2 \omega_0$.
After RWA the only term that survives is 
\begin{equation}
    \dot{a} = - \ii \omega_0 a - \ii \frac{\tilde{\gamma}_0}{2} \ee^{- \ii \Omega t} a^*
\end{equation}
and hence we find 
\begin{equation}
    \dv{t} \bmqty{a \ee^{- \ii \omega_0 t} \\ a^* \ee^{\ii \omega t}} = \bmqty{
        0 & - \ii \frac{\tilde{\gamma}_0}{2} \\
        \ii \frac{\tilde{\gamma}_0^*}{2} & 0 
    } \bmqty{a \ee^{- \ii \omega_0 t} \\ a^* \ee^{\ii \omega t}}
\end{equation}
We can easily see that the ``source'' of the $\ee^{- \ii \Omega t}$ term is $a^*$, 
which in turn comes from the $\ee^{\ii \Omega t}$ term and $a$.
This eventually gives us a feedback (usually \emph{positive} but not always -- see below) to $a$,
leading to the increase of the total energy.
The derivative of the ``free'' energy is 
\begin{equation}
    \dot{H}_{\text{SHO}} = \Re (- \ii \abs*{\tilde{\gamma}_0} \ee^{\ii \phi} a_0), 
\end{equation}
where $\phi$ is the phase of $\tilde{\gamma}_0$, 
and $a_0$ is the slow oscillating part of $a$.
This expression tells us two things.
First, the phase of the pumping matters: 
it can be chosen in a specific way to \emph{reduce} the total energy.
Second, the growth or damping of the total energy is exponential, 
and therefore if at first you don't have a finite amplitude, 
there is no growth at all.
To start the growth, noise is important; 
classically this comes from thermal fluctuation; 
in the quantum case, the above EOM involves some terms from non-trivial commutation relations 
and we may say the growth is started by the zero-point energy.

\section{Nonlinearity}

We know the free $\epsilon_0 \epsr \vb*{E}^2 / 2 + \vb*{B}^2 / 2 \mu_0$ Hamiltonian 
can be written as $\sum \omega a^* a$; 
similarly a nonlinear Hamiltonian can be written as 
$\sum a a ^* + \text{c.c.}$; 
in principle all modes can be mixed together by nonlinearity; 
usually however only transitions satisfy the phase matching condition are strong enough;
the definition of ``phase matching'' is somehow unclear: 
for plain waves, it means $\sum \vb*{k} = 0$ 
and has to be satisfied \emph{strictly}; 
but in other cases, like in a photonic crystal, 
if $\vb*{k}$ means Bloch wave vectors, then the  
TODO: again, if a transition channel doesn't satisfy the phase matching condition, 
does it mean this channel vanish in reciprocal space?

Phase matching in the time domain has to be exactly satisfied 
only when we are dealing with a scattering problem, 
in which we integrate from $t=-\infty$ to $t = \infty$ 
and get a $\sum \omega = 0$ condition; 
for a finite time problem, this condition doesn't need to be exactly satisfied, 
although a transition channel satisfying the $\sum \omega = 0$ condition 
still likely to be the most influential one.
In Feynman diagrams we do have strict frequency conservation conditions at each vertex
but then we don't have the on-shell condition for internal lines; 
by the correspondence between Feynman diagrams and Goldstone diagrams, 
we can let the internal lines to satisfy the on-shell conditions 
but let the total energy of the system to be off-shell, 
therefore arriving at results obtained from the EOM approach.

The EOM of one mode looks like (here NL means ``nonlinear'')
\begin{equation}
    \dot{a}_1 = - \ii \omega_0 a_1 - \ii \pdv{H_{\text{NL}}}{a_1^*}.
\end{equation}
For SHG, for example, we have 
\begin{equation}
    \begin{aligned}
        \dot{a}_1 &= - \ii \omega_1 a_1 + \ii \tilde{\gamma}   a_1^* a_2, \\
        \dot{a}_2 &= - \ii \omega_2 a_2 + \ii \tilde{\gamma}^* a_1   a_1.
    \end{aligned} 
\end{equation}

We can also study a nonlinear system using nonlinear Maxwell's equations.
This can be seen as a cross-verification of the correctness 
of the mode-based, usually somehow simplified formalism;
when the dielectric function has a strong ``dispersion'' -- i.e. frequency dependence -- 
the Hamiltonian approach is also hard to use.

\section{Temporal module coupling}

Now we consider the coupling of modes with the external world.
The formalism is almost identical to the so-called input-output formalism in quantum optics.
As a motivation, consider the following model: 
imagine we have a one-dimensional photonic crystal, 
with one defect (for example a missing cavity in a string of cavities).
Now if a pulse is injected from one end of the photonic crystal 
we expect to see reflectivity and radiation to the 3D space around the defect.
Whatever the defect is, it can be modeled as a mode (or several modes)
coupled with ``ports'' around it.
In the absence of the ports, the EOM of the system always takes the form 
$\dot{a} = - \ii \omega_0 a$, 
or possibly with a decay. 
Now we consider the structure of a port. 
We represent the waves going ``into'' the defect as $\Si$
and the waves going ``out'' of the defect as $\So$.
Both variables may be linear combination of modes in the ports, 
or even contain some nonlinear effects.
The EOM of the defect then is (TODO: Peter says it only holds under RWA) 
\begin{equation}
    \dot{a} = - \ii \omega_0 a - \frac{a}{\tau} + K \Si.
\end{equation}
Here we do the Markovian approximation and assume that the $a / \tau$ term
faithfully captures the coupling of the system with the output port.
Note that this assumption is completely \emph{orthogonal} 
to the assumption that the bath is thermalized, 
and therefore the equation above is \emph{not} a Langevin equation,
and still we can further loosen this approximation 
and include frequency dependence in the energy loss term.
Indeed, although from the perspective of the mode $a$, 
the interaction with modes in the port causes an exponential decay, 
from the perspective of modes in the ports 
there is no real dissipation and coupling with the disorder can be captured as a scattering process 
-- see the end of this section. 
Of course it's possible to have ``dissipative loss'', 
which means we have modes that are not considered as a part of the port 
but are nevertheless coupled to $a$ and damp it;%
\footnote{
    Sometimes this kind of loss is known as \concept{internal loss}; 
    note that here the term \term{internal} means ``not a part of the system we are interested in''. 
}
here we ignore these dissipation channels and assume that 
dissipation mainly comes from the output modes of the port.
Now if we somehow manage to integrate out $a$ 
and consider the theory of the port.
Ignoring all nonlinear effects and retardation effects, 
the relation between $\So$ and $\Si, a$ can only take the following form: 
\begin{equation}
    \So = \beta \Si + \gamma a, 
\end{equation}
where $\beta$ and $\gamma$ have to be related to $\tau$.

When $\Si, \So$ are appropriately normalized we expect to see that 
$\abs*{\Si}^2$ is the power incident while 
$\abs*{\So}^2$ is the output power.
For example, suppose we have a ring resonator, and we place a defect in it.
Since a ring resonator is usually thin, 
it can be modeled as a pipe with periodic boundary condition, 
and the power going through an arbitrary point in the ring resonator takes the form of 
\begin{equation}
    P = \vg \frac{\omega_0 a^* a}{L}, 
\end{equation}
where $a$ is a mode in the ring resonator (not the mode around the defect),
and therefore $S$ should be defined such that 
\begin{equation}
    P = S S^* = \vg \frac{\omega_0 a^* a}{L}.
\end{equation}
Moreover we normalize $a$ so that $\abs*{a}^2$ is the mode energy.

Now we seriously consider how to derive the relation between $\tau$ and $\gamma, K$.
The most famous example for coupling with outgoing modes leading to dissipation 
is arguably the fact that an infinite network of LC circuits is equivalent to a resistor.
Similar systems include a mass point connected to an infinite spring or a leaky cavity.
When there is no input to the defect mode, 
the loss of energy in the mode equals the outgoing flux $\So$, and therefore 
\begin{equation}
    \dv{U_{\text{defect}}}{t} = - \frac{2}{\tau} U_{\text{defect}}
    = - \abs*{\So}^2,
\end{equation}
and therefore under the normalization convention 
\begin{equation}
    U_{\text{defect}} = \abs*{a}^2,
\end{equation}
we have
\begin{equation}
    \So = \underbrace{\sqrt{\frac{2}{\tau}} \ee^{\ii \alpha}}_\gamma a.
\end{equation}
The phase factor is a variable that depends on the exact definition of the output of the system;
consider for example a defect embedded into a wave guide: 
we need to choose a point at which we decide ``we are no longer near the defect'', 
and the definition -- hence the phase -- of the outgoing mode depends on this point.
TODO: what happens when the input is turned on? 
We will get something like $2\Re K S_i^* a = \abs*{S_i}^2$.
Whether this is always true can be verified using a harmonic oscillator; 
essentially that's about what is the definition of ``outgoing''. 
Using MBPT this can be easily justified:
when everything is linear, couplings between the $\Si$ and $\So$ modes do not influence each other.

We still have $K$ and $\beta$. Since $a, \Si, \So$ are assumed to form a closed system, 
we expect they form a system with time reversal symmetry.
After time  reversal operation, hence the EOM of $a$ becomes 
\begin{equation}
    -\dv{t} a^*(-t) = - \ii \omega_0 a^*(-t) - \frac{a^*(-t)}{\tau} + \Tre{K S_{\text{i}}},
\end{equation} 
where we don't immediately write what's the time reverse of $S_{\text{i}}$:
time reversal operation also changes the mode label, 
and the correct transform is $\Tre{S_{\text{i}}} = \So^*(-t)$
(in principle we can put an additional phase factor between $\Tre \Si$ and $\So^*$; 
for convenience this is not done here), 
so eventually 
\begin{equation}
    -\dv{t} a^*(-t) = - \ii \omega_0 a^*(-t) - \frac{a^*(-t)}{\tau} + K \So^*(-t),
\end{equation}
and therefore 
\begin{equation}
    \dv{t} a(-t) = - \ii \omega_0 a(-t) + \frac{a(-t)}{\tau} - K^* \So(-t).
\end{equation}
Similarly 
\begin{equation}
    \Si(-t) = \beta^* \So(-t) + \gamma^* a(-t).
\end{equation}
Note that $\Tre$ doesn't change the coefficients but does turn $a$ to $a^*$ 
because the lesser are complex fields.
Now the EOMs should be equivalent to the EOMs before time reversal operation, 
and when the input modes are turned off, from 
\[
    \So(-t) = - \frac{\gamma^*}{\beta^*} a(-t) = - \frac{\ee^{- \ii \alpha}}{\beta^*} \sqrt{\frac{2}{\tau}} a(-t)
\]
we find 
\begin{equation}
    \beta = - \ee^{2 \ii \alpha}.
\end{equation}
Similarly by comparing the EOMs of $a$, when the input is turned off, we have 
\[
    \dv{t} a(-t) = - \ii \omega_0 a(-t) + \frac{a(-t)}{\tau} 
    - K^* \ee^{\ii \alpha} \sqrt{\frac{2}{\tau}} a(-t) ,
\] 
and since we also expect to see 
\[
    \dv{t} a(-t) = \frac{1}{\tau} a(-t),
\]
we have 
\begin{equation}
    K = \ee^{\ii \alpha} \sqrt{\frac{2}{\tau}},
\end{equation} 
and similarly
\begin{equation}
    \gamma = \ee^{\ii \alpha} \sqrt{\frac{2}{\tau}}.
\end{equation}
The most general EOMs therefore are 
\begin{equation}
    \dot{a} = - \ii \omega_0 a - \frac{a}{\tau} + \ee^{\ii \alpha} \sqrt{\frac{2}{\tau}} \Si, \quad 
    \So = - \ee^{2 \ii \alpha} \Si + \ee^{\ii \alpha} \sqrt{\frac{2}{\tau}} a.
\end{equation}
The positions of the $\ee^{\ii \alpha}$ factors are quite clear:
it means the time it takes for the input current to arrive at $a$ and goes back.
The minus sign reminds us of the phase shift in electromagnetic wave reflection.
For the single-mode-single-port case,
since $\alpha$ can be shifted by redefining the modes, 
usually either we choose $\ee^{\ii \alpha} = 1$ or $\ee^{\ii \alpha} = -1$.
In the former convention we have 
\begin{equation}
    \dot{a} = - \ii \omega_0 a - \frac{a}{\tau} + \sqrt{\frac{2}{\tau}} \Si, \quad 
    \So = - \Si + \sqrt{\frac{2}{\tau}} a. 
\end{equation}

We can easily extend the above EOMs to the case with multiple ports, 
and the only thing needed is to sum over all $1/\tau$ terms and $\Si$ terms, 
while keeping the $\So = - \Si + \sqrt{2/\tau} a$ equation for each port,  
assuming that we only have hidden degrees of freedom that make the ports interact with each other
or otherwise we should include a non-trivial scattering matrix to link the input and output modes of different ports.

From the equations concepts like impedance can then be defined: 
we can switch to the frequency domain, and we will find 
\begin{equation}
    \frac{\So(\omega)}{\Si(\omega)} = - \frac{\omega - \omega_0 - \ii / \tau}{\omega - \omega_0 + \ii / \tau}.
    \label{eq:si-to-so}
\end{equation}
Note that the relation between $\Si(\omega)$ and $\So(\omega)$
-- which, when there is no energy degeneracy, are just the value of \emph{one} mode at $t=-\infty$ and $t=\infty$ -- 
is basically a relatively trivial scattering matrix and is a pure-state, unitary relation,
even through there is ``dissipation'' from the perspective of mode $a$.
It's clear that Markov approximation doesn't make the system ``noisy'';
that's to say, Markov approximation doesn't necessitate mixed-state formalisms, 
like the quantum Langevin equation: 
we need to explicitly declare that the bath is indeed noisy to finally get the quantum Langevin equation. 
The scattering matrix element \eqref{eq:si-to-so} can actually be derived using Feynman diagrams: 
we are interested in $b(\omega)$-to-$b(\omega)$ scattering, 
and according to LSZ reduction formula, 
the corresponding $S$-matrix equals to one plus the sum of all amputated diagrams.
In the present case we only have one such diagram, 
which contains one $b$-to-$a$ vertex, one $a$ propagator, and one $a$-to-$b$ vertex, 
and the $a$ propagator has its own self-energy correction due to the presence of $b$ modes, 
which has been approximated by the Markov approximation.
Thus the $S$ matrix is 
\[
    \sim 1 + \left(-\ii \sqrt{\frac{2}{\tau}}\right) \frac{\ii}{\omega - \omega_0 + \frac{\ii}{\tau}} \left(-\ii \sqrt{\frac{2}{\tau}}\right),
\]
which evaluates to exactly \eqref{eq:si-to-so} 
(except one minus sign which comes from the definition of $\Si$ and $\So$).

We don't have any ``real'' mixed state 

\section{Supermodes}

Now consider two modes $a$ and $b$, 
each of which has a port linked to it.
Again assuming linear coupling between them only and turning off the ports for the moment, we have 
\begin{equation}
    \dot{a} = - \ii \omega_a a + K_{ab} b, \quad 
    \dot{b} = - \ii \omega_b b + K_{ba} a,
\end{equation}
and the energy conservation condition (or in other words, unitarity) is equivalent to
\begin{equation}
    K_{ba} = - K_{ab}^*.
\end{equation}

We can then diagonalize the dynamic matrix and obtain two resulting modes, 
often known as \concept{supermodes}.
In one of them $a, b$ move in the same direction at the same time, 
while in the other $a, b$ move in opposite directions at the same time.
What makes the supermodes different from ordinary modes is that 
the ports are attached to $a$ and $b$, not them, 
and therefore the time evolution of the supermodes may be rather exotic.
If we have two ports, then the phases of the two $\Si$ need to match in a specific way 
to excite one supermode. 
If $a$ and $b$ are attached to the same port, 
then their (spontaneous) radiations to the port cancel each other in one supermode 
while they enhances each other in another, 
so the first supermode is incredibly long lived 
while the second damps rather fast; 
similarly the first supermode will be incredibly hard to excite 
if we are only allowed to use the ports.

Consider one case: mode $a_2$ is connected to a port $(\Si, \So)$,
while mode $a_1$ is coupled to mode $a_2$.
When there is no input energy, the equation about the port is 
\begin{equation}
    \So = - \ii \sqrt{\frac{2}{\tau}} a_2.
\end{equation}
Further, assuming that we have no excitation in the ``minus'' supermode, we have 
\begin{equation}
    a_1 = a_2 = \frac{1}{\sqrt{2}} a_+, 
\end{equation}
and therefore the output power is  
\begin{equation}
    P_{\text{out}} = \abs*{\So}^2 = \frac{2}{\tau} \abs*{a_2}^2
    = \frac{2}{\tau} \frac{\abs*{a_+}^2}{2} ,
\end{equation}
and putting this to the EOM of the energy of $a_+$, 
we find the lifetime of $a_+$ is $2 \tau$, not $\tau$.

Then let's consider the case where two modes are coupled to one port.
The output flux now is 
\begin{equation}
    \So = \Si - \ii \sqrt{\frac{2}{\tau_1}} a_1 - \ii \sqrt{\frac{2}{\tau_2}} a_2,
    \label{eq:a1-a2-to-So}
\end{equation}
and again we ignore the input flux.
Note that we haven't derived any equation about the coupling between two modes and one port; 
in most experimental settings, we can divide a large port (say, a waveguide) 
into several small ports and apply the one-mode-one-port theory to each of them.
This leads to \eqref{eq:a1-a2-to-So}, but leaves the possibility that 
$S_{\text{o1}}$ and $S_{\text{o2}}$ have a phase difference, 
which is indeed the case for a waveguide 
if the positions where $a_1$ and $a_2$ are connected to the waveguide  
are far from each other.
Also if the two modes are in the same device, 
nothing prohibits us to have a non-diagonal matrix between $\So$ and $a$'s.
Here for the sake of simplicity we ignore this possibility.
Suppose we only have excitations in the $+$ supermode,
and this means 
\begin{equation}
    a_1 = a_2 = \frac{a_+}{\sqrt{2}}.
\end{equation}
The output flux is 
\begin{equation}
    \So = - \ii \left(\sqrt{\frac{1}{\tau_1}} + \sqrt{\frac{1}{\tau_2}}\right) \abs*{a_+}^2,
\end{equation}
and therefore the loss of the $+$ supermode is enhanced 
(just consider the case $\tau_{1,2} = \tau$).

On the other hand, for the $-$ supermode we have 
\begin{equation}
    a_1 = \frac{a_+}{\sqrt{2}}, \quad a_2 = - \frac{a_+}{\sqrt{2}},
\end{equation}
and $\So$ now is 
\begin{equation}
    \So = - \ii \left( \sqrt{\frac{1}{\tau_1}} - \sqrt{\frac{1}{\tau_2}} \right),
\end{equation}
and clearly when $\tau_1 = \tau_2$ we don't have damping of $a_-$ at all!

Modes that never damp and correspondingly can't be excited by $\Si$ 
are known as \concept{dark modes}.

\section{Scattering matrices}

Suppose we have a two-port system; 
$a_1$ and $a_2$ are input modes of the two ports, 
while $b_1$ and $b_2$ are output modes of the two ports.
The relation between $a_{1,2}$ and $b_{1,2}$ is called the scattering matrix of the system.
What it means by ``the relation between \dots'' needs some explanation.

This scattering matrix is not the same as 
the two-particle scattering scattering matrix:
if the two-port system is linear we still need a $2\times 2$ matrix 
to describe the relation between the input and output fields, 
because the input field consists of at least two modes ($a_{1, 2}$)
and so does the output field; 
the scattering matrix here however indeed is the same as the \emph{one-particle} $S$-matrix, 
although under a different basis.
The input modes are obtained by diagonalizing the free Hamiltonian 
(e.g. a Hamiltonian whose eigenstates are plane waves)
\emph{in the ``input'' subspace}, 
which means the Hilbert space form by collecting all waves going towards the scattering center, 
and the output modes are defined accordingly.
The normalization of the input and output wave functions should be chosen wisely such that  
\begin{equation}
    \ket*{\text{scattering stationary state}} = \ket*{\text{one input state}} + \sum c_i \ket*{\text{output states}}
\end{equation}
is already normalized under the inner product defined in the \emph{whole} Hilbert space when 
\begin{equation}
    \sum \abs*{c_i}^2 = 1.
\end{equation}
Of course, that means the amplitudes of the input and output wave functions are smaller than one.
For a one dimensional scattering problem where the scattering center is at $x=0$, 
this means a input state look like 
\begin{equation}
    \psi(x) = \frac{1}{\sqrt{L}} \begin{cases}
        \ee^{\ii k x} , & x < 0, \\
        0, & x > 0.
    \end{cases}
\end{equation}
where $L$ is the ``sample size'' and $- L/2 \leq x \leq L/2$.
Note that the normalization coefficient is not the one used in the half-space 
but the one used in the whole space.
Based on the settings above, 
it can be proved that the $S$ matrix defined in ordinary scattering matrix,
\[
    S = U(\infty, -\infty) = \sum_i \ee^{- \ii \omega_i t} \dyad{\text{stationary state $i$}}
\]
is exactly the same as what you get by arranging coefficients of output states  
in scattering stationary states that contain only one input state component. 

Until now we are working in the Schrodinger picture.
Turning to the Heisenberg picture, 
we know the time evolution of annihilation operators (and hence field operators) in the Heisenberg picture
is the same as the time evolution of the wave function in the Schrodinger picture 
(consider how $\mel**{0}{a}{\psi}$ involves, which is independent to the picture used),
and hence $S$ can also be seen as the time evolution operator of mode annihilation operators, 
and hence sometimes people write $b_i = S_{ij} a_j$, 
and thus the scattering matrix becomes ``the relation between the output modes and the input modes''.
This understanding of equations with the form of $b_i = S_{ij} a_j$
no longer works when two modes are connected together to form a close loop, 
but things like $b_i = S_{ij} a_j$ can still be understood 
as shorthands for analyses of \emph{wave functions} in a stationary state.

We also note that the scattering matrix formalism 
is itself a \emph{pure state} formalism:
in high-energy physics the scattering matrix is related to scattering experiments 
which involve measurement, 
but the scattering matrix itself is evaluated thoroughly in the pure state formalism.
This is also the case here.

Scattering matrices often have \concept{reciprocity}.
The reciprocal condition for the Green's function ${G}_\omega(\vb*{r}, \vb*{r}')$ is quite simple:
\begin{equation}
    G_\omega (\vb*{r}, \vb*{r}') = G_\omega (\vb*{r}', \vb*{r}).
\end{equation}
This condition means the \emph{transpose} of the scattering matrix is just the origin scattering matrix.
We note that this is \emph{not} time reversal symmetry.
In terms of the scattering matrix, reciprocity means 
\begin{equation}
    S^\top = S.
\end{equation}

The energy conservation condition is equivalent to unitarity.
This can be easily proven: the energy conservation condition is equivalent to  
\begin{equation}
    b^\dagger b = a^\dagger a,
\end{equation}
where $a$ is the column vector containing all input modes 
and $b$ is the column vector containing all output modes.
Since $b = S a$, we have 
\begin{equation}
    a^\dagger (S^\dagger S - I) a = 0,
\end{equation}
and hence (TODO: corner case)
\begin{equation}
    S^\dagger S = I.
\end{equation}

Time reversal symmetry is almost always true; 
of course when an external magnetic field is present 
we don't have time reversal symmetry, 
but this merely suggests at the necessity to change the direction of the magnetic field 
when doing the time reversal operation.
Here we define (TODO: equivalent definitions: the direction of $p$ is changed; or $\ii \to - \ii$)
\begin{equation}
    \mathrm{TR}[Q(t)] = Q^*(-t).
\end{equation}
Time reversal symmetry means 
\begin{equation}
    S^* = S^{-1}.
\end{equation}

There is a lot of abuse (or confusing usage) of the term \term{breaking the time reversal symmetry}.
As is mentioned, when we have a strong magnetic field 
it's often said that the time reversal symmetry is broken; 
when the magnetic field is included as a part of the system 
(not an arbitrary external field), 
time reversal symmetry isn't truly broken, 
but reciprocity is indeed broken.
Consider a circulator of electrons: 
if an electron is injected into it through one port 
and we record the port from which the electron goes out, 
an electron going into the second port doesn't go out of the first port.
Of course we can still say time reversal symmetry is broken here 
if we keep the magnetic field not flipped when we apply the time reversal operation. 

\section{Noise}

Often the modes going in and out in the ports are indeed noisy, 
and in this case the temporal coupled theory becomes Langevin-like; 
the Langevin-like theory can of course be derived from 
the intuition that energy loss shouldn't make a theory about $a$  non-unitary 
and some sort of noise has to be there to make the theory unitary, 
or the intuition that coupling with the external environment 
From equipartition theorem we can see that the power spectral density going into the system, 
i.e. the power per unit of frequency, is 
\begin{equation}
    \dv{P}{\omega} \simeq \frac{\kB T}{2 \pi}.
\end{equation}

Since now the modes in the ports are indeed noisy, 
we can study them using theories about stochastic signals.
To define the average of $s(t)$ or $s(t_1) s(t_2)$ or \dots,
we can either take the ensemble average 
(a probabilistic average according to the probabilistic distribution of trajectories) 
or time average (average over a large period of time).
In physics the most frequent types of noises are stationary, 
meaning that the the occasion we take the average doesn't matter, 
and ergodic, meaning that time average is equivalent to ensemble average. 

The (two-point) correlation function, $\expval{s^*(t_1) s(t_2)}$, 
can actually be measured directly experimentally.
We can adapt a interferometer to do it: 
instead of measuring the \emph{sum} of the two signals from two paths, 
we measure the \emph{product} of the two signals.
Because of the stationary nature of noises, 
we have 
\begin{equation}
    \expval{s^*(t) s(t + \tau)} = C_s(\tau).
\end{equation}

By Fourier transform, if we choose the following normalization: 
\begin{equation}
    s(t) = \int_{-T/2}^{T/2} \dd{\omega} s(\omega) \ee^{- \ii \omega t}, 
\end{equation}
we have 
\begin{equation}
    C_s(\tau) = \lim_{T \to \infty} \frac{2\pi}{T} 
    \int_{-\infty}^{\infty} \dd{\omega} \expval*{\abs*{s(\omega)}^2} \ee^{- \ii \omega \tau}
    \eqqcolon \int_{-\infty}^{\infty} \dd{\omega} C_s(\omega) \ee^{- \ii \omega \tau},
\end{equation}
and therefore 
\begin{equation}
    C_s(\omega) = \lim_{T \to \infty} \frac{2\pi}{T} \expval*{\abs*{s(\omega)}^2}
    = \frac{1}{\delta(0)} \expval*{\abs*{s(\omega)}^2}.
\end{equation}
This is the Wiener–Khinchin theorem in the specific case of time averaging.
The theorem can be proven as well with ensemble average; 
this is more easily done by starting from $\expval{s^*(\omega) s(\omega)}$. 
As $s(\omega)$ is just a mode in $s(t)$, 
this means the two-point correlation function and the power spectral density 
are related by Fourier transform.

This means from the power spectral density of a noise 
we can know a lot about its time correlation.
For example, a truly white noise has $\delta(\tau)$ time correlation.
Or conversely if we need a wide power spectral density we then need an extremely narrow pulse.
Note that $C_s(\omega)$ equals to $\expval{s^*(\omega) s(\omega')}$ divided by a common $\delta(\omega - \omega')$ factor:
as long as the system has time translational symmetry, 
in the frequency domain correlation function we always have a $\delta(\omega - \omega')$ factor.
It's what \emph{else} there is besides this $\delta$-function factor that $C_s(\omega)$ describes.

Now we turn to the temporal mode coupling theory. 
We still have 
\begin{equation}
    \dot{a} = - \ii \omega_0 a + \frac{a}{\tau} + \sqrt{\frac{2}{\tau}} \bi,
\end{equation}
where $\bi$ is just $\Si$.
The correlation function of $\bi$ can be derived from the equipartition theorem;
for a one-dimensional transmission line it's
\begin{equation}
    \expval{\bi^*(\omega) \bi(\omega')} = \frac{\kB T}{2 \pi} \delta(\omega - \omega'),
\end{equation}
because by definition we know 
\begin{equation}
    P = \expval*{\bi^*(t) \bi(t)} = C_{\bi}(t) = \int C_{\bi}(\omega) \dd{\omega} ,
\end{equation} 
and therefore 
\begin{equation}
    \dv{P}{\omega} = \frac{\kB T}{2\pi} \quad \Rightarrow \quad C_{\bi}(\omega) = \frac{\kB T}{2\pi},
\end{equation}
and from Wiener–Khinchin theorem we have 
\begin{equation}
    \expval*{\bi^*(\omega) \bi(\omega')} = \frac{\kB T}{2\pi} \delta(\omega - \omega').
\end{equation}

The correlation function of $a$ can be calculated from 
\begin{equation}
    a(\omega) = \sqrt{\frac{2}{\tau}} \frac{\bi(\omega)}{\ii (\omega_0 - \omega) + \frac{1}{\tau}}.
\end{equation} 
We find 
\begin{equation}
    \expval{a^*(\omega) a(\omega')} = \frac{1}{\tau} \frac{\kB T}{\pi} \frac{1}{(\omega - \omega_0)^2 + \frac{1}{\tau^2}} \delta(\omega - \omega').
\end{equation}
The equipartition theorem for $a$ can then be directly verified: we have 
\begin{equation}
    \int \int \dd{\omega} \dd{\omega'} \expval{a^*(\omega') a(\omega)} = \kB T.
\end{equation}
The spectrum of the reflected wave can be calculated as well. 
From 
\begin{equation}
    \bo = - \bi + \sqrt{\frac{2}{\tau}} a ,
\end{equation}
we get 
\begin{equation}
    \bo(\omega) = \frac{\frac{1}{\tau} - \ii (\omega_0 - \omega)}{\frac{1}{\tau} + \ii (\omega_0 - \omega)} \bi(\omega), 
\end{equation}
and 
\begin{equation}
    \expval{\bo^*(\omega') \bo(\omega)} = \frac{\kB T}{2 \pi} \delta(\omega - \omega').
\end{equation}
So the energy distribution of $\bo$ is exactly the same as the energy distribution of $\bi$.
The only difference between $\bi$ and $\bo$ is a phase factor.

Now we take a slightly different approach. 
If we know nothing about the port stuff, 
can we still get the correlation function of the noise term 
coming together with the dissipation term?
From 
\begin{equation}
    \dot{a} = - \ii \omega_0 a - \frac{a}{\tau} + \eta(t)
\end{equation}
we get 
\begin{equation}
    \dv{t} \abs*{a}^2 = - \frac{2}{\tau} \abs*{a} + (a^* \eta + \eta^* a).
\end{equation}
After taking the time average on both sides, 
since $\dv{t} \expval*{\abs*{a}^2} = 0$,
we find 
\begin{equation}
    \frac{2}{\tau} \expval{a^* a} = \expval{a^* \eta + \eta^* a}.
    \label{eq:aa-aeta}
\end{equation}
The relation between $a$ and $\eta$ is given by 
\begin{equation}
    a(t) = \int_{-\infty}^{t} \ee^{- \ii \tilde{\omega}_0 (t - t')} \sqrt{\frac{2}{\tau}} \eta(t') \dd{t'},
\end{equation}
where 
\begin{equation}
    \tilde{\omega}_0 = \omega - \frac{\ii}{\tau}.
\end{equation}
So now the RHS of \eqref{eq:aa-aeta} becomes an expression of $\expval{\eta(t) \eta(t')}$.
Now we have to introduce the approximation that 
\begin{equation}
    \expval{\eta(t) \eta(t')} = A \delta(t - t')
\end{equation}
to simply the following procedure. 
Using 
\[
    \int \dd{t} \Theta(t - t') \delta(t - t') = \frac{1}{2},
\]
we find 
\[
    \frac{2}{\tau} \expval{a^* a} = \frac{2}{\tau} \kB T = A,
\]
and eventually
\begin{equation}
    \expval{\eta(t) \eta(t')} = \frac{2}{\tau} \kB T \delta(t - t').
\end{equation}

\section{Input-output formalism}

Now we quantize the temporal coupling theory.
Since we were mostly working on physical quantity EOMs before,
the forms of the eventual effective EOMs should be exactly the same 
in classical and quantum systems.
The main difference is the interpretation of the variables.
For example, in the quantum case,
$\expval{a^\dagger a}$ has a clear meaning:
the number of photons in the mode $a$.

The Hamiltonian takes the form 
\begin{equation}
    H = \hbar \omega_0 a^\dag a + \sum_k \hbar \omega_k b_k^\dag b_k 
    - \hbar \sum_k \tilde{g}_k a^\dag b_{k} + \text{h.c.},
\end{equation}
and the EOMs are 
\begin{equation}
    \dot{b}_k = - \ii \omega_k b_k + \ii g_k^* a, \quad
    \dot{a} = - \ii \omega_0 + \ii \sum_k g_k b_k.
\end{equation}
The following derivation follows exactly what is done in the classical case; 
now however we want to make sense of what exactly are $\Si$ and $\So$.
Assume that the port is indeed a transmission line,
and $k$ used to label $b_k$ then is the wave vector.
Suppose the length of the port is $L$,
and we have 
\begin{equation}
    k = \frac{2\pi}{L} m, 
\end{equation}
where $m$ is an integer.
The real space $b$ is 
\begin{equation}
    b(x) = \frac{1}{\sqrt{L}} \sum_k \ee^{\ii k x} b_k,
\end{equation} 
and its ``value'', like the expectation value, 
gives the shape of wave packets in a quantum state.
Here the normalization is equivalent to assuming that 
\begin{equation}
    U_{\text{tot}} = \int \dd{x} \abs*{b(x, t)}^2 = \sum_k b^\dag_k b_k.
\end{equation}
Of course we can split an arbitrary wave packet into a right-mover and a left-mover,
and therefore we define 
\begin{equation}
    b_+(x) = \frac{1}{\sqrt{L}} \sum_{k>0} \ee^{\ii k x} b_k,
\end{equation}
and 
\begin{equation}
    b_-(x) = \frac{1}{\sqrt{L}} \sum_{k<0} \ee^{\ii k x} b_k.
\end{equation}

Going to the continuous limit, we make the following substitution:
\begin{equation}
    b(k)^{\text{continuous $k$}} = \frac{\sqrt{L}}{2\pi} b_k^{\text{discrete $k$}}.
\end{equation}
This leads to 
\begin{equation}
    b(x) = \int \dd{k} \ee^{\ii k x} b(k).
\end{equation}

Next we evaluate the noise spectrum. We have (here we work in the classical case)
\begin{equation}
    \begin{aligned}
        \expval{b^*(k) b(k')} &= \lim_{L \to \infty} \expval{
            \frac{\sqrt{L}}{2\pi} b_k^*
            \frac{\sqrt{L}}{2\pi} b_{k'} 
        } = \frac{1}{2\pi} \lim_{L \to \infty} \underbrace{\frac{L}{2\pi}}_{1 / \Delta k} \expval{b_k^* b_{k'}} \\
        &= \frac{1}{2\pi} \lim_{L \to \infty} \kB T \delta_{kk'} / \Delta k \\
        &= \frac{\kB T}{2\pi} \delta(k - k').
    \end{aligned}
\end{equation}

\section{Quantum noise}

Replacing $a$ and $b$ by their quantum operator counterparts 
leads us into the regime of quantum noise and fluctuation, in the Heisenberg picture.
This also relates the most natural way to express the rotating wave approximation
to the standard definition of rotating wave approximation in quantum mechanics, 
i.e. by applying a picture transform.

The key thing that needs to be highlighted is now dynamic variables don't always commute.
This means we don't always have $C(\tau) = C(-\tau)^\dag$ (TODO).
One way is to always work with $(AB + BA)/2$.

The time evolution operator of a time dependent Hamiltonian is 
\begin{equation}
    U = \timeorder \exp(- \frac{\ii}{\hbar} \int \dd{t'} H(t')).
\end{equation}
When the Hamiltonian is time dependent but we have 
\begin{equation}
    \comm*{H(t)}{H(t')} = 0,
\end{equation}
this can be simplified into 
\begin{equation}
    U = \exp(- \frac{\ii}{\hbar} \int \dd{t'} H(t')).
\end{equation}
An example of time dependent Hamiltonian commuting with itself at different times is  
\begin{equation}
    H = (\omega + \Delta \omega(t)) a^\dag a,
\end{equation}
while a Hamiltonian that doesn't commute with itself at a different time is 
\begin{equation}
    H = \omega a^\dag a + f(t) (a + a^\dag).
\end{equation}

From $\comm*{a}{a^\dag} = 1$ it follows the trivial result $\dv{t} \comm*{a(t)}{a(t)^\dag} = 0$.
This result helps us to determine some basic behaviors of the Langevin term in a system 
coupled to a Markovian external bath.
From the equation without the Langevin term
\[
    \dot{a} = - \ii \omega_0 a - \frac{1}{\tau} a 
\]
we get the troublesome commutation relation
\[
    \dv{t} \comm*{a}{a^\dag} = - \frac{2}{\tau} \comm*{a}{a^\dag},
\]
which is expected since unitarity is broken.
Now starting from 
\begin{equation}
    \dot{a} = - \ii \omega_0 a - \frac{1}{\tau} a + \eta 
\end{equation}
we get 
\begin{equation}
    \dv{t} \comm*{a}{a^\dag} = - \frac{2}{\tau} \comm*{a}{a^\dag} + \comm*{\eta}{a^\dagger} + \comm*{a}{\eta^\dag} ,    
\end{equation}
and since the LHS is zero, and therefore $\comm*{a}{a^\dag}$ is always unity,
we find 
\begin{equation}
    \comm*{\eta}{a^\dag} + \comm*{a}{\eta^\dag} = \frac{2}{\tau}.
\end{equation}
This is a condition that \emph{all} Langevin ``noises'' 
(or non-thermalized port operators) have to follow.

Actually we can even derive the Langevin equation under some assumptions.
We know there is a direct relation between $a$ and $\eta$, namely 
\begin{equation}
    a(t) = \int \Theta(t - t') \dd{t'} \ee^{- \ii (t - t') (\omega + \ii / \tau)} \eta(t'),
\end{equation}
and therefore 
\begin{equation}
    \comm*{a}{\eta} = \int_{0}^{t} \ee^{- \ii (\omega + \ii / \tau) (t - t')} \comm*{\eta(t')}{\eta(t)^\dag} \dd{t'}.
\end{equation}
If we assume (here is the assumption!) that 
\begin{equation}
    \comm*{\eta(t')}{\eta^\dag(t)} = D \delta(t - t'),
\end{equation}
we find (note that $\int_{-\infty}^{0} \delta(t) \dd{t} = 1/2$)
\[
    \comm*{a}{\eta} = \int_{0}^{t} \ee^{- \ii (\omega + \ii / \tau) (t - t')} D \delta(t - t') \dd{t'}
    = \frac{1}{2} D,
\]
and we eventually find 
\begin{equation}
    D = \frac{2}{\tau}, \quad 
    \comm*{\eta(t)}{\eta^\dag(t')} = \frac{2}{\tau} \delta(t - t').
\end{equation}

\end{document}