\documentclass[hyperref, a4paper]{article}

\usepackage{geometry}
\usepackage{titling}
\usepackage{titlesec}
% No longer needed, since we will use enumitem package
% \usepackage{paralist}
\usepackage{enumitem}
\usepackage{footnote}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathtools}
\usepackage{bbm}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{soulutf8}
\usepackage{physics}
\usepackage{tensor}
\usepackage{siunitx}
\usepackage[version=4]{mhchem}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{autobreak}
\usepackage[ruled, vlined, linesnumbered]{algorithm2e}
\usepackage{nameref,zref-xr}
\zxrsetup{toltxlabel}
\usepackage[backend=bibtex]{biblatex}
\addbibresource{elasticity.bib}
\usepackage[colorlinks,unicode]{hyperref} % , linkcolor=black, anchorcolor=black, citecolor=black, urlcolor=black, filecolor=black
\usepackage[most]{tcolorbox}
\usepackage{prettyref}

% Page style
\geometry{left=3.18cm,right=3.18cm,top=2.54cm,bottom=2.54cm}
\titlespacing{\paragraph}{0pt}{1pt}{10pt}[20pt]
\setlength{\droptitle}{-5em}

% More compact lists 
\setlist[itemize]{
    itemindent=17pt, 
    leftmargin=1pt,
    listparindent=\parindent,
    parsep=0pt,
}

% Math operators
\DeclareMathOperator{\timeorder}{\mathcal{T}}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\legpoly}{P}
\DeclareMathOperator{\primevalue}{P}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\res}{Res}
\DeclareMathOperator{\sinc}{sinc}
\newcommand*{\ii}{\mathrm{i}}
\newcommand*{\ee}{\mathrm{e}}
\newcommand*{\const}{\mathrm{const}}
\newcommand*{\suchthat}{\quad \text{s.t.} \quad}
\newcommand*{\argmin}{\arg\min}
\newcommand*{\argmax}{\arg\max}
\newcommand*{\normalorder}[1]{: #1 :}
\newcommand*{\pair}[1]{\langle #1 \rangle}
\newcommand*{\fd}[1]{\mathcal{D} #1}
\DeclareMathOperator{\bigO}{\mathcal{O}}

% TikZ setting
\usetikzlibrary{arrows,shapes,positioning}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{calc}
\tikzstyle arrowstyle=[scale=1]
\tikzstyle directed=[postaction={decorate,decoration={markings,
    mark=at position .5 with {\arrow[arrowstyle]{stealth}}}}]
\tikzstyle ray=[directed, thick]
\tikzstyle dot=[anchor=base,fill,circle,inner sep=1pt]

% Algorithm setting
% Julia-style code
\SetKwIF{If}{ElseIf}{Else}{if}{}{elseif}{else}{end}
\SetKwFor{For}{for}{}{end}
\SetKwFor{While}{while}{}{end}
\SetKwProg{Function}{function}{}{end}
\SetArgSty{textnormal}

\newcommand*{\concept}[1]{{\textbf{#1}}}

% Embedded codes
\lstset{basicstyle=\ttfamily,
  showstringspaces=false,
  commentstyle=\color{gray},
  keywordstyle=\color{blue}
}

% Reference formatting
\newcommand*{\citesec}[1]{\S~{#1}}
\newcommand*{\citechap}[1]{chap.~{#1}}
\newcommand*{\citefig}[1]{Fig.~{#1}}
\newcommand*{\citetable}[1]{Table~{#1}}
\newcommand*{\citepage}[1]{pp.~{#1}}
\newrefformat{fig}{Fig.~\ref{#1}}
\newcommand*{\term}[1]{\textit{#1}}

% Color boxes
\tcbuselibrary{skins, breakable, theorems}

\newtcbtheorem{infobox}{Box}{
    enhanced,
    boxrule=0pt,
    colback=blue!5,
    colframe=blue!5,
    coltitle=blue!50,
    borderline west={4pt}{0pt}{blue!65},
    sharp corners,
    fonttitle=\bfseries, 
    breakable,
    before upper={\parindent15pt\noindent}}{box}
\newtcbtheorem[use counter from=infobox]{theorybox}{Box}{
    enhanced,
    boxrule=0pt,
    colback=orange!5, 
    colframe=orange!5, 
    coltitle=orange!50,
    borderline west={4pt}{0pt}{orange!65},
    sharp corners,
    fonttitle=\bfseries, 
    breakable,
    before upper={\parindent15pt\noindent}}{box}
\newtcbtheorem[use counter from=infobox]{learnbox}{Box}{
    enhanced,
    boxrule=0pt,
    colback=green!5,
    colframe=green!5,
    coltitle=green!50,
    borderline west={4pt}{0pt}{green!65},
    sharp corners,
    fonttitle=\bfseries, 
    breakable,
    before upper={\parindent15pt\noindent}}{box}


\newenvironment{shelldisplay}{\begin{lstlisting}}{\end{lstlisting}}

\newcommand*{\kB}{k_{\text{B}}}
\newcommand*{\muB}{\mu_{\text{B}}}
\newcommand*{\efermi}{E_{\text{F}}}
\newcommand*{\pfermi}{p_{\text{F}}}
\newcommand*{\vfermi}{v_{\text{F}}}
\newcommand*{\sA}{\text{A}}
\newcommand*{\sB}{\text{B}}
\newcommand*{\Tc}{T_{\text{c}}}
\newcommand*{\hethree}{$^3$He}
\newcommand*{\hefour}{$^4$He}
\newcommand{\epsr}{\epsilon_{\text{r}}}
\newcommand{\chie}{\chi_{\text{e}}}
\newcommand*{\Gammae}{\Gamma_{\text{e}}}
\newcommand*{\Gammag}{\Gamma_{\text{g}}}
\newcommand*{\omegae}{\omega_{\text{e}}}
\newcommand*{\omegag}{\omega_{\text{g}}}
\newcommand*{\omegaeg}{\omega_{\text{eg}}}
\newcommand*{\ptwfc}[2]{\psi^{(#2)}_{#1}}
\newcommand*{\mueg}{\mu_{\text{eg}}}
\newcommand*{\muge}{\mu_{\text{ge}}}
\newcommand*{\Ezzero}{E_{z0}}
\newcommand*{\kete}{\ket*{\text{e}}}
\newcommand*{\ketg}{\ket*{\text{g}}}
\newcommand*{\coeffe}{c_{\text{e}}}
\newcommand*{\coeffg}{c_{\text{g}}}
\newcommand*{\pope}{p_{\text{e}}}
\newcommand*{\popg}{p_{\text{g}}}
\newcommand*{\ptwo}{P^{(2)}}
\newcommand*{\vp}{v_{\text{p}}}
\newcommand*{\vg}{v_{\text{g}}}
\newcommand*{\chitwo}{\chi^{(2)}}
\newcommand*{\chithree}{\chi^{(3)}}
\newcommand*{\omegap}{\omega_{\text{p}}}
\newcommand*{\Si}{S_{\text{i}}}
\newcommand*{\So}{S_{\text{o}}}
\newcommand*{\Tre}{\mathcal{T}}

\title{Topics in optics and quantum electronics}
\author{Jinyuan Wu}

\begin{document}

\maketitle

\section{Mode description of electromagnetic fields}

In this section we mainly focus on the mode description of classical electrodynamics.
Modes are seen in resonators, waveguides, photonic crystals, and more.
Loss can then be included perturbatively and we get leaky modes.

In the case of electrodynamics in vacuum, in the Coulomb gauge (i.e. $\varphi = 0$), we have 
\begin{equation}
    \curl\curl{\vb*{A}} = \left(\frac{\omega}{c}\right)^2 \vb*{A}, 
\end{equation}
and under the inner product definition 
\begin{equation}
    \braket*{\vb*{A}}{\vb*{B}} \coloneqq \int \dd{\vb*{r}} \vb*{A}^* \cdot \vb*{B},
\end{equation}
the LHS of the equation on $\vb*{A}$ is Hermitian, guaranteeing that $\omega$ is real.

From the well-known relation $\comm*{a}{a^\dagger} = 1$, 
it follows that the classical counterparts of creation and annihilation operators 
are complex variables which satisfies the relation 
\begin{equation}
    \poissonbracket{a}{a^*} = - \ii.
    \label{eq:a-astar-relation}
\end{equation}
This means if the Poisson brackets are to be defined in terms of $a$ and $a^*$, it should be defined as 
\begin{equation}
    \poissonbracket{A}{B} = \frac{1}{\ii} \left(\pdv{A}{a} \pdv{B}{a^*} - \pdv{A}{a^*} \pdv{B}{a}\right),
    \label{eq:poisson-ab}
\end{equation}
where $a$ and $a^*$ are to be regarded as independent variables.
The general relation between $a, a^*$ and $x, p$ is 
\begin{equation}
    a = \alpha q + \ii \beta p, \quad 
    q = \frac{a + a^*}{2 \alpha}, \quad p = \frac{a - a^*}{2 \ii \beta}, \quad 2 \alpha \beta = 1.
    \label{eq:a-astar-x-p-relation}
\end{equation}
If $q$ and $p$ follow the canonical commutation relation,
then $a$ and $a^*$ follow the commutation relation between 
a creation operator and a annihilation operator, and vice versa. 

We can also justify \eqref{eq:a-astar-relation} by Hamiltonian's equations.
From \eqref{eq:a-astar-x-p-relation} and the cian rule, we find 
\begin{equation}
    \begin{aligned}
    \dot{a} &= \pdv{a}{q} \dot{q} + \pdv{a}{p} \dot{p}
    = \pdv{a}{q} \pdv{H}{p} - \pdv{a}{p} \pdv{H}{q} \\
    &= \pdv{a}{q} \left(\pdv{H}{a} \pdv{a}{p} + \pdv{H}{a^*} \pdv{a^*}{p}\right)
    - \pdv{a}{p}  \left(\pdv{H}{a} \pdv{a}{q} + \pdv{H}{a^*} \pdv{a^*}{q}\right) \\
    &= - 2 \ii \alpha \beta \pdv{H}{a^*},
    \end{aligned}
\end{equation}
and similarly 
\begin{equation}
    \dot{a}^* = 2 \ii \alpha \beta \pdv{H}{a}.
\end{equation}
Now that means we have to define the Poisson bracket as  \eqref{eq:poisson-ab}
to keep the form 
\begin{equation}
    \dv{A}{t} = \poissonbracket{A}{H}.
\end{equation}

The $2 \alpha \beta = 1$ constraint can be explained by conservation of area in phase space: 
roughly speaking the area inside the trajectory of $a(t)$ or $(q(t), p(t))$ 
is the ``strength of oscillation'', like the number of photons.

Now going back to the problem of electromagnetic fields.
Consider the one dimensional periodic boundary condition problem.
We have (in phaser form)
\begin{equation}
    \tilde{E}_\pm(x, t) = \tilde{E}_\pm^0 \ee^{\ii k x - \ii \omega t}, 
\end{equation}
and we consider $A$ to be the coordinate and hence $-E = \pdv*{A}{t}$ is the momentum.
To decompose $A$ into modes, we write $A$ as static, spatially varying factors weighted by time-dependent real values,
and thus $A = q(t) A_0 \ee^{\ii k z}$, 

\section{Perturbation theory}

Stationary electrodynamics in terms of $\vb*{E}$ is not a Hermitian problem, 
but a generalized Hermitian one:
the problem is 
\begin{equation}
    \curl{\curl{\vb*{E}}} = \left(\frac{\omega}{c}\right)^2 \epsr(\vb*{r}) \vb*{E}(\vb*{r}),
\end{equation}
and although the operators on both LHS and RHS are Hermitian, 
the LHS multiplied with the inverse of the RHS is not.
The time-independent perturbation theory for this problem is therefore slightly more complicated, 
and the ``energy'' being solved is 
\begin{equation}
    \alpha \coloneqq \frac{\omega^2}{c^2},
\end{equation}
instead of the frequency. 
\begin{equation}
    \Delta \omega^{(1)}_n = - \frac{\omega_n^{(0)}}{2} \mel*{\tilde{\vb*{E}}_n}{\epsilon_0 \Delta \epsr}{\tilde{\vb*{E}}_n}.
    \label{eq:first-order-delta-eps}
\end{equation}
The minus sign can be understood using the following line of argumentation:
if a piece of dielectric is inserted into an isolated system, 
the total energy is conserved but some energy is stored 
within the internal degree of freedom dielectric, 
so $\omega (a^\dagger a + 1/2)$, the vacuum part of the Hamiltonian has to decrease.
One important caveat is that the leading order of energy correction 
\emph{can't} be evaluated by $\mel*{\vb*{E}_n}{\Delta \epsr}{\vb*{E}_n}$:
since the mapping from the electric field to the magnetic field 
involves at least one time derivative, 
the electromagnetic Hamiltonian in the form of $\vb*{E}^2 + \vb*{B}^2$
can't be seen as an ordinary Hamiltonian whose corresponding wave functions are electric fields, 
and therefore ordinary perturbation techniques fail.
One way to work around this is doing everything in terms of modes.
This is done in homework 2. 

\section{Loss as perturbation}

Now we study the influence of $\epsilon_2$, the imaginary part of $\epsr$, as a perturbation.
We can just inject $\Delta \epsr = \ii \epsilon_2$ into \eqref{eq:first-order-delta-eps}, 
and we get 
\begin{equation}
    \Delta \omega_n^{(1)} = - \ii \frac{1}{\tau_n}, \quad 
    \frac{1}{\tau_n} = \frac{\omega_n}{2} \mel*{\tilde{\vb*{E}}_n}{\epsilon_0 \epsilon_2}{\tilde{\vb*{E}}_n}.
\end{equation}
This means the time evolution of the corresponding mode $a_n$ contains a $\ee^{- t / \tau_n}$ factor, 
and the EOM seems to be 
\begin{equation}
    \dot{a} = - \ii \omega a - \frac{1}{\tau} a.
    \label{eq:eom-a-0}
\end{equation}
We can add a driving field to the RHS of this equation of motion; 
without considering any nonlinear effects,%
\footnote{
    Damping always comes from coupling of the system with a much larger, external bath.
    The first order correction from the bath 
    adds a finite imaginary part to the self energy of the system; 
    the interaction with the bath of course also modifies the 
    coupling between the bath and the system, 
    but this is not first-ordered, 
    and can be ignored when damping is small.
}
the lifetime remains the same regardless of the possible external driving.
The question is whether \eqref{eq:eom-a-0} is truly the correct equation 
for a damped system without external driving.
It is of course possible to have a different kind of damping:
if we choose to not start from the imaginary part of $\epsr$ 
and instead insert a $- k v$ type of damping into the EOM of $p$ and $q$, 
then the damping term for $a$ should assumes the form of 
\begin{equation}
    \dot{a} + \ii \omega_0 a = - \frac{a}{\tau} + \frac{a^*}{\tau}.
    \label{eq:eom-a-0-coupled}
\end{equation}
Interestingly, from an equally intuitive starting point, 
now we get a set of coupled EOMs for $a$ and $a^*$, 
while in \eqref{eq:eom-a-0}, the EOMs for $a, a^*$ are not coupled.

The ``real'' form of the damping term depends on how the system couples to the bath; 
if we are studying a harmonic oscillator then \eqref{eq:eom-a-0-coupled} is exact, 
while \eqref{eq:eom-a-0} is not; 
the most natural way to include loss in Maxwell's equations also leads to 
something similar to \eqref{eq:eom-a-0-coupled}.
So indeed, although \eqref{eq:eom-a-0} seems quite natural, 
it's not exact, and the question is how it's related to the exact equations.

The answer is rotating wave approximation. 
From the usual procedure we will find the coupled terms in the loss are related to fast oscillation terms, 
which can be ignored for the low frequency part of $a, a^*$. 
This is valid as long as there is separation between the time scales of damping and of oscillation.
This is the condition of the validity of RWA of an external driving field, 
which, in the latter case, means the frequency of driving should be close to $\omega_0$
while the driving \emph{amplitude} should be small.
Since the damping term is not oscillating, 
the validity condition of RWA for damping is simpler.

TODO: does rotating wave approximation change the way the system couple to external fields?
(c.f. general relativity and the $f(R)$ theory)

\section{Mode coupling}

Consider a ring resonator. 
We have a right mover mode and a left mover mode, 
which we refer to as 
\begin{equation}
    \tilde{E}_k = a_k E_k^0 \ee^{\ii k z}
\end{equation}
and 
\begin{equation}
    \tilde{E}_{-k} = a_{-k} E_{-k}^0 \ee^{- \ii k z},
\end{equation}
respectively. 

Now suppose we have such a field configuration:
\begin{equation}
    E = a_k \tilde{E}_k + \text{c.c.} + a_{-k} \tilde{E}_{-k},
\end{equation}
and there is a $\Delta \epsr$ perturbation in the system.
The change of the frequency is therefore given by  
\begin{equation}
    \Delta \omega^{(1)} = - \frac{\omega^{(0)}}{2} \expval{\Delta \epsr}
    \propto (a_k^* + a_{-k}) (a_k^* + a_{-k}) + \text{c.c.},
\end{equation}
and therefore the contribution of $\Delta \epsr$ to the energy of the system looks like
\begin{equation}
    \Delta E = - \tilde{\gamma} (a_k^* + a_{-k}) (a_k^* + a_{-k}) + \text{c.c.} 
\end{equation}
The EOM of $a_k$ therefore is (TODO: formalism)
\begin{equation}
    \dot{a}_k = - \ii \omega_k a_k + 2 \ii \gamma (a_k^* + a_{-k}).
\end{equation}
Again we can apply RWA and remove the ``fast'' $a_k^*$ term, 
so eventually the EOMs of the two modes are (TODO: RWA without external driving)
\begin{equation}
    \dot{a}_k = - \ii \omega_0 a_k + \ii \tilde{g} a_{-k}, \quad 
    \dot{a}_{-k} = - \ii \omega_0 a_{-k} + \ii \tilde{g}^* a_{k}, \quad 
    \tilde{g} = 2 \gamma.
\end{equation}
The EOMs are still within the Hamiltonian dynamics framework, 
so we can diagonalize them and find that when 
\begin{equation}
    \Delta \epsr = \Delta \epsilon \cos(2kz),
\end{equation}
the new mode 
$a_{k} + a_{-k}$ has a lower frequency $\omega_0 - \abs*{\tilde{g}}$
while the new mode $a_{k} - a_{-k}$ has a higher frequency $\omega_0 + \abs*{\tilde{g}}$.
But the above formalism is not restricted to this perturbation: 
whatever $\Delta \epsr$ is, the way it affects the two modes we consider here 
is by influencing $\tilde{g}$.
Specifically, $\abs*{\tilde{g}}$ can be used to estimate the roughness of a ring resonator.

Of course, we can also perturb the system by modulation in \emph{time}.
Consider a classic $LC$-circuit, for example:
it cam be verified that an oscillating \(C\) results in 
\begin{equation}
    H = \omega_0 a^* a + \frac{\gamma(t)}{2} (a + a^*)^2,
\end{equation}
because there is no \(L^2\) term that cancels the \(a a^*\) terms.
The resulting EOMs are 
\begin{equation}
    \dot{a} = - \ii \omega_0 a - \ii \gamma(t) (a + a^*), 
\end{equation}
and the perturbation $\gamma(t)$ usually takes the form like 
\begin{equation}
    \gamma(t) = \frac{\tilde{\gamma}_0}{2} \ee^{- \ii \Omega t} + \text{c.c.}
\end{equation}
where $\Omega = 2 \omega_0$.
After RWA the only term that survives is 
\begin{equation}
    \dot{a} = - \ii \omega_0 a - \ii \frac{\tilde{\gamma}_0}{2} \ee^{- \ii \Omega t} a^*
\end{equation}
and hence we find 
\begin{equation}
    \dv{t} \bmqty{a \ee^{- \ii \omega_0 t} \\ a^* \ee^{\ii \omega t}} = \bmqty{
        0 & - \ii \frac{\tilde{\gamma}_0}{2} \\
        \ii \frac{\tilde{\gamma}_0^*}{2} & 0 
    } \bmqty{a \ee^{- \ii \omega_0 t} \\ a^* \ee^{\ii \omega t}}
\end{equation}
We can easily see that the ``source'' of the $\ee^{- \ii \Omega t}$ term is $a^*$, 
which in turn comes from the $\ee^{\ii \Omega t}$ term and $a$.
This eventually gives us a feedback (usually \emph{positive} but not always -- see below) to $a$,
leading to the increase of the total energy.
The derivative of the ``free'' energy is 
\begin{equation}
    \dot{H}_{\text{SHO}} = \Re (- \ii \abs*{\tilde{\gamma}_0} \ee^{\ii \phi} a_0), 
\end{equation}
where $\phi$ is the phase of $\tilde{\gamma}_0$, 
and $a_0$ is the slow oscillating part of $a$.
This expression tells us two things.
First, the phase of the pumping matters: 
it can be chosen in a specific way to \emph{reduce} the total energy.
Second, the growth or damping of the total energy is exponential, 
and therefore if at first you don't have a finite amplitude, 
there is no growth at all.
To start the growth, noise is important; 
classically this comes from thermal fluctuation; 
in the quantum case, the above EOM involves some terms from non-trivial commutation relations 
and we may say the growth is started by the zero-point energy.

\section{Nonlinearity}

We know the free $\epsilon_0 \epsr \vb*{E}^2 / 2 + \vb*{B}^2 / 2 \mu_0$ Hamiltonian 
can be written as $\sum \omega a^* a$; 
similarly a nonlinear Hamiltonian can be written as 
$\sum a a ^* + \text{c.c.}$; 
in principle all modes can be mixed together by nonlinearity; 
usually however only transitions satisfy the phase matching condition are strong enough;
the definition of ``phase matching'' is somehow unclear: 
for plain waves, it means $\sum \vb*{k} = 0$ 
and has to be satisfied \emph{strictly}; 
but in other cases, like in a photonic crystal, 
if $\vb*{k}$ means Bloch wave vectors, then the  
TODO: again, if a transition channel doesn't satisfy the phase matching condition, 
does it mean this channel vanish in reciprocal space?

Phase matching in the time domain has to be exactly satisfied 
only when we are dealing with a scattering problem, 
in which we integrate from $t=-\infty$ to $t = \infty$ 
and get a $\sum \omega = 0$ condition; 
for a finite time problem, this condition doesn't need to be exactly satisfied, 
although a transition channel satisfying the $\sum \omega = 0$ condition 
still likely to be the most influential one.
In Feynman diagrams we do have strict frequency conservation conditions at each vertex
but then we don't have the on-shell condition for internal lines; 
by the correspondence between Feynman diagrams and Goldstone diagrams, 
we can let the internal lines to satisfy the on-shell conditions 
but let the total energy of the system to be off-shell, 
therefore arriving at results obtained from the EOM approach.

The EOM of one mode looks like (here NL means ``nonlinear'')
\begin{equation}
    \dot{a}_1 = - \ii \omega_0 a_1 - \ii \pdv{H_{\text{NL}}}{a_1^*}.
\end{equation}
For SHG, for example, we have 
\begin{equation}
    \begin{aligned}
        \dot{a}_1 &= - \ii \omega_1 a_1 + \ii \tilde{\gamma}   a_1^* a_2, \\
        \dot{a}_2 &= - \ii \omega_2 a_2 + \ii \tilde{\gamma}^* a_1   a_1.
    \end{aligned} 
\end{equation}

We can also study a nonlinear system using nonlinear Maxwell's equations.
This can be seen as a cross-verification of the correctness 
of the mode-based, usually somehow simplified formalism;
when the dielectric function has a strong ``dispersion'' -- i.e. frequency dependence -- 
the Hamiltonian approach is also hard to use.

\section{Temporal module coupling}

Now we consider the coupling of modes with the external world.
The formalism is almost identical to the so-called input-output formalism in quantum optics.
As a motivation, consider the following model: 
imagine we have a one-dimensional photonic crystal, 
with one defect (for example a missing cavity in a string of cavities).
Now if a pulse is injected from one end of the photonic crystal 
we expect to see reflectivity and radiation to the 3D space around the defect.
Whatever the defect is, it can be modeled as a mode (or several modes)
coupled with ``ports'' around it.
In the absence of the ports, the EOM of the system always takes the form 
$\dot{a} = - \ii \omega_0 a$, 
or possibly with a decay. 
Now we consider the structure of a port. 
We represent the waves going ``into'' the defect as $\Si$
and the waves going ``out'' of the defect as $\So$.
Both variables may be linear combination of modes, 
or even contain some nonlinear effects.
The EOM of the defect then is (TODO: Peter says it only holds under RWA) 
\begin{equation}
    \dot{a} = - \ii \omega_0 a - \frac{a}{\tau} + K \Si.
\end{equation}
Here we do a Langevin-like approximation and assume that the dissipation term 
faithfully captures the coupling of the system with the output port.
Of course it's possible to have ``dissipative loss'', 
which means we have modes that are not considered as a part of the port 
but are nevertheless coupled to $a$ and damp it;%
\footnote{
    Sometimes this kind of loss is known as \concept{internal loss}; 
    note that here the term \term{internal} means ``not a part of the system we are interested in''. 
}
here we ignore these dissipation channels and assume that 
dissipation mainly comes from the output modes of the port.
Now if we somehow manage to integrate out $a$ 
and consider the theory of the port.
Ignoring all nonlinear effects and retardation effects, 
the relation between $\So$ and $\Si, a$ can only take the following form: 
\begin{equation}
    \So = \beta \Si + \gamma a, 
\end{equation}
where $\beta$ and $\gamma$ have to be related to $\tau$.

When $\Si, \So$ are appropriately normalized we expect to see that 
$\abs*{\Si}^2$ is the power incident while 
$\abs*{\So}^2$ is the output power.
For example, suppose we have a ring resonator, and we place a defect in it.
Since a ring resonator is usually thin, 
it can be modeled as a pipe with periodic boundary condition, 
and the power going through an arbitrary point in the ring resonator takes the form of 
\begin{equation}
    P = \vg \frac{\omega_0 a^* a}{L}, 
\end{equation}
where $a$ is a mode in the ring resonator (not the mode around the defect),
and therefore $S$ should be defined such that 
\begin{equation}
    P = S S^* = \vg \frac{\omega_0 a^* a}{L}.
\end{equation}
Moreover we normalize $a$ so that $\abs*{a}^2$ is the mode energy.

Now we seriously consider how to derive the relation between $\tau$ and $\gamma, K$.
The most famous example for coupling with outgoing modes leading to dissipation 
is arguably the fact that an infinite network of LC circuits is equivalent to a resistor.
Similar systems include a mass point connected to an infinite spring or a leaky cavity.
When there is no input to the defect mode, 
the loss of energy in the mode equals the outgoing flux $\So$, and therefore 
\begin{equation}
    \dv{U_{\text{defect}}}{t} = - \frac{2}{\tau} U_{\text{defect}}
    = - \abs*{\So}^2,
\end{equation}
and therefore 
\begin{equation}
    \So = \underbrace{\sqrt{\frac{2}{\tau}} \ee^{\ii \alpha}}_\gamma a.
\end{equation}
The phase factor is a variable that depends on the exact definition of the output of the system;
consider for example a defect embedded into a wave guide: 
we need to choose a point at which we decide ``we are no longer near the defect'', 
and the definition -- hence the phase -- of the outgoing mode depends on this point.
TODO: what happens when the input is turned on? 
We will get something like $2\Re K S_i^* a = \abs*{S_i}^2$.
Whether this is always true can be verified using a harmonic oscillator; 
essentially that's about what is the definition of ``outgoing''. 
Using MBPT this can be easily justified:
when everything is linear, couplings between the $\Si$ and $\So$ modes do not influence each other.

We still have $K$ and $\beta$. Since $a, \Si, \So$ are assumed to form a closed system, 
we expect they form a system with time reversal symmetry.
After time  reversal operation, hence the EOM of $a$ becomes 
\begin{equation}
    -\dv{t} a^*(-t) = - \ii \omega_0 a^*(-t) - \frac{a^*(-t)}{\tau} + \Tre{K S_{\text{i}}},
\end{equation} 
where we don't immediately write what's the time reverse of $S_{\text{i}}$:
time reversal operation also changes the mode label, 
and the correct transform is $\Tre{S_{\text{i}}} = \So^*(-t)$
(in principle we can put an additional phase factor between $\Tre \Si$ and $\So^*$; 
for convenience this is not done here), 
so eventually 
\begin{equation}
    -\dv{t} a^*(-t) = - \ii \omega_0 a^*(-t) - \frac{a^*(-t)}{\tau} + K \So^*(-t),
\end{equation}
and therefore 
\begin{equation}
    \dv{t} a(-t) = - \ii \omega_0 a(-t) + \frac{a(-t)}{\tau} - K^* \So(-t).
\end{equation}
Similarly 
\begin{equation}
    \Si(-t) = \beta^* \So(-t) + \gamma^* a(-t).
\end{equation}
Note that $\Tre$ doesn't change the coefficients but does turn $a$ to $a^*$ 
because the lesser are complex fields.
Now the EOMs should be equivalent to the EOMs before time reversal operation, 
and when the input modes are turned off, from 
\[
    \So(-t) = - \frac{\gamma^*}{\beta^*} a(-t) = - \frac{\ee^{- \ii \alpha}}{\beta^*} \sqrt{\frac{2}{\tau}} a(-t)
\]
we find 
\begin{equation}
    \beta = - \ee^{2 \ii \alpha}.
\end{equation}
Similarly by comparing the EOMs of $a$, when the input is turned off, we have 
\[
    \dv{t} a(-t) = - \ii \omega_0 a(-t) + \frac{a(-t)}{\tau} 
    - K^* \ee^{\ii \alpha} \sqrt{\frac{2}{\tau}} a(-t) ,
\] 
and since we also expect to see 
\[
    \dv{t} a(-t) = \frac{1}{\tau} a(-t),
\]
we have 
\begin{equation}
    K = \ee^{\ii \alpha} \sqrt{\frac{2}{\tau}},
\end{equation} 
and similarly
\begin{equation}
    \gamma = \ee^{\ii \alpha} \sqrt{\frac{2}{\tau}}.
\end{equation}
The most general EOMs therefore are 
\begin{equation}
    \dot{a} = - \ii \omega_0 a - \frac{a}{\tau} + \ee^{\ii \alpha} \sqrt{\frac{2}{\tau}} \Si, \quad 
    \So = - \ee^{2 \ii \alpha} \Si + \ee^{\ii \alpha} \sqrt{\frac{2}{\tau}} a.
\end{equation}
The positions of the $\ee^{\ii \alpha}$ factors are quite clear:
it means the time it takes for the input current to arrive at $a$ and goes back.
The minus sign reminds us of the phase shift in electromagnetic wave reflection.
For the single-mode-single-port case,
since $\alpha$ can be shifted by redefining the modes, 
usually either we choose $\ee^{\ii \alpha} = 1$ or $\ee^{\ii \alpha} = -1$.
In the former convention we have 
\begin{equation}
    \dot{a} = - \ii \omega_0 a - \frac{a}{\tau} + \sqrt{\frac{2}{\tau}} \Si, \quad 
    \So = - \Si + \sqrt{\frac{2}{\tau}} a. 
\end{equation}


From the equations concepts like impedance can then be defined. 
We can easily extend the above EOMs to the case with multiple ports, 
and the only thing needed is to sum over all $1/\tau$ terms and $\Si$ terms, 
while keeping the $\So = - \Si + \sqrt{2/\tau} a$ equation for each port 
(assuming that we only have hidden degrees of freedom that make the ports interact with each other).

\section{Supermodes}

Now consider two modes $a$ and $b$, 
each of which has a port linked to it.
Again assuming linear coupling between them only and turning off the ports for the moment, we have 
\begin{equation}
    \dot{a} = - \ii \omega_a a + K_{ab} b, \quad 
    \dot{b} = - \ii \omega_b b + K_{ba} a,
\end{equation}
and the energy conservation condition (or in other words, unitarity) is equivalent to
\begin{equation}
    K_{ba} = - K_{ab}^*.
\end{equation}

We can then diagonalize the dynamic matrix and obtain two resulting modes, 
often known as \concept{supermodes}.
In one of them $a, b$ move in the same direction at the same time, 
while in the other $a, b$ move in opposite directions at the same time.
What makes the supermodes different from ordinary modes is that 
the ports are attached to $a$ and $b$, not them, 
and therefore the time evolution of the supermodes may be rather exotic.
If we have two ports, then the phases of the two $\Si$ need to match in a specific way 
to excite one supermode. 
If $a$ and $b$ are attached to the same port, 
then their (spontaneous) radiations to the port cancel each other in one supermode 
while they enhances each other in another, 
so the first supermode is incredibly long lived 
while the second damps rather fast; 
similarly the first supermode will be incredibly hard to excite 
if we are only allowed to use the ports.

Consider one case: mode $a_2$ is connected to a port $(\Si, \So)$,
while mode $a_1$ is coupled to mode $a_2$.
When there is no input energy, the equation about the port is 
\begin{equation}
    \So = - \ii \sqrt{\frac{2}{\tau}} a_2.
\end{equation}
Further, assuming that we have no excitation in the ``minus'' supermode, we have 
\begin{equation}
    a_1 = a_2 = \frac{1}{\sqrt{2}} a_+, 
\end{equation}
and therefore the output power is  
\begin{equation}
    P_{\text{out}} = \abs*{\So}^2 = \frac{2}{\tau} \abs*{a_2}^2
    = \frac{2}{\tau} \frac{\abs*{a_+}^2}{2} ,
\end{equation}
and putting this to the EOM of the energy of $a_+$, 
we find the lifetime of $a_+$ is $2 \tau$, not $\tau$.

Then let's consider the case where two modes are coupled to one port.
The output flux now is 
\begin{equation}
    \So = \Si - \ii \sqrt{\frac{2}{\tau_1}} a_1 - \ii \sqrt{\frac{2}{\tau_2}} a_2,
    \label{eq:a1-a2-to-So}
\end{equation}
and again we ignore the input flux.
Note that we haven't derived any equation about the coupling between two modes and one port; 
in most experimental settings, we can divide a large port (say, a waveguide) 
into several small ports and apply the one-mode-one-port theory to each of them.
This leads to \eqref{eq:a1-a2-to-So}, but leaves the possibility that 
$S_{\text{o1}}$ and $S_{\text{o2}}$ have a phase difference, 
which is indeed the case for a waveguide 
if the positions where $a_1$ and $a_2$ are connected to the waveguide  
are far from each other.
Also if the two modes are in the same device, 
nothing prohibits us to have a non-diagonal matrix between $\So$ and $a$'s.
Here for the sake of simplicity we ignore this possibility.
Suppose we only have excitations in the $+$ supermode,
and this means 
\begin{equation}
    a_1 = a_2 = \frac{a_+}{\sqrt{2}}.
\end{equation}
The output flux is 
\begin{equation}
    \So = - \ii \left(\sqrt{\frac{1}{\tau_1}} + \sqrt{\frac{1}{\tau_2}}\right) \abs*{a_+}^2,
\end{equation}
and therefore the loss of the $+$ supermode is enhanced 
(just consider the case $\tau_{1,2} = \tau$).

On the other hand, for the $-$ supermode we have 
\begin{equation}
    a_1 = \frac{a_+}{\sqrt{2}}, \quad a_2 = - \frac{a_+}{\sqrt{2}},
\end{equation}
and $\So$ now is 
\begin{equation}
    \So = - \ii \left( \sqrt{\frac{1}{\tau_1}} - \sqrt{\frac{1}{\tau_2}} \right),
\end{equation}
and clearly when $\tau_1 = \tau_2$ we don't have damping of $a_-$ at all!

Modes that never damp and correspondingly can't be excited by $\Si$ 
are known as \concept{dark modes}.

\section{Scattering matrices}

Suppose we have a two-port system; 
$a_1$ and $a_2$ are input modes of the two ports, 
while $b_1$ and $b_2$ are output modes of the two ports.
The relation between $a_{1,2}$ and $b_{1,2}$ is called the scattering matrix of the system.
What it means by ``the relation between \dots'' needs some explanation.

This scattering matrix is not the same as 
the two-particle scattering scattering matrix:
if the two-port system is linear we still need a $2\times 2$ matrix 
to describe the relation between the input and output fields, 
because the input field consists of at least two modes ($a_{1, 2}$)
and so does the output field; 
the scattering matrix here however indeed is the same as the \emph{one-particle} $S$-matrix, 
although under a different basis.
The input modes are obtained by diagonalizing the free Hamiltonian 
(e.g. a Hamiltonian whose eigenstates are plane waves)
\emph{in the ``input'' subspace}, 
which means the Hilbert space form by collecting all waves going towards the scattering center, 
and the output modes are defined accordingly.
The normalization of the input and output wave functions should be chosen wisely such that  
\begin{equation}
    \ket*{\text{scattering stationary state}} = \ket*{\text{one input state}} + \sum c_i \ket*{\text{output states}}
\end{equation}
is already normalized under the inner product defined in the \emph{whole} Hilbert space when 
\begin{equation}
    \sum \abs*{c_i}^2 = 1.
\end{equation}
Of course, that means the amplitudes of the input and output wave functions are smaller than one.
For a one dimensional scattering problem where the scattering center is at $x=0$, 
this means a input state look like 
\begin{equation}
    \psi(x) = \frac{1}{\sqrt{L}} \begin{cases}
        \ee^{\ii k x} , & x < 0, \\
        0, & x > 0.
    \end{cases}
\end{equation}
where $L$ is the ``sample size'' and $- L/2 \leq x \leq L/2$.
Note that the normalization coefficient is not the one used in the half-space 
but the one used in the whole space.
Based on the settings above, 
it can be proved that the $S$ matrix defined in ordinary scattering matrix,
\[
    S = U(\infty, -\infty) = \sum_i \ee^{- \ii \omega_i t} \dyad{\text{stationary state $i$}}
\]
is exactly the same as what you get by arranging coefficients of output states  
in scattering stationary states that contain only one input state component. 

Until now we are working in the Schrodinger picture.
Turning to the Heisenberg picture, 
we know the time evolution of annihilation operators (and hence field operators) in the Heisenberg picture
is the same as the time evolution of the wave function in the Schrodinger picture 
(consider how $\mel**{0}{a}{\psi}$ involves, which is independent to the picture used),
and hence $S$ can also be seen as the time evolution operator of mode annihilation operators, 
and hence sometimes people write $b_i = S_{ij} a_j$, 
and thus the scattering matrix becomes ``the relation between the output modes and the input modes''.
This understanding of equations with the form of $b_i = S_{ij} a_j$
no longer works when two modes are connected together to form a close loop, 
but things like $b_i = S_{ij} a_j$ can still be understood 
as shorthands for analyses of \emph{wave functions} in a stationary state.

We also note that the scattering matrix formalism 
is itself a \emph{pure state} formalism:
in high-energy physics the scattering matrix is related to scattering experiments 
which involve measurement, 
but the scattering matrix itself is evaluated thoroughly in the pure state formalism.
This is also the case here.

Scattering matrices often have \concept{reciprocity}.
The reciprocal condition for the Green's function ${G}_\omega(\vb*{r}, \vb*{r}')$ is quite simple:
\begin{equation}
    G_\omega (\vb*{r}, \vb*{r}') = G_\omega (\vb*{r}', \vb*{r}).
\end{equation}
This condition means the \emph{transpose} of the scattering matrix is just the origin scattering matrix.
We note that this is \emph{not} time reversal symmetry.
In terms of the scattering matrix, reciprocity means 
\begin{equation}
    S^\top = S.
\end{equation}

The energy conservation condition is equivalent to unitarity.
This can be easily proven: the energy conservation condition is equivalent to  
\begin{equation}
    b^\dagger b = a^\dagger a,
\end{equation}
where $a$ is the column vector containing all input modes 
and $b$ is the column vector containing all output modes.
Since $b = S a$, we have 
\begin{equation}
    a^\dagger (S^\dagger S - I) a = 0,
\end{equation}
and hence (TODO: corner case)
\begin{equation}
    S^\dagger S = I.
\end{equation}

Time reversal symmetry is almost always true; 
of course when an external magnetic field is present 
we don't have time reversal symmetry, 
but this merely suggests at the necessity to change the direction of the magnetic field 
when doing the time reversal operation.
Here we define (TODO: equivalent definitions: the direction of $p$ is changed; or $\ii \to - \ii$)
\begin{equation}
    \mathrm{TR}[Q(t)] = Q^*(-t).
\end{equation}
Time reversal symmetry means 
\begin{equation}
    S^* = S^{-1}.
\end{equation}

There is a lot of abuse (or confusing usage) of the term \term{breaking the time reversal symmetry}.
As is mentioned, when we have a strong magnetic field 
it's often said that the time reversal symmetry is broken; 
when the magnetic field is included as a part of the system 
(not an arbitrary external field), 
time reversal symmetry isn't truly broken, 
but reciprocity is indeed broken.
Consider a circulator of electrons: 
if an electron is injected into it through one port 
and we record the port from which the electron goes out, 
an electron going into the second port doesn't go out of the first port.
Of course we can still say time reversal symmetry is broken here 
if we keep the magnetic field not flipped when we apply the time reversal operation. 


\end{document}