\documentclass[hyperref, a4paper]{article}

\usepackage{geometry}
\usepackage{titling}
\usepackage{titlesec}
% No longer needed, since we will use enumitem package
% \usepackage{paralist}
\usepackage{enumitem}
\usepackage{footnote}
\usepackage{enumerate}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathtools}
\usepackage{bbm}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{physics}
\usepackage{tensor}
\usepackage{siunitx}
\usepackage[version=4]{mhchem}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{autobreak}
\usepackage[ruled, vlined, linesnumbered]{algorithm2e}
\usepackage{nameref,zref-xr}
\zxrsetup{toltxlabel}
\zexternaldocument*[optics-]{./optics/optics}[optics.pdf]
\zexternaldocument*[solid-]{./solid/solid}[solid.pdf]
\zexternaldocument*[info-]{./information/quantum-circuit}[quantum-circuit.pdf]
\usepackage[colorlinks,unicode]{hyperref} % , linkcolor=black, anchorcolor=black, citecolor=black, urlcolor=black, filecolor=black
\usepackage{prettyref}

% Page style
\geometry{left=3.18cm,right=3.18cm,top=2.54cm,bottom=2.54cm}
\titlespacing{\paragraph}{0pt}{1pt}{10pt}[20pt]
\setlength{\droptitle}{-5em}
\preauthor{\vspace{-10pt}\begin{center}}
\postauthor{\par\end{center}}

% More compact lists 
%\setlist[itemize]{
    %itemindent=17pt, 
    %leftmargin=1pt,
    %listparindent=\parindent,
    %parsep=0pt,
%}

% Math operators
\DeclareMathOperator{\timeorder}{\mathcal{T}}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\legpoly}{P}
\DeclareMathOperator{\primevalue}{P}
\DeclareMathOperator{\sgn}{sgn}
\newcommand*{\ii}{\mathrm{i}}
\newcommand*{\ee}{\mathrm{e}}
\newcommand*{\const}{\mathrm{const}}
\newcommand*{\suchthat}{\quad \text{s.t.} \quad}
\newcommand*{\argmin}{\arg\min}
\newcommand*{\argmax}{\arg\max}
\newcommand*{\normalorder}[1]{: #1 :}
\newcommand*{\pair}[1]{\langle #1 \rangle}
\newcommand*{\fd}[1]{\mathcal{D} #1}
\DeclareMathOperator{\bigO}{\mathcal{O}}

% TikZ setting
\usetikzlibrary{arrows,shapes,positioning}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{decorations.markings}
\tikzstyle arrowstyle=[scale=1]
\tikzstyle directed=[postaction={decorate,decoration={markings,
    mark=at position .5 with {\arrow[arrowstyle]{stealth}}}}]
\tikzstyle ray=[directed, thick]
\tikzstyle dot=[anchor=base,fill,circle,inner sep=1pt]

% Algorithm setting
% Julia-style code
\SetKwIF{If}{ElseIf}{Else}{if}{}{elseif}{else}{end}
\SetKwFor{For}{for}{}{end}
\SetKwFor{While}{while}{}{end}
\SetKwProg{Function}{function}{}{end}
\SetArgSty{textnormal}

\newcommand*{\concept}[1]{{\textbf{#1}}}

% Embedded codes
\lstset{basicstyle=\ttfamily,
  showstringspaces=false,
  commentstyle=\color{gray},
  keywordstyle=\color{blue}
}

\newcommand{\opticsdoc}{\href{../optics/optics}{the optics note}}
\newcommand{\soliddoc}{\href{../solid/solid}{the solid state physics note}}

\newrefformat{fig}{Figure~\ref{#1} on page~\pageref{#1}}
\newrefformat{sec}{Section~\ref{#1}}

\newenvironment{qanda}{\setlength{\parindent}{0pt}}{\bigskip}
\newcommand{\Q}{\bigskip\bfseries Q: }
\newcommand{\A}{\par\textbf{A:} \normalfont}

\title{A Travel Guide}
\author{Jinyuan Wu}

\begin{document}

\maketitle

\section{What to expect in courses and famous textbooks}

% TODO: connect notes together

Golden rules:
\begin{itemize}
    \item Don't stick to only one book, but it doesn't mean you shouldn't read books carefully. Many things that 
    can not easily be googled just lay on some corners of a famous textbook.
    \item 
\end{itemize}

\subsection{General physics}

For famous physicists like Feynman who always emphasize hand-on experiences, general physics is important to build 
physical pictures. For others, the unsystematic way of teaching in general physics makes it the most difficult 
subject. 

\subsection{Electrodynamics}

(Classical) electrodynamics are mostly about different ``modes'' of electromagnetic fields, including 
electrostatics, magnetostatics, quasi-static fields, radiation, and cavity modes. These are all different 
behaviors of the Maxwell equations under different external excitations.  

\subsection{Solid State Physics}

Though named as ``solid state'', most things you can find in a solid state physics course are about band metals 
and band insulators, partly because the effective theories of these states of matter are \emph{free} in the sense 
of QFT, and therefore can be handled with few-body quantum mechanics. The symmetry of lattices, 32 point groups 
and 230 space groups, phonons, and electron bands are typical topics in a course named solid state physics.

\subsection{Quantum field theory}

Quantum field theories are not really about the general theory of the quantum theory of fields, which, to be exact,
is not really finished. We know few things about non-perturbative field theory (we do know some specific cases, 
like conformal field theories, and studies dubbed as ``non-perturbative'' are usually about these theories with 
additional structures; duality is another way to explore non-perturbative field theories), and we cannot even 
give a mathematically exact definition of \emph{quantum fields}. We also do not know how to construct a QFT in 
an arbitrary spacetime. We also do not know what renormalization really means (it is formally equivalent to working 
around a RG fixed point, but RG flows from a well-defined UV theory to an IR theory, while renormalization in the 
context of high energy physics does not start from a well-defined theory - we just pretend it does, and do not
question how infinite bare parameters can make sense). 

What is taught in QFT is mostly about how to use the perturbative field theory formalism (which does not exist 
mathematically - there is no interaction picture, for example) in the \emph{relativistic} spacetime to find 
a framework about \emph{particles}. We can say that QFT is where the so-called \emph{particle-wave duality} 
really makes sense, where perturbation of quantum fields is conceived as particles. 

No one really care whether there are other probabilities in quantum theories of fields, or quantum many-body 
theories. It is possible that in a non-perturbative field theory, the excitations do not look quite like 
particles, and it is also possible that there exist quantum many-body theories beyond the standard approach.
These possibilities are merely mentioned (not to say addressed) in QFT courses.

\subsection{Condensed matter field theory}

Note that the final product of a condensed matter field theory is usually a four-point correlation function,
since almost the only thing we are able to measure is the response of the density (or current) of the system
under an external electromagnetic field, which acts on the density operator. Therefore, what immediately 
contribute to experimental results are usually (bosonic) density modes, or \emph{generalized hydrodynamic
modes}. Actually, the final results of high energy QFT can also be thought to be density correlation functions,
since what we care the most is $\abs*{\mathcal{M}}^2$, and all factors in it are in the form of $\bar{\psi} \psi$,
though usually we calculate the collision of two particles, and $\mathcal{M} \sim \expval*{\bar{\psi} \bar{\psi} \psi \psi}$,
and therefore $\mathcal{M}^2$ is the product of four density operators, i.e. eight field operators.

\section{Confusion in formalisms and jargons}

Golden rules:
\begin{itemize}
    \item Never pay too much attention to formalisms. Never, never, pay too much attention to generalization of formalisms.
    \item Physics is built up by \emph{cases}. Always calculate important cases.
    \item Formalisms are built to deal with important cases. If you face obstacles when trying to generalize one theory, it is almost always the case that the generalization is related to some highly non-trivial phenomena.
    For example, generalization of quantum field theories into an arbitrary spacetime is related to a self-consistent theory on quantum gravity, which remains an open question with no hope to be answered in the foreseeable future.
\end{itemize}

\subsection{Application of math in physics}

\subsection{General notion of signals and waves}

This part is about general notion of signals and waves. Instructions on these topics mostly appear in books whose names contain \emph{signal and system} or \emph{mathematical method}.

\begin{qanda}

\Q Why the stable state of a system is used to predict the time evolution of something like a pulse? 
For example, in optics we often calculate the stable electromagnetic fields around a surface and then use the results to predict
the behavior of a pulse incident to the surface.
\A This can be viewed as a version of scattering theory, where the scattering eigenstates are made of both the
input wave function and the output wave function.

\end{qanda}

\subsection{Quantum theories}

This part is about the basic formalisms of any quantum theory, including the meaning of states, operators and other.

\begin{qanda}

\Q Why is it legit to quantize a theory? Is there always a duality between a classical theory and a quantum theory?
\A Actually there is no \emph{quantization}. We only have \emph{classicalization}. The quantization procedures shown in quantum mechanics and quantum field theory textbooks are just pedagogical \emph{arguments} that make readers accept the quantum theories introduced.
Some classical theories give weird results when quantized. For example, a certain string theory lives in a 11-dimensional spacetime or otherwise we have Lorentz anomalies.
Some quantum theories just do not have an obvious classical counterpart, for example certain conformal field theories.

\Q What does it mean by the term \concept{quantum fluctuation}?
\A 

\Q Are ladder operators always possible to define in any operator algebra?
\A In principle you can always find ladder operators or an adapted version of them to resolve the problem of degeneracy.
But generally speaking there are no such beautiful expressions as is the case in linear oscillators or spins.
These two systems have ladder operators with simple analytic forms because of their good Lie algebra structures.

We should not worry about this, however, as roughly speaking, the only systems physicists know are oscillators and spins.
Generalization is not that important. Again, \emph{specific yet thought-provoking cases} are what really matter.

\Q The concept of measurement makes me nervous ...
\A Entanglement

It should be noted, however, that any feasible exact calculations are done in a setting where a quantum system is embedded into a classical background.
It is easy to notice that there is almost no measurement in the sense of quantum mechanics in quantum field theories, because 

\end{qanda}

\subsection{From quantum to classical}

\begin{qanda}

\Q Is taking the classical limit just taking the $\hbar \to 0$ limit?
\A Things are more tricky than a simple $\hbar \to 0$ limit. When we are talking about \emph{classical statistical mechanics}, we calculate the partition function as if all operators commute, which is % TODO: 

% TODO
So we can say that the classical limit can be viewed as a mean field theory, where the quantum fluctuation 
is ignored.

\Q Why are classical kinetic theories or fluid dynamics or things like that used in systems that are quantum enough?
\A Because the derivations of these theories are actually not restricted to classical theories, though they are often carried out in classical systems.

\end{qanda}

\subsection{Low energy effective theories}

\begin{qanda}

\Q It seems people always taking a low energy effective theory for granted and do not reason about whether there is a well-defined one ...
\A The mere existence of a low energy effective theory can indeed be taken for granted because in principle, we can just analyze the low-energy subspace of a Hamiltonian and find some labels to label them, thus effectively writing down a low energy effective theory.

What cannot be taken for granted is \emph{which} labels they are. The low energy effective theory of an interacting particle system is not always a theory about particles - for example in vicinity of a critical point we may get a conformal field theory, the correlation functions of which lack clear poles and therefore there is no well-defined particles.
Sometimes formally we can write down an effective theory concerning several specific degrees of freedom, but in this situation sometimes the degrees of freedom fluctuate strongly and the theory is not well-defined (or in the language of field theories, have infinitely high order terms). 
When choosing the degree of freedoms in the low energy effective theory, we are making very strong assumption about the system under investigation, which relies on intuition and numerical results.

An infamous example is high temperature superconductivity. Numerous so-called ``effective models'' have been proposed - the number of which is possibly more than the number of researchers in this area - yet none of them is convincing enough.

\Q It seems the analytic approaches to obtain an EFT are not many ...
\A Unfortunately, yes, especially in condensed matter physics. What we do have are:
\begin{itemize}
    \item Hubbard-Stratonovich transformation. 
    \item Parton constructions.
    \item Approximate commutation relation, i.e. in the low energy subspace, if two operators have approximately 
    the same commutation relation, then we can replace one by the other, and in this way the Hamiltonian may be 
    simplified.
    \item Response theory. When the response of a system is completely known, we may write down a theory with 
    exactly the same response, and this is an effective theory.
\end{itemize}
All of these approaches have the deficient we mentioned above that we do not know if a degree of freedom fluctuate wildly.
Formally several Hubbard-Stratonovich transformations can be applied to a field theory with a $\bar{\psi} \bar{\psi} \psi \psi$ interaction term, but most of the Hubbard-Stratonovich parameters have large fluctuation.
Therefore, which channel to choose is still practically dependent to experimental or numerical results.

\Q How can we find particles decay in a unitary theory?
\A The concept of ``decaying'' is kind of misleading in a completely pure state theory because of a state $\ket*{\psi}$ ``decays'' into another one $\ket*{\phi}$, 
it simply means that $\ket*{\psi}$ is not an eigenstate of the Hamiltonian, and thus has non-trivial time evolution.
We will also see $\ket*{\phi}$ evolves back into $\ket*{\psi}$ provided that we let the system evolves as it likes and do not introduce any perturbation.
Indeed this is often the case in nonlinear optics, where the width of a nonlinear crystal must be calculated with caution to maximize the transition rate, because, say, if we want a SHG process to merge two light beams into one, the inverse DHG process is also happening, so after going forward for a certain distance, the merged beam will again be split into two.
In this case, neither the one-beam state nor the two-beam state is an eigenstate of the Hamiltonian, 
so their weights in the actual state of the system keep vibrating.
Another example is light-atom interaction, where we can see both emission and radiation.
The same argument works in many-body theories, where the real eigenstates of the Hamiltonian cannot be depicted intuitively and are usually made up by the superposition of states containing different numbers of particles, 
so generally a single-particle state or multiple-particle state has non-trivial time evolution, so it will evolve to a state where the species, momentum and number of particles all change, and then it will evolve ``back''.
We say ``back'' because the energies of the eigenstate components in the initial state are often incommensurable,
so the time evolution of the state vector is not really periodic, but anyway it can always be as close to the initial state as we want provided that the evolution time is long enough.
In this sense, a particle has a lifetime, after which it becomes something else, but then after a long, long period of time, it comes back. (Now we see one exception: if the phase space is not bounded - for example in an infinite universe - then the state may not come ``back''. Consider two particles flying away from each other, for example. We will discuss this fact soon.)

If we look closer to how we get the ``decay rate'' we may get deeper understanding of what is happening.
A typical process to calculate the decay rate is as follows: By Feynman diagrams we make the self-energy correction,
the leading term of which is usually like 
\[
    \begin{aligned}
        \begin{gathered}
            \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
                %uncomment if require: \path (0,300); %set diagram left start at 0, and has height of 300
                
                %Straight Lines [id:da34956617828435] 
                \draw    (89,112) -- (169.71,112) ;
                %Shape: Arc [id:dp5329085264889877] 
                \draw  [draw opacity=0][dash pattern={on 0.84pt off 2.51pt}] (100,112) .. controls (100,111.84) and (100,111.67) .. (100,111.51) .. controls (99.98,94.94) and (113.41,81.5) .. (129.97,81.49) .. controls (146.42,81.48) and (159.78,94.69) .. (159.99,111.08) -- (130,111.49) -- cycle ; \draw  [dash pattern={on 0.84pt off 2.51pt}] (100,112) .. controls (100,111.84) and (100,111.67) .. (100,111.51) .. controls (99.98,94.94) and (113.41,81.5) .. (129.97,81.49) .. controls (146.42,81.48) and (159.78,94.69) .. (159.99,111.08) ;
                \end{tikzpicture}
        \end{gathered} = 
        \Sigma^\text{ret} &= \frac{g^2}{V} \sum_{\vb*{k}} \frac{1}{\omega - \xi_{\vb*{k}} + \ii 0^+} \\
        &= g^2 \int \frac{\dd[3]{\vb*{k}}}{(2\pi)^3} \left( \primevalue \frac{1}{\omega - \xi_{\vb*{i}}} + \ii \pi \delta(\omega - \xi_{\vb*{k}}) \right),
    \end{aligned}
\]
which obviously has a finite imaginary part. This can be seen as an example of the optical theorem, where the imaginary part of a diagram corresponds to the square of the absolute value of half of the diagram, i.e.
\[
    \Im \begin{gathered}
        \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
            %uncomment if require: \path (0,300); %set diagram left start at 0, and has height of 300
            
            %Straight Lines [id:da34956617828435] 
            \draw    (89,112) -- (169.71,112) ;
            %Shape: Arc [id:dp5329085264889877] 
            \draw  [draw opacity=0][dash pattern={on 0.84pt off 2.51pt}] (100,112) .. controls (100,111.84) and (100,111.67) .. (100,111.51) .. controls (99.98,94.94) and (113.41,81.5) .. (129.97,81.49) .. controls (146.42,81.48) and (159.78,94.69) .. (159.99,111.08) -- (130,111.49) -- cycle ; \draw  [dash pattern={on 0.84pt off 2.51pt}] (100,112) .. controls (100,111.84) and (100,111.67) .. (100,111.51) .. controls (99.98,94.94) and (113.41,81.5) .. (129.97,81.49) .. controls (146.42,81.48) and (159.78,94.69) .. (159.99,111.08) ;
            \end{tikzpicture}
    \end{gathered} \sim \left|\begin{gathered}
        \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
            %uncomment if require: \path (0,300); %set diagram left start at 0, and has height of 300
            
            %Straight Lines [id:da34956617828435] 
            \draw    (89,112) -- (132.71,112) ;
            %Shape: Arc [id:dp5329085264889877] 
            \draw  [draw opacity=0][dash pattern={on 0.84pt off 2.51pt}] (100,112) .. controls (100,111.84) and (100,111.67) .. (100,111.51) .. controls (99.99,99.34) and (107.22,88.86) .. (117.63,84.15) -- (130,111.49) -- cycle ; \draw  [dash pattern={on 0.84pt off 2.51pt}] (100,112) .. controls (100,111.84) and (100,111.67) .. (100,111.51) .. controls (99.99,99.34) and (107.22,88.86) .. (117.63,84.15) ;
            %Shape: Arc [id:dp9924278461942098] 
            \draw  [draw opacity=0] (93.51,91.45) .. controls (96.04,85.66) and (100.24,81.01) .. (105.35,77.89) -- (121,103.49) -- cycle ; \draw   (93.51,91.45) .. controls (96.04,85.66) and (100.24,81.01) .. (105.35,77.89) ;
            %Straight Lines [id:da08566752462678706] 
            \draw    (101.71,80.53) -- (107.08,76.7) ;
            \draw [shift={(108.71,75.53)}, rotate = 504.46] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (12,-3) -- (0,0) -- (12,3) -- cycle    ;
            \end{tikzpicture}            
    \end{gathered}\right|^2.
\]
The RHS of the equation is a decay channel, explaining why we get a finite imaginary part of the self energy despite that the self energy corresponds to a diagram which apparently has nothing to do with decaying.
Since now the correlation function (or the $S$-matrix element) has a finite imaginary part, 
we say the particle decays and will never come back because the correlation function gives the system's response to a particle added in, and an imaginary part means that as time goes by, it is more and more unlikely to see the particle being there.

So we see the imaginary part of the self-energy - hence particle decay - comes from the infinitesimal imaginary part $0^+$ in the propagator, which in turn comes from the so-called causality condition.
Note, however, that a so-called anti-causal solution satisfies the equations of motion as well as a so-called causal one 
and is actually ``causal'' in that it can be described by a equation that predicts the future with the past.
What the causality condition really requires is that there is no well-fabricated ``global'' events in the boundary of the spacetime that give rise to 
certain seemingly strange phenomenon like waves concentrating to a point and then a source appearing there.
In other words, what the causality condition is actually an assumption that we can only see waves propagating away from the source, that by default there is no electric current in a circuit until someone starts the generator, etc.
This is relevant to causality only when the background is ``clean'', i.e. fluctuations are limited to small regions, there is no advanced wave or inward propagating waves or radiation filling the whole spacetime, etc.  
We can see that this assumption works perfectly well in decay experiments particle physics, where the out particles separate with each other and propagate unboundedly.
In an infinite universe where Poincaré recurrence breaks down the inverse process never happens, and in a finite yet large universe since it takes a really, really long period of time for the decay products to come back, so approximately the inverse process never happens.
The assumption also works well in condensed matter physics, where the whole material serves as a reservoir and ``observe'' the decay products almost immediately after they are generated. 
For example, by interaction an electron with high momentum soon passes its momentum to the rest of the electron gas, and it is not likely that we will soon see the Poincaré recurrence though the system is bounded.
Actually the logic also works in particle physics, where the decay products are ``observed'' by the rest of the lab, and we do not need to ask the rest of the universe to keep these decay products away.

So in conclusion, there is particle decay in scattering processes in unitary systems because we implicitly and manually \emph{require} so: 
It either comes from weird properties of our theory that the wave function is not even quasiperiodic 
when the universe is infinite (This can also be seen considering that we need to replace a discrete sum of possible momenta into a continuous one to get a well-defined and smooth imaginary part - the latter is a system with infinite degrees of freedom. This is a key difference between a few-body system and a many-body system.) or comes from the fact that the decay products are observed by the environment and therefore hidden non-unitary properties exist in our theory.
These two explanations cab be regarded as equivalent, as observation involves an external environment, 
which can be essentially regarded as an infinitely large universe, 
and both of them can be viewed as special cases of ``clean'' backgrounds.
This equivalence can also be seen in classical theories. 
For example, absorption layers are often used to mimic a free space in electrodynamics. 
(Also, note that the condition that radiation comes from the source to the free space in classical electrodynamics is also imposed by adding $\ii 0^+$ into the Green function. Everything agrees quite well here.)

Now we return to the two examples in optics mentioned above: the second order nonlinear transition and the atom-in-optical-field system.
We see clear periodic phenomena in these systems, so where is decay here?
Well, actually, there is, and actually we already have a well-established tool to calculate it: the Fermi golden rule. 
The key point here is that the periods in these systems are not that long by the everyday standard, 
but already long enough under the standard of these systems themselves.
For example, the wave function of an two-level atom in an optical field may be viewed as vibrating with the Rabi frequency, 
but the frequency is usually small, and during a small period of time we may make a Markov approximation to the equation governing $\braket{\text{ground}}{\psi}$.
By introducing the Markov assumption we are doing the same thing we did in the scattering theory: assuming that the ``out'' states - this time the component of the excited state - will never come back.
So in the end, we obtain a stochastic description of the atom: 
Assuming the atom is initially at the ground state, 
and at each time step it has some probability to go to (or ``decay to'', though this time the target state has a higher energy thanks to the external optical field) the excited state.
We can do the same thing again for an atom at the excited state, describing it with a stochastic process that has a constant decay rate.
So in the end, we can see that despite the fact that the wave function of the atom vibrates (which is a simplified version of Poincaré recurrence), at each small period of time (but not too small or Fermi golden rule fails) the atom can be viewed in a decay process: first ``decaying'' to the excited state, and then decay to the ground state.
The same logic applies to nonlinear optics: though along the whole crystal, the photon numbers on different modes change, 
at a small period we can just view the nonlinear transition as scattering.

Now we address the last concern that may arise: The imaginary part of the pole in the correlation function means 
the response of the system weakens as time goes by, but actually the response is vibrating.
Since we already know that the vibrating response is a simplified version of Poincaré recurrence,
and when we are studying the response of the system we usually assume the response is zero where $\abs*{T} \to \infty$, 
we see that the correlation function (calculated with $\ii 0^+$ etc.) can only show the response 
of the system \emph{before} Poincaré recurrence. So there is nothing wrong here.
Actually, most of the time people only care about \emph{linear response}, which assumes the change of the
density matrix is linear to the external field, which is definitely wrong when the time is long enough 
(because it is proportional to things like $\sin t$).

\Q According to the Lindblad formalism (or other master equation formalisms, a generalized one being 
Mori-Zwanzig formalism), integrating out some degrees of freedom introduces memory effects and random 
fluctuation, while in Wilson RG we integrate out high energy degrees of freedom and still get a unitary 
theory, and in high energy physics sometimes people also integrate out low energy degrees of freedom.
What is wrong here?
\A All the claims are correct. The reason we can integrate out high energy degrees of freedom in 
the Wilson RG, or integrate out low energy degrees of freedom in high energy physics is because 
these degrees of freedom \emph{never} appear in input and output states, or in other words, they 
never appear as external lines in Feynman diagrams. Intuitively, therefore, these degrees of freedom
can be hidden into a ``big grey circle'' like these:
\[
\begin{gathered}
    \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
        %uncomment if require: \path (0,300); %set diagram left start at 0, and has height of 300
        
        %Shape: Circle [id:dp6202869349429303] 
        \draw  [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ,fill opacity=0.5 ] (142,157.01) .. controls (142,133.81) and (160.81,115) .. (184.01,115) .. controls (207.21,115) and (226.02,133.81) .. (226.02,157.01) .. controls (226.02,180.21) and (207.21,199.02) .. (184.01,199.02) .. controls (160.81,199.02) and (142,180.21) .. (142,157.01) -- cycle ;
        %Straight Lines [id:da18526729229123706] 
        \draw    (100,119) -- (145.02,139.71) ;
        \draw [shift={(122.51,129.35)}, rotate = 204.7] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (12,-3) -- (0,0) -- (12,3) -- cycle    ;
        %Straight Lines [id:da8868545233093175] 
        \draw    (111.02,208.71) -- (149.02,179.71) ;
        \draw [shift={(130.02,194.21)}, rotate = 142.65] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (12,-3) -- (0,0) -- (12,3) -- cycle    ;
        %Straight Lines [id:da5728892663174006] 
        \draw    (217.02,131.71) -- (255.02,102.71) ;
        \draw [shift={(236.02,117.21)}, rotate = 142.65] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (12,-3) -- (0,0) -- (12,3) -- cycle    ;
        %Straight Lines [id:da6216033129708001] 
        \draw    (222,174) -- (267.02,194.71) ;
        \draw [shift={(244.51,184.35)}, rotate = 204.7] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (12,-3) -- (0,0) -- (12,3) -- cycle    ;
        \end{tikzpicture}
\end{gathered}.
\]

This is not the most general case, though. For example, when studying conductivity in a metal, we may 
consider thermal phonons that come out from ``nowhere'' and scatter with electrons. In this case phonons
do appear in external lines, and therefore integrating out phonon degrees of freedom and obtain something 
unitary like the BCS theory is impossible - if you do integrate out phonons, you will find the theory 
about electrons is non-Hermitian: there is a magical force making electrons damp, etc. 
Another case where integrating out degrees of freedom and getting a unitary theory is impossible is 
\emph{decay}, which is discussed in the previous Q\&A. In this case, calculating Feynman diagrams  
that have a finite imaginary part is equivalent to something like ``integrating out the Fermi sea'':
for example, when we are calculating the conductivity, we calculate the two-point correlation function
of electrons, which, physically speaking, is equivalent to ignoring the many-body nature of the 
system. Many-body effects like a high-energy electron decaying into a low-energy electron and an
electron-hole pair will not go away, though, and they are included by the finite imaginary part 
of the two-point function. If we write down an effective theory of the electrons according to 
the two-point correlation function, it is by no means unitary, just as is the case in macroscopic 
electrodynamics.

Actually the idea can be explained using photonic crystals. What if we inject a beam of light with a 
frequency in a forbidden band into a photonic crystal? Well, we say ``due to the strong coupling between
photon and the lattice, the photons are strongly absorbed''. But if the photonic crystal is made of 
insulator, then the excitations in the material created by the absorption of photons will not last long,
and backward radiation happens then, so in the end, photons occurs again. Due to 
the fact that excitations in an insulator do not have strong coupling, the system can still be viewed 
as a few-body system, and excitations in the insulator do not form a reservoir. Therefore, in 
electromagnetic, excitations in an insulator can be seen as internal lines in Feynman diagrams and 
be hidden in a grey blob, and after integrating out these excitations we still get a unitary theory 
of electromagnetic waves. However, in metals, we have a reservoir of free electrons, and therefore 
when photons are absorbed, some electrons are excited, and they pass their energy to the whole 
electronic liquid, so in the end what happens is increasing of temperature. Therefore, electrons 
in a metal cannot be fully described as internal lines in Feynman diagram, and a certain degree
of dissipation - which we call \concept{resistance} - occurs.

It must be noted here that the noise/dissipation/fluctuation terms introduced 
when something appearing in the input or output states is integrated out exists
even when $T = 0$.
The origin of the noise or similar things is the same as the origin of temperature:
the system in question is entangled with some outside objects,
and keeping our eyes only on the system equivalently introduces a noise term.
But not all noises can be caught by the single parameter $T$.
That's why in Landau Fermi liquid, 
even when $T \to 0$,
we still need to estimate the collision rate of quasiparticles,
and cannot take the well-definedness of quasiparticles for granted,
because it's of course possible that the single-electron picture fails even around the ground state,
and we need to see how long a single quasiparticle state can last 
before it turns into something else because of interaction.
All of these happen when $T = 0$ as well.

\Q Sometimes the parameters in a unitary effective theory have temperature dependence \dots
\A From Mori-Zwanzig formalism we see integrating out degrees of freedom generally leads to three 
things: ``renormalization'' of parameters in the rest of the system, memory effect, and fluctuation
force. When the latter two can be ignored and the part of the system that is integrated out 
is in thermal equilibrium, we get an effective theory which is unitary yet has temperature 
dependence. One example of this is the temperature dependence of electronic band structure.
This resembles mean field theory in that only a part of interaction channels are considered.
Just like mean field theory, the dissipative channels can be added back. Actually all these 
can be contained in the Keldysh formalism. An example can be find \href{https://arxiv.org/pdf/1210.3623.pdf}{here}.

\end{qanda}

\subsection{Quantum field theories}

\begin{qanda}

\Q The second quantization procedure that somehow ``promotes'' the wave function of a single particle to a quantum field does not seem to make sense.
\A The term \concept{second quantization} has several different meanings, all of which can be found in textbooks.
    
\Q Is any quantum theory of fields a theory of particles? Or in other words, are quantum fields just convenient ways to represent particles' behaviors?
\A The two claims are only true in \emph{perturbative quantum field theories}, which are the only \emph{quantum field theories} used in particle physics.
There are occasions, however, when quantum fields themselves have physical meanings.
The conformal field theory is an example. Another example is the lattice field theory approach, for example lattice QCD.
In the strong coupling regime, new emergent phenomena occur and it is not correct to assume that the system of interest is made of particles corresponding to the fields appearing in the Lagrangian.

The term \concept{quantum field theory}, therefore, is kind of confusing because it may be used to denote the \emph{quantum mechanics of fields}, where fields themselves are what we are interested in, or the perturbative theory used in particle physics, when a theory can actually be defined with just Feynman rules without mentioning fields. 
Griffith's textbook on particle physics is a good example of \emph{quantum field theory (in the latter sense) without fields}.
Normal textbooks whose titles include the word ``field'' will first introduce the quantum theory of fields in the former sense and then secretly slip to the latter view of point.

\Q Is the Lagrangian really necessary considering the fact that perturbative quantum field theories can be defined purely in terms of Feynman diagrams and that some field theories do not have a Lagrangian? Or is the Hamiltonian really necessary?
\A Since everything we are interested in in a quantum field theory can be computed using correlation functions, a quantum field theory can be seen as a (semi-)probabilistic theory where we are interested in the moments (i.e. correlation functions) of variables.
So yes, in principle we can define a quantum field theory using purely correlation functions, and if we want to impose certain constraints we can rephrase them in terms of the relations between different correlation functions.
For example, a free theory may be defined as a theory where the Wick theorem holds.
This approach is called \concept{bootstrap}.

It should be pointed out that bootstrap is not practical for most theories we will encounter. Conformal field theories and topological field theories can be bootstrapped because they are highly constrained, but it is impossible to bootstrap, say, QED, from properties like symmetry.
In this case we simply say it is not possible to define the field theory in question using bootstrap.
The idea that the Lagrangian or the Hamiltonian should be used less and we should focus more on correlation functions and amplitudes, however, is important, which gives rise to the so-called \concept{amplitudology}.

\Q There seems to be difference between the Green function in quantum mechanics and the Green function in quantum field theory \dots
\A The notion of Green function of the ``propagator'' in quantum mechanics may be kind of confusing because it 
serves as matrix elements of the unitary time evolution operator, while it can be derived with a Schr\"{o}dinger 
equation \emph{with an external source}, which does not seem to make sense because such a equation is not unitary.
This comes from a mathematical theorem - the Duhamel's principle - that connects the linear inhomogeneous evolution
to th linear homogeneous evolution.
We see the same physical picture in electrodynamics, where the propagation of the optical field may be regarded
as radiation from sources placed on the wave surface.

The propagators in quantum field theories are also Green functions of ``wave equations''.
These propagators, however, are not directly related to the time evolution of states, since ``wave equations''
in quantum field theories are just synonyms of free Lagrangians. 
We need to consider propagators in quantum field theories simply because we want to use Wick theorem.

We can see the corresponding objects of QFT propagators in QM are expressions like $\mel{\psi}{\timeorder q(t_1) q(t_2)}{\psi}$,
while the corresponding objects of QM propagators in QFT are ``Green functionals''.
Anyway, a free quantum field theory can be viewed as a single particle quantum mechanics (or ``quasi''-quantum mechanics for things like photons), so in the specific case these two types of propagators are equivalent.

\end{qanda}

\subsection{Kinetic theory, hydrodynamics and other seemingly purely ``classical'' theories}

\begin{qanda}

\Q Can we interpret kinetic theories or hydrodynamic equations or something like this as quantum?  
\A Yes. Kinetic equations may be seen as Green function EOMs in disguise (with Wigner transformation, 
for example), and hydrodynamic equations can be derived on top of kinetic equations.

\Q Then can we interpret things like the \emph{distribution functions} in kinetic theories or hydrodynamic equations 
as quantum operators?
\A This may be subtle if we just consider the ``classical'' derivation of kinetic theories or 
hydrodynamic equations, where the dynamic variables seem to emerge from \emph{statistical} average 
of microscopic details, rather than pure state quantum operators. At the first glance, if we interpret
a kinetic theory or a hydrodynamic theory or something as \emph{quantum}, we are saying they correspond 
to an equation about \emph{expectations} in a quantum theory. An EOM of $\expval*{\hat{A}}$ is of course 
not the same as an EOM of $\hat{A}$. The latter may contain several terms that vanish when in the expectation 
brackets. For example, the quantum Heisenberg equation of damping is a quantum Langevin equation, while the 
EOM of its expectation is the same as the classical equation with damping, without the Langevin fluctuating 
force term. Actually, this has a deeper reason: the fluctuation-dissipation theorem. If we see dissipation
of $\expval*{A}$, $A$ must have some fluctuation that can only be revealed by correlation functions like $\expval*{A A}$.

However, we \emph{do have} quantum hydrodynamic in the sense that the density and velocity are operators \cite{Penrose1954-zv}. Elements in hydrodynamics, like the convective derivative,
can be derived directly from definitions like 
\begin{equation}
    \rho(\vb*{r}) = \sum_i \delta(\vb*{r} - \vb*{r}_i), \quad 
    \vb*{j}(\vb*{r}) = \frac{1}{2m} \sum_i (\vb*{p}_i \rho + \rho \vb*{p}_i).
\end{equation} 
This shows how a group of small, non-continuous and fluctuating degrees of freedom can assemble into 
a continuous, smooth and geometric object, and the effective theory on this object hints almost nothing 
about the quantum many-body theory of the former degrees of freedom. This \emph{quantum hydrodynamic}
approach is now better known as \emph{bosonization}, though in principle a theory can be bosonize using 
modes other than the density modes. The term \emph{quantum hydrodynamics} is now more frequently connected
with kinetic theories in the sense of ``Green function EOMs in disguise'', as well as Bohmian mechanics.

For kinetic theories, when the underlying many-body theory can be bosonized, we get a series of equations  
on density-like fluctuations (not necessarily $\sim c_{\vb*{k}}^\dagger c_{\vb*{k}}$, but are bilinear forms), 
and after doing a Wigner transformation to these fluctuations we get linear kinetic equations that are \emph{exact}.
In other cases, distribution functions in kinetic theories can be viewed as operators in a \emph{mean field} 
quantum many-body theory, where the high-order fluctuations (like ones 
$\sim \expval*{\bar{\psi} \bar{\psi} \psi \psi}$) are ignored. Things like temperature dependence in kinetic 
theories (like a Boltzmann equation with relaxation time approximation) may be viewed as introduced by 
a \emph{thermal} mean field theory.

From the Feynman diagram point of view, when we interpret the variables in kinetic equations as a 
classical number, what we are actually doing is assuming that collisions do not interfere each other, 
since there is no such thing as loop correction in a classical perturbation theory. 
This works when the temperature is high, so all loop corrections that involve integral like $\int_0^\beta \dd{\tau}$
are suppressed in the imaginary time field theory. 
From another point of view, when the temperature is high, thermal fluctuation kills quantum interference. 
Intuitively speaking, the $1/T$ time scale is roughly the time scale of thermal fluctuation, 
i.e. quantum processes with a time scale longer than $1/T$ are unlikely to happen.

So the conclusion is variables in kinetic theories (or hydrodynamic equations)
sometimes can indeed be interpreted as operators,
and whether this really makes sense strongly depends on the details of a system, because no one really knows 
whether it makes sense to summarize interaction between particles in that collision term
(instead of, say, creating new degrees of freedom).
This is just like reducing a quantum many-body system into a single-particle quantum mechanical one.
If the reduction works, it means the ladder diagram approximation works in the quantum many-body system.
The derivation of the ``quantum kinetic equations''
with operators as variables 
looks the same as the derivation of classical kinetic equations,
but in the former, 
what really tells us a lot about the system 
is the ansatz that hydrodynamic degrees of freedom are sufficient
to describe the system,
while in the latter,
that only hydrodynamic degrees of freedom are important in the theory 
\emph{may} be true because of the pure-state properties of our system,
but it may also be true because of statistical average.
It's well possible for a theory without a quantum hydrodynamic description
(I mean, with operators as variables)
to have a classical hydrodynamic description:
the classical hydrodynamics is correct 
whenever \emph{one} of its quantum extension is correct,
and that one doesn't need to be the density-operator-based-hydrodynamics one.

\Q Then what does the nonlinear collision term on the RHS of a Boltzmann equation mean? Can it be seen as coming 
from a higher order term in the Hamiltonian?
\A 

\Q What is the corresponding objects of temperature, pressure, etc. in a quantum theory?
\A It is kind of strange because currently we do not have satisfactory theories on how these quantities emerge 
from a many-body system. They are to be understood as parameters (instead of quantum operators) in a certain wave function of density matrix 
ansatz (e.g. $\beta$ in $\ee^{- \beta H}$), but why these ansatzes works well in our world remains uncertain.
For example, thermalization is still in the frontline of physics. 

Once we accept these ansatzes, however, we can calculate things like thermal conductivity in a quite precise 
manner. For example, in the linear response region, generally speaking we are to rewrite the density matrix
into the form of 
\[
    \rho' = \ee^{- \beta H + p A},
\]
where $p$ is the perturbation of a parameter like $\grad T$ or $\grad p$, and then evolve the density matrix 
according to $H$. 

\end{qanda}

\subsection{Stochastic processes}

\begin{qanda}
    
\Q Why are stochastic processes so often seen in physics?
\A Because they are the classical limits of quantum ones. This can be found in most decent books concerning non-equilibrium many-body physics.
Our previous discussion on how seemingly non-unitary processes occur in a totally unitary theory also provides 
good examples. The Markov approximation of a atom in an external light field, for example, shows how Schrödinger 
equation can be approximately rephrased into a master equation. 

\Q Why do we use the fluctuation-dissipation theorem to derive the relation 
between the relation between the fluctuation term and the damping term in the Langevin equation,
since the proof of the fluctuation-dissipation theorem doesn't contain any external thermal perturbation?
\A It should be noted that the fluctuation-dissipation theorem 
is derived \emph{without} mentioning any thermal fluctuation of the system
(\prettyref{fig:onion-system}):
We just assume the initial state is fully thermalized,
and then the system evolves on its own.
When we invoke the fluctuation-dissipation theorem to derive a constraint on 
the damping term and the fluctuating term in a Langevin equation,
we are \emph{not} invoking the theorem considering the \emph{particle} as the system.
Here we have an onion-like structure 
for any system described by a Langevin-like equation:
The system is surrounded by an environment
(in the case of Brownian motion, it's the water surrounding the particle,
and in the case of atomic damping, 
it's the light field in the cavity) that is large, 
and usually has linear response to any external fluctuation,
and the environment is surrounded by a larger environment.
The damping and fluctuating terms in the Langevin equation 
are the properties of the water, the light field, etc.,
that is, the smaller environment directly surrounding the system described by the Langevin equation.
We can safely apply the fluctuation-dissipation theorem to the smaller environment,
which is now the \emph{system}, 
and the larger environment is now the \emph{environment}.
The smaller system is still large enough 
so that its time evolution is basically decided by its own Hamiltonian,
not by its interaction with the larger environment.

It's kind of misleading to show the basic techniques of linear response theory 
by calculating a harmonic oscillator, therefore:
A single oscillator is too small compared with the usual environment,
and its behaviors are better described by a Langevin-like equation.
Note that even when $T = 0$,
there can still be damping and fluctuation in the Langevin equation,
simply because of there is still interaction between the system and the environment
(which is described by fluctuation-dissipation theorem and linear response theory,
while the system is described the Langevin equation and can have highly nonlinear response).

It's still possible to write a Langevin-like equation for a large system,
like the water in Brownian motion.
Here the system in the Langevin equation is the low-energy degrees of freedom,
and the smaller environment is the hidden degrees of freedom 
(like small scale flows),
and the larger environment is the environment surrounding the many-body system.
This is covered in the (88.9) and (88.10) of the volume on condensed matter physics in 
The fluctuation in a classical liquid is assumed to come from 
``local spontaneous stress and heat flow'',
and a Langevin equation (88.8) -- this time on a \emph{field} instead of a coordinate --
is given.

We can even see \prettyref{fig:onion-system} in ordinary condensed matter physics:
The theory of the behavior of a whole solid doesn't need any ``quantum jump'' formulation,
because everything is considered in $\ee^{- \ii H t}$;
but a kinetic theory tracing a \emph{single} electron definitely needs 
notions of collision, etc.

\end{qanda}

\begin{figure}
    \centering
    \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
        %uncomment if require: \path (0,300); %set diagram left start at 0, and has height of 300
        
        %Shape: Ellipse [id:dp8043932652565831] 
        \draw   (100,185.09) .. controls (100,142.52) and (195.48,108) .. (313.25,108) .. controls (431.02,108) and (526.5,142.52) .. (526.5,185.09) .. controls (526.5,227.67) and (431.02,262.19) .. (313.25,262.19) .. controls (195.48,262.19) and (100,227.67) .. (100,185.09) -- cycle ;
        %Shape: Ellipse [id:dp3528292568204894] 
        \draw   (120,185.09) .. controls (120,153.56) and (180.55,128) .. (255.25,128) .. controls (329.95,128) and (390.5,153.56) .. (390.5,185.09) .. controls (390.5,216.63) and (329.95,242.19) .. (255.25,242.19) .. controls (180.55,242.19) and (120,216.63) .. (120,185.09) -- cycle ;
        %Shape: Ellipse [id:dp12948925288702262] 
        \draw   (146,186.19) .. controls (146,166.31) and (175.44,150.19) .. (211.75,150.19) .. controls (248.06,150.19) and (277.5,166.31) .. (277.5,186.19) .. controls (277.5,206.07) and (248.06,222.19) .. (211.75,222.19) .. controls (175.44,222.19) and (146,206.07) .. (146,186.19) -- cycle ;
        
        % Text Node
        \draw (165,177.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {Langevin};
        % Text Node
        \draw (285,177.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {linear response};
        % Text Node
        \draw (416,177.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {almost static};
        
        
    \end{tikzpicture}
    \caption{The onion structure in Langevin equation:
    the inner system can be highly nonlinear and is described by a Langevin equation,
    while the system directly attached to it -- and which serves as the reservoir of the inner system -- 
    is largely linear and is described by the linear response theory,
    for which perturbation from the external world isn't quite important,
    and the most external system almost doesn't change at all.}
    \label{fig:onion-system}
\end{figure}

\section{Concrete math}

This section is about mathematics for concrete calculations.

\subsection{Integral table}

\subsection{Differential geometry}

\subsection{Approximations in ODEs and PDEs}

Note that some approximations never work for the whole $0 < t < \infty$ time span.

\section{Notations and conventions and how to remember them}

\subsection{Fourier transformation}

We have 
\[
    \braket*{\vb*{x}}{\vb*{p}} = \ee^{\ii \vb*{p} \cdot \vb*{x}},
\]
and therefore 
\[
    \braket*{\vb*{x}}{\psi} \sim \int \dd[n]{\vb*{p}} \ee^{\ii \vb*{p} \cdot \vb*{x}} \braket*{\vb*{p}}{\psi}.
\]
That is why in physics we usually use the following convention when doing Fourier transformation in the spatial coordinates:
\begin{equation}
    f(\vb*{x}) \sim \int \dd[n]{\vb*{p}} \ee^{\ii \vb*{p} \cdot \vb*{x}} f(\vb*{p}), \quad f(\vb*{p}) \sim \int \dd[n]{\vb*{x}} \ee^{- \ii \vb*{p} \cdot \vb*{x}} f(\vb*{x}).
\end{equation}
On the other hand, in the Schrödinger picture we have 
\[
    \ket*{\psi(t)} \sim \ket*{\psi(0)} \ee^{- \ii E t},
\]
and therefore in the temporal coordinate we do Fourier transformation in the following way:
\begin{equation}
    f(t) \sim \int \dd{\omega} f(\omega) \ee^{- \ii \omega t}, \quad f(\omega) \sim \int \dd{t} \ee^{\ii \omega t} f(t).
\end{equation}
Therefore, when we do Fourier transformation in all coordinates of a covariant relativistic system we have 
\begin{equation}
    f(x) \sim \int \dd{p} f(p) \ee^{- \ii p \cdot x}, \quad f(p) \sim \int \dd{x} f(x) \ee^{\ii p \cdot x}.
\end{equation}
where we use the $(+, -, -, -)$ metric.

\subsection{Wick rotation}

\subsection{Linear response theory}

Suppose we add an external field to the physical quantity $A$ by adding a term in the Hamiltonian
\begin{equation}
    H_\text{ext} = - F A,
\end{equation}
and we need to track how another quantity $B$ changes, which is related to $A$ by the retarded Green function. 
In the literature, different authors use different notations, and sometimes it is hard to keep all coefficients 
correct. We can remember the relation between $A$ and $B$ as 
\begin{equation}
    \Delta B \sim \ee^{\ii H t} B \ee^{- \ii H t} \simeq \ii \comm*{H}{B} \simeq - \ii \comm*{A}{B} F.
\end{equation}
We can then add things like the time integral.

\section{Confusing terms}

\begin{itemize}
    \item \concept{EOM}, equation of motion
\end{itemize}

\bibliographystyle{plain}
\bibliography{formalism/fluid-quantum} 

\end{document}