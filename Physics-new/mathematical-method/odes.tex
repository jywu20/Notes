\documentclass[hyperref, a4paper]{article}

\usepackage{geometry}
\usepackage{titling}
\usepackage{titlesec}
% No longer needed, since we will use enumitem package
% \usepackage{paralist}
\usepackage{enumitem}
\usepackage{footnote}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathtools}
\usepackage{bbm}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{physics}
\usepackage{tensor}
\usepackage{siunitx}
\usepackage[version=4]{mhchem}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{underscore}
\usepackage{autobreak}
\usepackage[ruled, vlined, linesnumbered]{algorithm2e}
\usepackage{nameref,zref-xr}
\zxrsetup{toltxlabel}
\usepackage[colorlinks,unicode]{hyperref} % , linkcolor=black, anchorcolor=black, citecolor=black, urlcolor=black, filecolor=black
\usepackage[most]{tcolorbox}
\usepackage{prettyref}

% Page style
\geometry{left=3.18cm,right=3.18cm,top=2.54cm,bottom=2.54cm}
\titlespacing{\paragraph}{0pt}{1pt}{10pt}[20pt]
\setlength{\droptitle}{-5em}

% More compact lists 
\setlist[itemize]{
    %itemindent=17pt, 
    %leftmargin=1pt,
    listparindent=\parindent,
    parsep=0pt,
}

\setlist[enumerate]{
    %itemindent=17pt, 
    %leftmargin=1pt,
    listparindent=\parindent,
    parsep=0pt,
}

% Math operators
\DeclareMathOperator{\timeorder}{\mathcal{T}}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\legpoly}{P}
\DeclareMathOperator{\primevalue}{P}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\res}{Res}
\newcommand*{\ii}{\mathrm{i}}
\newcommand*{\ee}{\mathrm{e}}
\newcommand*{\const}{\mathrm{const}}
\newcommand*{\suchthat}{\quad \text{s.t.} \quad}
\newcommand*{\argmin}{\arg\min}
\newcommand*{\argmax}{\arg\max}
\newcommand*{\normalorder}[1]{: #1 :}
\newcommand*{\pair}[1]{\langle #1 \rangle}
\newcommand*{\fd}[1]{\mathcal{D} #1}
\DeclareMathOperator{\bigO}{\mathcal{O}}

% TikZ setting
\usetikzlibrary{arrows,shapes,positioning}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{calc}
\tikzstyle arrowstyle=[scale=1]
\tikzstyle directed=[postaction={decorate,decoration={markings,
    mark=at position .5 with {\arrow[arrowstyle]{stealth}}}}]
\tikzstyle ray=[directed, thick]
\tikzstyle dot=[anchor=base,fill,circle,inner sep=1pt]

% Algorithm setting
% Julia-style code
\SetKwIF{If}{ElseIf}{Else}{if}{}{elseif}{else}{end}
\SetKwFor{For}{for}{}{end}
\SetKwFor{While}{while}{}{end}
\SetKwProg{Function}{function}{}{end}
\SetArgSty{textnormal}

\newcommand*{\concept}[1]{{\textbf{#1}}}

% Embedded codes
\lstset{basicstyle=\ttfamily,
  showstringspaces=false,
  commentstyle=\color{gray},
  keywordstyle=\color{blue}
}

\lstdefinestyle{console}{
    basicstyle=\footnotesize\ttfamily,
    breaklines=true,
    postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space}
}

% Reference formatting
\newrefformat{fig}{Figure~\ref{#1}}

% Color boxes
\tcbuselibrary{skins, breakable, theorems}
\newtcbtheorem[number within=section]{warning}{Warning}%
  {colback=orange!5,colframe=orange!65,fonttitle=\bfseries, breakable}{warn}
\newtcbtheorem[number within=section]{note}{Note}%
  {colback=green!5,colframe=green!65,fonttitle=\bfseries, breakable}{note}
\newtcbtheorem[number within=section]{info}{Info}%
  {colback=blue!5,colframe=blue!65,fonttitle=\bfseries, breakable}{info}

% Displaying texts in bookmarkers

\pdfstringdefDisableCommands{%
  \def\\{}%
  \def\ce#1{<#1>}%
}

\pdfstringdefDisableCommands{%
  \def\texttt#1{<#1>}%
  \def\mathbb#1{#1}%
}
\pdfstringdefDisableCommands{\def\eqref#1{(\ref{#1})}}

\makeatletter
\pdfstringdefDisableCommands{\let\HyPsd@CatcodeWarning\@gobble}
\makeatother

\newenvironment{shelldisplay}{\begin{lstlisting}}{\end{lstlisting}}

\newcommand{\shortcode}[1]{\texttt{#1}}

\lstset{style = console}

% Make subsubsection labeled
\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}

\newcommand*{\laplace}{\mathcal{L}}
\newcommand*{\zerotoinf}{\int_{0}^{\infty}}
\newcommand*{\inftoinf}{\int_{-\infty}^{\infty}}

\title{ODEs}
\author{Jinyuan Wu}

\begin{document}

\maketitle

\section{First order ODEs}

\subsection{Linear ODEs}\label{sec:1st.linear}

An ODE in the form of 
\begin{equation}
    y'(x) + p(x) y(x) = q(x) 
\end{equation}
is considered \concept{linear}.
All linear ODEs can be solved by the following procedure.
First we have 
\begin{equation}
    (y' + py) \ee^{\int p \dd{x}} 
    = q \ee^{\int p \dd{x}},
\end{equation}
and now the LHS is a derivative:
\begin{equation}
    \dv{x} \left(
        y \ee^{\int p \dd{x}}
    \right) = q \ee^{\int p \dd{x}},
\end{equation}
and now we can integrate over $x$ and get 
\begin{equation}
    y \ee^{\int p \dd{x}} = 
    \int q \ee^{\int p \dd{x}} \dd{x},
\end{equation}
\begin{equation}
    y  = \ee^{- \int p \dd{x}}
    \int q \ee^{\int p \dd{x}} \dd{x} .
\end{equation}

\subsection{``Energy-conservation lines'' and exact equations}

Another way to represent the solution of an ODE is 
the form $\phi(x, y) = \const$.
Note that the RHS contains no variables, and we have 
\begin{equation}
    0 = \dv{\phi}{x} = \pdv{\phi}{x} + \pdv{\phi}{y} \dv{y}{x},
    \label{eq:implicit-derivative}
\end{equation}
and thus if 
\begin{equation}
    y' = f(x, y)
\end{equation}
is algebraically equivalent to \eqref{eq:implicit-derivative},
the equation is already solved:
We should find $M, N$ such that 
\begin{equation}
    y' = - \frac{M}{N}, \quad
    M = \pdv{\phi}{x}, \quad
    N = \pdv{\phi}{y},
\end{equation}
and then $\phi(x, y)$ solves the equation.
In this case we say $y' = - M / N$ is \concept{exact}.

To test for exactness, we only have to test whether
\begin{equation}
    \pdv{M}{y} = \pdv{N}{x},
\end{equation}
and if so, the existence of $\phi$ is guaranteed.
(Since we work on a topological trivial space, 
things like cohomology group will not bother us.)
We can now use ``partial integral'' to find $\phi$.

Example: suppose in a calculation we find  
\begin{equation}
    \pdv{\phi}{x} = 2 y^2 + y \ee^{xy}, \quad 
    \pdv{\phi}{y} = 4xy + x \ee^{xy} + 2y.
\end{equation}
After partial integration, we find 
\begin{equation}
    \phi(x, y) = \underbrace{2 xy^2 + \ee^{xy} + h(y)}_{
        \int \pdv{\phi}{x} \dd{x}
    }
    = \underbrace{2 xy^2 + \ee^{xy} + y^2 + g(x)}_{
        \int \pdv{\phi}{y} \dd{y}
    },
\end{equation}
and we have to choose 
\begin{equation}
    h(y) = y^2, \quad g(x) = \const,
\end{equation}
and the solution is 
\begin{equation}
    \phi(x, y) = 2 xy^2 + \ee^{xy} + y^2 + \const.
\end{equation}

Note that even when the decomposition $f = - M / N$
doesn't give an exact equation for us, 
we can still use the method of exact equations:
we can multiply a factor $\mu$ to both $M$ and $N$,
and try to guess the form of $\mu$ so that 
\begin{equation}
    \pdv{(\mu M)}{y} = 
    \pdv{(\mu N)}{x}.
    \label{eq:generalized-exact-condition}
\end{equation}

An example can be found in solving 
\begin{equation}
    y' = - \frac{1}{3x - \ee^{-2y}}.
\end{equation}
We have 
\[
    \pdv{1}{y} = 0, \quad 
    \pdv{(3x- \ee^{-2y})}{x} = 3,
\]
so the equation is not exact if we choose $M = 1$ and 
$N = 3x - \ee^{-2y}$.
However, \eqref{eq:generalized-exact-condition} 
can be fulfilled now: 
it's now 
\[
    \pdv{\mu}{y} = 3 \mu + \left(
        3x - \ee^{-2y}
    \right) \pdv{\mu}{x},
\]
and the most convenient way to solve it 
(we \emph{don't} need to find all solutions of this equation!)
is to let $\mu$ contain $y$ only,
so the tricky term on the RHS disappears,
and thus we choose $\mu = \ee^{3y}$,
and we get 
\[
    \phi(x, y) = \int \mu M \dd{x} 
    = \int \ee^{3y} \dd{x} = x \ee^{3y} + u(y),
\]
\[
    \phi(x, y) = \int \mu N \dd{y}
    = \int (3x \ee^{3y} - \ee^{y} ) \dd{y} 
    = x \ee^{3y} - \ee^{y} + v(x),
\]
so 
\begin{equation}
    \phi(x, y) = x \ee^{3y} - \ee^{y} + \const.
\end{equation}

\subsection{Bernoulli equation}

Consider the following \concept{Bernoulli equation}  
\begin{equation}
    y' + P(x) y = R(x) y^\alpha.
\end{equation}
When $\alpha = 0, 1$, 
the equation can be solved by the standard methods 
for linear first order ODEs.
When this is not the case, 
we may do the substitution 
\begin{equation}
    v = y^\beta,
\end{equation}
and then the equation becomes 
\[
    \frac{1}{\beta} v^{1/\beta - 1} v' + P(x) v^{1/\beta} 
    = R(x) v^{\alpha / \beta},
\]
\begin{equation}
    v' + P(x) v = R(x) v^{1 + \frac{\alpha - 1}{\beta}}.
\end{equation}
The next step is to choose a good beta so that 
the equation gets simplified.
We may want to make to exponent to be zero, 
and this means we should choose 
\begin{equation}
    \beta = 1 - \alpha,
\end{equation}
and the ODE is now 
\begin{equation}
    v' + Pu = R,
\end{equation}
which can then be solved by the method in  
\prettyref{sec:1st.linear}.

\section{Second order ODEs}

\subsection{Linear 2nd order ODE with initial values}

A linear second order ODE has the following form:
\begin{equation}
    y'' + p(x) y' + q(x) y = f(x).
    \label{eq:linear-2nd-ode-1}
\end{equation}
It usually comes with initial value conditions 
\begin{equation}
    y(x_0) = A, \quad y'(x_0) = B.
\end{equation}
This course is about concrete calculations,
but knowing what we are doing makes sense is important.
Here is an existence and uniqueness theorem:
if $p(x), q(x)$, and $f(x)$ are continuous 
over an interval $I$,
and $x_0 \in I$, 
then a unique solution exists for \eqref{eq:linear-2nd-ode-1}
with the initial conditions given above.

Usually, we start by looking at 
the \concept{homogeneous} second order ODE 
\begin{equation}
    y'' + p(x) y' + q(x) y = 0.
    \label{eq:2nd-order-ode-homogeneous}
\end{equation}
The influence of $f(x)$ can be included as the 
``response'' of the LHS.
The full solution of \eqref{eq:2nd-order-ode-homogeneous}
takes the form 
\begin{equation}
    y = c_1 y_1 + c_2 y_2,
\end{equation}
where $c_1, c_2$ are constants to be decided 
by initial conditions,
and $y_1$ and $y_2$ are linearly independent solutions 
of \eqref{eq:2nd-order-ode-homogeneous}.
The \concept{Wronskian} is defined as 
\begin{equation}
    W(x) = 
    \begin{vmatrix}
        y_1 & y_2 \\ y_1' & y_2'
    \end{vmatrix}.
\end{equation} 
By checking if it is non-zero at most points,
we can find whether $y_1$ and $y_2$ are truly linearly independent to each other.

There is a method to arrive at $y_2$ from $y_1$:
we can always take the ansatz 
\begin{equation}
    y_2 = y_1 u, 
    \label{eq:from-y1-to-y2}
\end{equation}
and therefore we get 
\[
    (u'' y_1 + 2 u' y_1' + u y_1'')
    + p (u' y_1 + u y_1')
    + q u y_1 = 0,
\]
and the condition that $y_1$ is a solution to 
\eqref{eq:2nd-order-ode-homogeneous} means 
\begin{equation}
    u'' + \underbrace{\frac{2y_1' + p y_1}{y_1}}_{g(x)} u' = 0,
\end{equation}
which is essentially a first order ODE, 
because we can replace $u'$ by $v$,
and then we find 
\[
    \ln v = - \int g(x) \dd{x},
\]
and 
\begin{equation}
    u(x) = \int \ee^{- \int g(x) \dd{x}} \dd{x}.
\end{equation}

\subsection{Constant coefficients}

The equation 
\begin{equation}
    y'' + A y + By = 0
\end{equation}
can be solved directly by the following construction:
\begin{equation}
    y = c_1 \ee^{\lambda_1 x} + c_2 \ee^{\lambda_2 x},
\end{equation}
where $\lambda_1, \lambda_2$ are solutions of 
\begin{equation}
    \lambda^2 + A \lambda + B = 0.
    \label{eq:character-eq}
\end{equation}

For example, to solve the equation 
\begin{equation}
    y'' - 2 y' + 10 y = 0,
\end{equation}
we just solve 
\[
    \lambda^2 - 2 \lambda + 10 = 0,
\]
which gives us 
\begin{equation}
    \lambda = 1 \pm 3 \ii,
\end{equation}
and therefore a general solution is 
\begin{equation}
    y = \ee^{x} (c_1 \ee^{3 \ii x} + c_2 \ee^{- 3 \ii x}).
\end{equation}
It should be noted that $c_1, c_2$ can be complex,
even when we restrict $y$ in $\mathbb{R}$:
we can let the imaginary part of $y$ vanish 
as long as we impose some constraints over $c_1, c_2$.
If we are determined to work in the real space, 
two alternative linearly independent solutions can be used:
\begin{equation}
    y_1(x) = \ee^{x} \cos(3x), \quad 
    y_2(x) = \ee^{x} \sin(3x).
\end{equation}
Although we can immediately say 
they are linearly independent,
we can use them as a demonstration of the 
Wronskian method: 
now we have 
\begin{equation}
    W(x) = y_1(x) y_2'(x) - y_2(x) y_1'(x) = 3 \ee^{2x},
\end{equation}
which of course isn't constantly zero.

\eqref{eq:character-eq} is faced with the problem 
of having only one solution 
when $A^2 - 4B = 0$.
In this case we need to go back to the standard procedure 
to get $y_2$ from $y_1$.
An example is 
\begin{equation}
    y'' + 6y + 9 = 0,
\end{equation}
for which \eqref{eq:character-eq} only gives 
\begin{equation}
    y_1 = \ee^{-3x}.
\end{equation}
Suppose $y_2 = u \ee^{-3x}$, we have TODO 

\subsection{Euler equation}

A \concept{Euler equation} has the following form:
\begin{equation}
    x^2 y'' + A x y' + B y = 0,
    \label{eq:euler}
\end{equation}
where $A, B$ are constants.
One solution can be immediate found: it always looks like 
\begin{equation}
    y = x^a.
\end{equation}
We then find 
\begin{equation}
    a(a-1) + A a + B = 0.
\end{equation}
If there are two solutions of the equation, 
\eqref{eq:euler} has already been solved.
If not, we can use the trick \eqref{eq:from-y1-to-y2}.

An example: let's solve 
\begin{equation}
    x^2 y'' + 3x y' + y = 0.
\end{equation}
The equation about $a$ is now 
\[
    a(a-1) + 3a + 1 = 0,
\]
and it only has one solution $a = -1$.
Therefore we have 
\[
    y_1 = \frac{1}{x}.
\]
Suppose 
\[
    y_2 = u y_1,
\]
we get 
\[
    x^2 \left(
        \frac{u''}{x^2} - \frac{2 u'}{x} + \frac{2u}{x^2}
    \right)
    + 3 x \left(
        \frac{u'}{x} - \frac{u}{x^2}
    \right)
    + \frac{u}{x} = 0,
\]
which is equivalent to 
\[
    v' x + v = 0, \quad v = u'
\]
the solution of which is 
\[
    \ln v + \ln x = \const,
\]
and therefore 
\[
    v = \frac{C'}{x}, \quad 
    u = C' \ln x + C,
\]
\[
    y_2 = \frac{1}{x} (C' \ln x + C).
\]
This essentially gives \emph{all} solutions we need: 
for $y_1$, we just have $u = 1$,
which corresponds to $C = 1$.
So now the equation is completely solved.

\subsection{Non-homogeneous cases or how to find the linear response}

Now we discuss how to solve 
\begin{equation}
    y'' + p(x) y' + q(x) y = f(x).
    \label{eq:2nd-ode-nonhomogeneous}
\end{equation}
A general solution is 
\begin{equation}
    y(x) = y_{\text{p}}(x) + y_{\text{h}}(x),
\end{equation}
where the subscript p means a particular solution,
and the subscript h means the general solution of the corresponding homogeneous equation.

We need some common sense to find a particular solution.
To solve 
\begin{equation}
    y'' - y' - 2y = 2 x^2 + 5,
\end{equation}
we don't expect $y$ to be, say, $\cos(2x)$:
instead, it's usually the case that $y$ is a polynomial.
An ansatz is 
\[
    y = A x^2 + Bx + C.
\]
We don't want a $x^3$ term because it doesn't appear on RHS.
The equation then becomes 
\[
    2 A - (2 A x + B x) - 2 (A x^2 + Bx + C) = 2 x^2 + 5, 
\]
\[
    -2 A = 2, \quad - 2A - 2 B = 0, \quad 2 A - B - 2 C = 5,
\]
and therefore $A = -1, B = 1, C = - 4$.
Therefore we get a particular solution: 
\begin{equation}
    y_{\text{p}} = - x^2 + x - 4.
\end{equation}

A particular solution may also be determined as 
a ``linear combination'' of homogeneous solutions,
although now the coefficients have temporal variation.
That's to say, we take the ansatz 
\begin{equation}
    y_{\text{p}}(x) = u(x) y_1(x) + v(x) y_2(x).
    \label{eq:2nd-ode-change-coefficients}
\end{equation}
This is quite similar to the procedure introduced in \prettyref{sec:1st.linear}.
After substituting $y$ with \eqref{eq:2nd-ode-change-coefficients} 
in \eqref{eq:2nd-ode-nonhomogeneous},
we get 
\begin{equation}
    u' y_1' + v' y_2' = f.
\end{equation}
Introducing the constraint 
\begin{equation}
    u' y_1 + v' y_2 = 0,
\end{equation}
we have 
\begin{equation}
    \pmqty{y_1 & y_2 \\ y_1' & y_2'} 
    \pmqty{u' \\ v'} = \pmqty{0 \\ f},
\end{equation}
from which we find $u', v'$ and hence $u, v$.
The Wronskian -- the determinant of the matrix on LHS -- 
is non-zero, so the equation always has a solution.

Example: let's solve 
\begin{equation}
    y'' + y = \tan x. 
\end{equation}
We have 
\[
    y_1 = \cos x, \quad y_2 = \sin x,
\]
and therefore 
\[
    W(x) = y_1 y_2' - y_2 y_1' = 1.
\]
So 
\[
    \begin{aligned}
        u' &= - \int \frac{y_2 f}{W(x)} \dd{x} = - \int \frac{\sin^2 x}{\cos x} \dd{x} 
        = - \int \frac{1 - \cos^2 x}{\cos x} \dd{x} \\
        &= - \frac{1}{2} \ln \abs{
            \frac{1 + \sin x}{1 - \sin x}
        } + \sin x,
    \end{aligned}
\]
and similarly we have 
\[
    v = 
\]

\subsection{Analyticity}

The stimulus can be non-analytic.
$f$ is analytic at $x_0$, if we can expand it into a power series around $x_0$:
\begin{equation}
    f(x) = \sum_{n=0}^\infty a_n (x - x_0)^n 
\end{equation}
in a interval around $x_0$.
The function $f(x) = \ln x$, then, is not analytic at $x = 0$ -- 
but $f(x) = \ln (x + 1)$ is analytic at $x=0$,
though not at $x=-1$.

There is a theorem: if $p$, $q$, $f$ are all analytic at $x_0$,
then \eqref{eq:2nd-ode-nonhomogeneous} together with conditions 
$y(x_0) = A$ and $y'(x_0) = B$ 
has a unique solution that is analytic at $x_0$.

An example: 
\begin{equation}
    x'' + y' - xy = 0, \quad y(0) = -2, y'(0) = 0.
\end{equation}
Suppose 
\[
    y(x) = \sum_{n=0}^\infty a_n x^n, \quad 
    y'(x) = \sum_{n=0}^\infty n a_n x^{n-1}, \quad 
    y''(x) = \sum_{n=0}^\infty n (n+1) a_n x^{n-2},
\]
we have 
\[
    \begin{aligned}
        0 &= \sum_{n=0}^\infty n (n - 1) a_n x^{n-2}
        + \sum_{n=0}^\infty n a_n x^{n-1}
        - \sum_{n=0}^\infty a_n x^{n+1} \\
        &= a_1 + 2 a_2 + 
        \sum_{n=1}^\infty x^n (
            (n + 2) (n + 1) a_{n+2} 
            + (n + 1) a_{n + 1} 
            - a_{n - 1}
        ),
    \end{aligned}
\]
and therefore 
\[
    a_1 + 2 a_2 = 0, \quad 
    a_{n+2} = - \frac{a_{n+1}}{n+2} + \frac{a_{n-1}}{(n+1) (n+2)}.
\]
The conditions $y(0) = -2$ and $y' (0) = 0$ means 
\[
    a_0 = -2, \quad a_1 = 0,
\]
and then we can in principle find all $a_n$'s -- 
although it's often hard to see a pattern and write down a closed-form expression for $a_n$.

Now we consider the equation 
\begin{equation}
    P(x) y'' + Q(x) y' + R(x) y = F(x).
\end{equation}
Of course we can divide the equation with $P(x)$ and go back to \eqref{eq:2nd-ode-nonhomogeneous},
but if $P(x)$ is zero at some points, 
$p, q, f$ in \eqref{eq:2nd-ode-nonhomogeneous} are no longer always analytic.
Thus the solution isn't guaranteed to be analytic everywhere. 
In other words, we no longer have a power series solution -- or do we?
We can still try a generalized power series, like 
\begin{equation}
    y(x) = \sum_{n=0}^\infty c_n (x - x_0)^{n + r},
\end{equation}
where $r$ can be a fraction. 
This is guaranteed with
\begin{equation}
    (x - x_0) y'' + Q(x) y' + R(x) y = F(x).
\end{equation}

To demonstrate this, 
consider 
\begin{equation}
    y'' + \frac{1}{2x} y' - \frac{1}{4x} y = 0.
\end{equation}
We plug 
\[
    y(x) = \sum_{n=0}^\infty c_n x^{n + r}
\]
into 
\[
    4 x y'' + 2 y' - y = 0,
\]
and get 
\[
    \sum_{n=0}^\infty \left(
        4 (n + r) (n + r - 1) c_n x^{n + r - 1}
        + 2 (n + r) c_n x^{n + r - 1} 
        - c_n x^{n + r}
    \right) = 0.
\]
The coefficient of the $x^{r-1}$ term is 
\[
    4 r (r - 1) + 2 r = 0,
\]
from which we find $r = 0, 1/2$.
For the rest of the terms, we have 
\[
    4 (n + r) (n + r - 1) c_n
    + 2 (n + r) c_n
    - c_{n-1} = 0,
\]
\[
    c_n = \frac{c_{n-1}}{2 (n + r) (2 n + 2 r - 1)}.
\]
When $r = 0$, this gives 
\[
    c_n = \frac{c_{n-1}}{2n (2n - 1)} , \quad 
    c_n = \frac{c_0}{(2n)!},
\]
and we get 
\begin{equation}
    y(x) = \sum_{n=0}^\infty \frac{x^n}{(2n)!}.
\end{equation}
When $r = 1/2$, this gives 
\[
    c_n = \frac{c_{n-1}}{(2n + 1) \cdot 2n}, \quad 
    c_n = \frac{c_0}{(2 n + 1)!},
\]
and we get 
\begin{equation}
    y(x) = \sum_{n=0}^\infty \frac{x^{n + 1/2}}{(2 n + 1)!}.
\end{equation}
So we have already obtained two independent solutions. 

\section{The Laplace transformation}

The \concept{Laplace transformation} is defined for a function $f(t)$ as 
\begin{equation}
    \mathcal{L}[f(t)] = F(s) = \int_{0}^{\infty} f(t) \ee^{-st} \dd{t},
\end{equation}
for every $s$ where the integral converges. 
It's easy to see that $\mathcal{L}$ is linear, 

\subsection{Laplace transforms of basic elementary functions}

The simplest Laplace transform is 
\begin{equation}
    \mathcal{L}[1] = \int_{0}^{\infty} \ee^{-st} \dd{t} = \frac{1}{s},
    \quad s > 0,
\end{equation}
and by integration by parts, we also have 
\begin{equation}
    \laplace[t] = \int_{0}^{\infty} t \ee^{-st} \dd{t}
    = \frac{1}{s^2}.
\end{equation}
By multiple rounds of integration by parts, we have 
\begin{equation}
    \laplace[t^2] = \frac{2}{s^3},
\end{equation}
and by iteratively using integration by parts we get 
\begin{equation}
    \laplace [t^n] = \frac{n!}{s^{n+1}}.
\end{equation}

Aside from polynomials, 
we have 
\begin{equation}
    \laplace[\ee^{at}] = \zerotoinf \ee^{-(s-a) t} \dd{t} = \frac{1}{s- a}, 
    \quad s > a. 
    \label{eq:eat-to-s-minus-a}
\end{equation}
Since we have 
\[
    \cos x = \frac{\ee^{\ii x} + \ee^{- \ii x}}{2}, \quad 
    \sin x = \frac{\ee^{\ii x} - \ee^{- \ii x}}{2 \ii}, 
\]
we have 
\begin{equation}
    \laplace[\cos (at)] = \frac{1}{2} \left(
        \frac{1}{s - \ii a} + \frac{1}{s + \ii a}
    \right) = \frac{s}{s^2 + a^2}.
\end{equation}
Similarly we have 
\begin{equation}
    \laplace[\sin(at)] = \frac{a}{s^2 + a^2}.
\end{equation}

Consider a pulse signal 
\begin{equation}
    f(t) = \begin{cases}
        0, &\quad t < a \text{ or } t > b, \\
        1, &\quad a \leq t \leq b.
    \end{cases}
\end{equation}
This can be easily implemented by the Heaviside function: 
we have 
\begin{equation}
    f(t) = H(t-a) - H(t-b).
\end{equation}
So the Laplace transform is 
\begin{equation}
    \laplace[f(t)] = \ee^{-sa} \laplace[1] - \ee^{- sb} \laplace[1]
    = \frac{1}{s} (\ee^{-sa} - \ee^{-sb}).
\end{equation}

\subsection{Laplace transform of differential equations}

We have 
\begin{equation}
    \begin{aligned}
        \laplace[f'(t)] &= \zerotoinf \dv{f}{t} \ee^{-st} \dd{t} 
        = \eval{f \ee^{-st}}_{s=0}^{\infty} 
        - \zerotoinf f \dv{t} \ee^{-st} \dd{t} \\ 
        &= - f(0) + s \zerotoinf f(t) \ee^{-st} \dd{t} 
        = s \laplace[f(t)] - f(0).
    \end{aligned}
\end{equation}
Applying this twice, we get 
\begin{equation}
    \laplace[f''(t)] = s \laplace[f'(t)] - f'(0) = s^2 \laplace[f(t)] - s f(0) - f'(0).
\end{equation}
The general formula is therefore 
\begin{equation}
    \laplace[f^{(n)}(t)] = s^n \laplace[f(t)] - s^{n-1} f(0) - s^{n-2} f'(0) - \cdots - f^{(n-1)}(0).
\end{equation}

This can be used to solve ODEs. 
Consider, for example, 
\begin{equation}
    y'' + 4y' + 3y = \ee^{t}, \quad 
    y(0) = 0, \quad y'(0) = 2.
\end{equation}
We have (below we follow the convention to use big letters to refer to functions in the Laplace space)
\[
    \laplace[\text{LHS}] = 
    (s^2 Y(s) - s y(0) - y'(0))
    + 4 (s Y(s) - y(0))
    + 3 Y(s) = 
    (s^2 + 4s + 3) Y(s) - 2,
\]
where we have already applied the initial conditions, 
and 
\[
    \laplace[\text{RHS}] = \frac{1}{s-1}.
\]
So what need to be done is to solve 
\[
    (s^2 + 4s + 3) Y(s) - 2 = \frac{1}{s - 1},
\]
and we get 
\begin{equation}
    Y(s) = \frac{2s-1}{(s-1) (s+1) (s+3)}.
\end{equation}

Thus, once we do the inverse Laplace transformation, 
we get $y(t)$. 
It's possible to do an inverse integral transformation,
but in this case, what's more convenient is to make the decomposition 
\[
    Y(s) = \frac{A}{s - 1} + \frac{B}{s + 1} + \frac{C}{s + 3},
\]
and we can find 
\[
    A = \frac{1}{8}, \quad B = \frac{3}{4}, \quad C = - \frac{7}{8}.
\]
Then we can read the Laplace transformation table in the inverse direction:
from \eqref{eq:eat-to-s-minus-a}, 
we find 
\begin{equation}
    y(t) = \frac{1}{8} \ee^{t} + \frac{3}{4} \ee^{-t} - \frac{7}{8} \ee^{-3t}.
\end{equation}
So the linear 2nd order ODE is completely solved. 

TODO: whether we will encounter things like 
\begin{equation}
    \laplace[f] = \frac{1}{s}, \quad s > 100.
\end{equation}
It seems in most real world problems, 
we don't really need to worry about the region of allowed $s$:
we just extend the region of allowed $s$ as large as we can, 
solve the algebraic equation in the Laplace space, 
and then go back. 

We can repeat the procedure in the last example 
for a system of ODEs. 
Consider for example 
\begin{equation}
    x' - 2y' = 1, \quad x' - x + y = 0, \quad x(0) = y(0) = 0.
\end{equation}
For the first equation, Laplace transform gives 
\[
    s X(s) - x(0) - 2 (s Y(s) - y(0)) = \laplace[1] = \frac{1}{s},
\]
and for the second equation we have 
\[
    s X(s) - x(0) - X(s) + Y(s) = 0.
\]
Solving this linear equation system, we get 
\[
    X(s) = \frac{1}{s^2 (2s - 1)}, \quad 
    Y(s) = - \frac{s-1}{s^2 (2s - 1)}.
\]
Since 
\[
    X(s) = - \frac{2}{s} - \frac{1}{s^2} + 2 \frac{1}{s-1/2},
\]
we have 
\begin{equation}
    x(t) = - 2 - t + 2 \ee^{t/2}.
\end{equation}
Similarly, 
\begin{equation}
    y(t) = -1 + \ee^{t/2} - t.
\end{equation}

\subsection{Shifting of $s$ and $t$}

Laplace transform of an integral can also be found. 
Here we investigate into things like 
\[
    \int_0^t f(t') \dd{t'},
\]
and we need to pay attention to what variable is inside the integration 
and what variable is exposed to the Laplace operator. 
We have 
\begin{equation}
    \begin{aligned}
        \laplace\left[
            \int_{0}^{t} f(t') \dd{t'}
        \right] &= - \frac{1}{s} \int_{t'=0}^{\infty} \int_{0}^{t} f(t') \dd{t'} \dd \ee^{-st} \\
        &= - \frac{1}{s} \left(
            \eval{\ee^{-st} \int_{0}^{t} f(t') \dd{t'}}_{0}^{\infty} - 
            \zerotoinf f(t) \ee^{-st} \dd{t} 
        \right) \\
        &= \frac{1}{s} \laplace[f(t)].
    \end{aligned}
\end{equation}
This is the inverse of the rule of derivatives above, 
which is expected. 

Another theorem is the \concept{$s$-shifting theorem}: 
we have 
\begin{equation}
    \laplace[\ee^{at} f(t)] = F(s-a) = \laplace[f(t)]_{s \to s - a}.
\end{equation}
This is exactly what leads to \eqref{eq:eat-to-s-minus-a}.
Correspondingly we have the $t$-shifting theorem: 
\begin{equation}
    \laplace[f(t-a) H(t-a)] = \ee^{-sa} \laplace[f(t)], 
\end{equation}
where $H(t-a)$ is 1 when $t \geq 0$ and zero otherwise,
and $a > 0$. 

The theorems can be used to evaluate Laplace transforms of complex functions. 
We have 
\begin{equation}
    \laplace[t^2 \ee^{-t}] = \laplace[t^2] |_{s \to s + 1}
    = \frac{2}{(s+1)^3}.
\end{equation}

\subsection{Convolution}

The \concept{convolution integral} of $f(t)$ and $g(t)$ is defined as 
\begin{equation}
    f \otimes g = \int_0^t f(t') g(t-t') \dd{t'}.
\end{equation}
This can be found very frequently in science: 
it appears when we deal with interaction:
for example, the $t - t'$ time may come from indirect interaction 
(where $t'$ is the time an intermediate step happens).

A way around the reasonable but hard to calculate convolution integral 
is taking its Laplace transform. 
We have 
\begin{equation}
    \begin{aligned}
        \laplace[f \otimes g]
        &= \int_{0}^{\infty} \ee^{-st} \int_{0}^{t} f(t') g(t-t') \dd{t'} \dd{t}  \\
        &= \int_{0}^{\infty} f(t') \int_{t'}^{\infty} g(t-t') \ee^{-st} \dd{t} \dd{t'} \\
        &= \int_{0}^{\infty} f(t') \int_{0}^{\infty} g(t'') \ee^{- s (t'' + t')} \dd{t''} \dd{t'} \\
        &= \int_{0}^{\infty} f(t') \ee^{-st'} \int_{0}^{\infty} g(t'') \ee^{- s t''} \dd{t''} \\
        &= \laplace[f] \laplace[g].
    \end{aligned}
\end{equation}
The second line uses another way to see the integration region: 
by saying $0 < t < \infty$, $0 < t' < t$, 
we also mean $0 < t' < \infty$, $t' < t < \infty$.
The third line replaces $t - t'$ with $t''$.
The final result no longer contains convolution.

One application of this fact is shown in the following example.
Consider 
\[
    \frac{1}{s^2 - a^2} = \underbrace{\frac{1}{s-a}}_{G(s)} \cdot \underbrace{\frac{1}{s+a}}_{F(s)}.
\]
What's its inverse Laplace transform?
By shifting theorem, we have 
\[
    f(t) = \ee^{-at}, \quad g(t) = \ee^{at}, 
\]
and therefore 
\begin{equation}
    \laplace^{-1}\left[
        \frac{1}{s^2 - a^2}
    \right] = \int_{0}^{t} \ee^{-at'} \ee^{a(t-t')} \dd{t'} = 
    \ee^{at} \int_{0}^{t} \ee^{-2at'} \dd{t'} = \frac{1}{a} \sinh (at). 
\end{equation}

We can also use the convolution theorem to give solutions to very generic equations. 
Consider the following ODE problem: 
\begin{equation}
    y'' - 5y' + 6y = f(t), \quad f(0) = f'(0) = 0.
\end{equation}
The point here is we \emph{don't} know what is $f(t)$,
but still want to give a template of the solution.
So we just do Laplace transform:
\[
    (s^2 - 5 s + 6) Y(s) = F(s). 
\]
To find the Laplace transform of $1 / (s^2 - 5s + 6)$, we have 
\[
    \laplace^{-1} \left[
        \frac{1}{(s^2 - 5s + 6)}
    \right]
    = \laplace^{-1} \left[
        \frac{1}{s-3} - \frac{1}{s-2} 
    \right] = \ee^{3t} - \ee^{2t}, 
\]
and 
\begin{equation}
    \begin{aligned}
        y(t) &= \laplace^{-1}[Y(s)] = \laplace^{-1} \left[
            \frac{1}{(s^2 - 5s + 6)} F(s)
        \right] \\
        &= \laplace^{-1} \left[
            \frac{1}{(s^2 - 5s + 6)}
        \right] \otimes f(t) \\
        &= \int_{0}^{t} f(t) (\ee^{3(t - t')} - \ee^{2 (t - t')}) \dd{t'}.
    \end{aligned}
\end{equation}

\section{Fourier analysis}

\subsection{Definition of Fourier series}

Consider a real, integrable function $f(x)$ on $- L \leq x \leq L$.
The Fourier series of it is 
\begin{equation}
    f(x) = \frac{1}{2} a_0 + \sum_{n=1}^{\infty} \left(
        a_n \cos(\frac{n \pi x}{L})
        + b_n \sin(\frac{n \pi x}{L})
    \right).
\end{equation}
This form has a very intuitive physical picture: 
any sound can be obtained by mixing a series of 
simple harmonic oscillations. 

The coefficients can be obtained by integrating from $-L$ to $L$. 
Since 
\begin{equation}
    \int_{-L}^{L} \sin(\frac{n \pi x}{L}) \dd{x} = 
    \int_{-L}^{L} \cos(\frac{n \pi x}{L}) \dd{x} = 0,
\end{equation}
we have 
\begin{equation}
    \frac{1}{L} \int_{-L}^{L} f(x) \dd{x} = a_0,
\end{equation}
and similarly since 
\begin{equation}
    \int_{L}^{-L} \cos(\frac{n \pi x}{L}) \cos(\frac{m \pi x}{L}) = 
    \int_{L}^{-L} \sin(\frac{n \pi x}{L}) \sin(\frac{m \pi x}{L}) = 
    \begin{cases}
        0, \quad n \neq m, \\
        L, \quad n = m,
    \end{cases}
\end{equation}
and the cross term 
\begin{equation} 
    \int_{L}^{-L} \sin(\frac{n \pi x}{L}) \cos(\frac{m \pi x}{L}) = 0,
\end{equation}
we find 
\begin{equation}
    a_m = \frac{1}{L} \int_{-L}^{L} f(x) \cos(\frac{m \pi x}{L}) \dd{x}, 
\end{equation}
\begin{equation}
    b_m = \frac{1}{L} \int_{-L}^{L} f(x) \sin(\frac{m \pi x}{L}) \dd{x}.
\end{equation}
Note that the first equation also covers the $m = 0$ case. 
The $1 / L$ normalization factor can be found by considering the $f(x) = 1$ case. 

Strictly speaking, 
the above equations only make sense when we can define the integrals. 
And another question 
-- whether the Fourier series really \emph{converges} to the original function $f(x)$ -- 
is a question needing investigation.
Another question is whether we can have Fourier series for discontinuous functions, 
or in other words, 
how can a stepwise function be simulated by a series of sine waves. 
Another problem is it seems at $x = \pm L$,
the Fourier series have to converge to the \emph{save} value,
because every term has the same value at $x = \pm L$.

To lay a rigorous foundation for Fourier series, 
we state the following theorem:
if $f(x)$ is piecewise smooth on $[-L, L]$, 
then the Fourier series of $f(x)$ converges to 
$(f(x^+) + f(x^-)) / 2$ at each $x \in (-L, L)$, 
and to $(f(-L) + f(L)) / 2$ at $x = \pm L$. 
Here the term \concept{piecewise smooth} means 
that the function may have jumps in $[-L, L]$, 
but there may be one or more point 
at which $f(x^+)$ and $f(x^-)$ are different. 
At such points of confusion, 
we just take an average of the two options. 

As an example, let's try Fourier series on 
\begin{equation}
    f(x) = \begin{cases}
        1, \quad & 0 \leq x \leq L \\
        -1, \quad & - L \leq x < 0.
    \end{cases}
\end{equation}
So 
\[
    a_0 = \frac{1}{L} (-L + L) = 0,
\] 
and indeed $a_m = 0$ for all $m$'s: 
the function in question is odd, 
and all the $a$ terms are even,
so automatically they all vanish. 
For the odd terms, we have 
\[
    \begin{aligned}
        b_n &= \frac{1}{L} \int_{-L}^{L} f(x) \sin(\frac{n \pi x}{L}) \dd{x} \\
        &= \frac{1}{L} \left(
            - \int_{-L}^{0} \sin(\frac{n \pi x}{L}) \dd{x} 
            + \int_{0}^{L} \sin(\frac{n \pi x}{L}) \dd{x} 
        \right) \\
        &= \frac{1}{n \pi} (2 - 2 \cos (n \pi)),
    \end{aligned}
\]
and therefore 
\begin{equation}
    b_n = \begin{cases}
        \frac{4}{n \pi} , &\quad n = 1, 3, 5, \dots,\\
        0, &\quad \text{otherwise},
    \end{cases}
\end{equation}
and 
\begin{equation}
    f(x) = \frac{4}{\pi} \sum_{n=1}^{\infty} \frac{1}{2n-1} \sin(\frac{(2n-1) \pi x}{L}).
\end{equation}

\subsection{Parseval's theorem}

We have 
\[
    \int_{-L}^{L} f(x)^2 \dd{x} 
    = \frac{1}{2} a_0 \int_{-L}^{L} f(x) \dd{x}
    + \sum_{n=1}^{\infty} \left(
        a_n \int_{-L}^{L} \cos(\frac{n \pi x}{L}) f(x) \dd{x}
        + b_n \int_{-L}^{L} \sin(\frac{n \pi x}{L}) f(x) \dd{x}
    \right),
\]
and the integral in each term again gives a Fourier coefficient,
and in the end we get 
\begin{equation}
    \frac{1}{L} \int_{-L}^{L} f(x)^2 \dd{x}
    = \frac{1}{2} a_0 
    + \sum_{n=1}^{\infty} (a_n^2 + b_n^2). 
\end{equation}
This is called \concept{Parseval's theorem}. 

\subsection{Complex Fourier series}

We can also change the way we show the Fourier series into the follows: 
\begin{equation}
    f(x) = \sum_{n=-\infty}^{\infty} d_n \ee^{\ii n \pi x / L}.
\end{equation}
This is merely a linear combination of the original definition in terms of $\sin$ and $\cos$.
Repeating the integral over $[-L, L]$, we get 
\[
    \begin{aligned}
        \int_{-L}^{L} f(x) \ee^{- \ii m \pi x / L} \dd{x} 
        &= \sum_{n=-\infty}^{\infty} d_n
        \int_{-L}^{L}  \ee^{\ii (n - m) \pi x / L} \dd{x} \\
        &= \sum_{n=-\infty}^{\infty} d_n
        \frac{2 L }{(n - m) \pi} \sin (n - m) \pi. 
    \end{aligned}
\] 
When $n \neq m$, the RHS is zero; 
when $n = m$, we need to take the $n - m \to 0$ limit and get 
\[
    \frac{2 L }{(n - m) \pi} \sin (n - m) \pi = 2 L.  
\]
So the conclusion is 
\begin{equation}
    d_m = \frac{1}{2L} 
        \int_{-L}^{L} f(x) \ee^{- \ii m \pi x / L} \dd{x} .
\end{equation}

This version of Fourier series immediately leads to Fourier transform when $L \to \infty$:
We define 
\begin{equation}
    \omega_n = \frac{n \pi}{L}, 
\end{equation}
and we get 
\begin{equation}
    \Delta \omega = \frac{\pi}{L}.
\end{equation}
Therefore 
\[
    \begin{aligned}
        f(x) &= \sum_{n=-\infty}^{\infty} \ee^{\ii n \pi x / L} \cdot \frac{1}{2L} 
            \int_{-L}^{L} f(x') \ee^{- \ii n \pi x' / L} \dd{x'} \\
            &= \sum_{n=-\infty}^{\infty} \frac{\Delta \omega}{2\pi} \ee^{\ii \omega_n x} \cdot  
            \int_{-L}^{L} f(x') \ee^{- \ii \omega_n x' } \dd{x'} \\
            &= \int_{-\infty}^{\infty} \frac{\dd{\omega}}{2\pi} \ee^{\ii \omega x} \cdot 
            \int_{-\infty}^{\infty} f(x') \ee^{- \ii \omega x'} \dd{x'}.
    \end{aligned}
\]
So we can just define 
\begin{equation}
    F(\omega) = \int_{-\infty}^{\infty} f(x') \ee^{- \ii \omega x'} \dd{x'},
\end{equation}
and 
\begin{equation}
    f(x) =\int_{-\infty}^{\infty} \frac{\dd{\omega}}{2\pi} \ee^{\ii \omega x} F(\omega).
\end{equation}

As an example, we evaluate the Fourier transform of 
\begin{equation}
    f(x) = x \ee^{- \abs*{x}}.
\end{equation}

\section{Sturm-Liouville equations}

A \concept{Sturm-Liouville} equation has the form 
\begin{equation}
    (ry')' + (q + \lambda p) y = 0
    \label{eq:sl.scheme}
\end{equation}
on an interval $[a, b]$. 
$r, q, p$ are known functions and $\lambda$ is a constant which is not known.
The goal is to find values of $\lambda$ 
for which the equation has non-trivial solutions.
This is essentially an eigenvalue problem. 

There are three types of boundary conditions that frequently appear. 
A \concept{regular} boundary condition looks like 
\begin{equation}
    A_1 y(a) + A_2 y'(a) = 0, \quad 
    B_1 y(b) + B_2 y'(b) = 0.
\end{equation}
TODO 

Here we state the following theorem:
if $\lambda_n$ and $\lambda_m$ are two eigenvalues with eigenfunctions $\phi_n$ and $\phi_m$,
then we have the orthogonal condition
\begin{equation}
    \int_{a}^{b} p(x) \phi_n(x) \phi_m(x) \dd{x} = 0.
    \label{eq:sl.orthogonal}
\end{equation}
To prove the theorem, we start with 
\[
    (r \phi_n')' \phi_m + (q + \lambda_n p) \phi_n \phi_m = 0,
\]
and 
\[
    (r \phi_m') \phi_n + (q + \lambda_m p) \phi_m \phi_n = 0,
\]
so we have 
\[
    (r \phi_n')' \phi_m 
    - (r \phi_m')' \phi_n 
    + (\lambda_n - \lambda_m) p \phi_m \phi_n = 0.
\]
So by integration by parts, we have 
\[
    \begin{aligned}
        &\quad (\lambda_n - \lambda_m) \int_{a}^{b} \dd{x} p \phi_m \phi_n   \\
        &= \int_{a}^{b} ((r \phi_n')' \phi_m 
        - (r \phi_m')' \phi_n ) \dd{x} \\
        &= \eval{
            ((r \phi_n') \phi_m 
            - (r \phi_m') \phi_n )
        }_a^b - 
        \int_{a}^{b} ((r \phi_n') \phi_m'
        - (r \phi_m') \phi_n' ) \dd{x} \\
        &= \eval{
            ((r \phi_n') \phi_m 
            - (r \phi_m') \phi_n )
        }_a^b.
    \end{aligned}
\]
We can easily verify that the RHS vanishes at $x = a, b$
for all reasonable boundary conditions 
(listed above). 
Therefore the LHS has to be zero, 
and since $\lambda_n \neq \lambda_m$,
the conclusion has to be that 
\eqref{eq:sl.orthogonal} is correct. 

The orthogonality property means we can express functions on $[a, b]$ 
as series expansions in $\{\phi_n\}$: 
we have 
\begin{equation}
    f(x) = \sum_{n=1}^{\infty} c_n \phi_n(x),
    \label{eq:sl.expansion}
\end{equation}
where 
\begin{equation}
    c_m = \frac{
        \int_{a}^{b} f(x) \phi_m(x) \dd{x}
    }{
        \int_{a}^{b} \phi_m(x)^2 \dd{x}
    }.
\end{equation}
Note that here $f(x)$ should follow the boundary conditions used to decide $\{\phi_m\}$;
however, there is no guarantee that 
the RHS of \eqref{eq:sl.expansion} is continuous at $x = a, b$:
it's usually the case that the value of the RHS at $x = a^+$
equals $f(a)$.
Therefore, in practical calculation, 
we can disregard the boundary condition requirement of $f(x)$,
and do \eqref{eq:sl.expansion} to \emph{all} $f(x)$,
as long as we only rely on the RHS 
in $(a, b)$ where $f(x)$ is continuous.
Here we encounter the same case in Fourier series.
If we only take finite terms in \eqref{eq:sl.expansion}, 
then the RHS may have wavy features around these discontinuous points, 
which is an indication of the subtleties concerning discontinuity.

Indeed, the Fourier series is just a specific case of 
the orthogonality theorem in Sturm-Liouville theory. 
Consider the problem 
\begin{equation}
    y'' + \lambda y = 0, \quad 
    y(-L) = y(L) = 0.
\end{equation}
The general solution of the differential equation is 
\[
    y(x) = A \cos \sqrt{\lambda} x + B \sin \sqrt{\lambda} x.
\]
When $B = 0$, 
the boundary condition means 
\begin{equation}
    \sqrt{\lambda} L = \left( n - \frac{1}{2} \right) \pi,
\end{equation}
and when $A = 0$, 
we have 
\begin{equation}
    \sqrt{\lambda} L = n \pi.
\end{equation}

There are other Sturm-Liouville problems that may be useful in physics. 
One is the \concept{Laguerre equation} 
\begin{equation}
    x y'' + (1 - x) y' + \lambda y = 0,
    \label{eq:sl.laguerre}
\end{equation}
and another is the \concept{Hermite equation} 
\begin{equation}
    y'' - 2 x y' + \lambda y = 0.
    \label{eq:sl.hermite}
\end{equation}
Note that although \eqref{eq:sl.laguerre} and \eqref{eq:sl.hermite} 
don't fit in the scheme of \eqref{eq:sl.scheme},
they indeed are Sturm-Liouville problems.
For \eqref{eq:sl.hermite}, 
we can multiply a factor $h$ to it and get 
\[
    h y'' - 2 x h y' + \lambda h y = 0,
\]
and dictate that 
\[
    h' = - 2 x h.
\]
Then we see we can just take 
\[
    h = \ee^{- x^2},
\]
so \eqref{eq:sl.hermite} is a Sturm-Liouville problem with 
\begin{equation}
    r(x) = \ee^{-x^2}, \quad p(x) = \ee^{-x^2}, \quad q(x) = 0.
\end{equation}
For \eqref{eq:sl.laguerre} we can do the same and get 
\begin{equation}
    r(x) = x \ee^{-x}, \quad 
    p(x) = \ee^{-x}, \quad 
    q(x) = 0.
\end{equation}

Another important equation is the \concept{Legendre} equation, 
which is 
\begin{equation}
    (1 - x^2) y'' - 2 x y + \lambda y = 0,
\end{equation}
We can demonstrate how to solve the equation using series expansion. 
Suppose 
\begin{equation}
    y(x) = \sum_{n=0}^{\infty} a_n x^n,
\end{equation}
we have 
\[
    (1 - x^2) \sum_{n=2}^{\infty} n(n-1) a_n x^{n-2}
    - 2x \sum_{n=1}^{\infty} n a_n x^{n-1}
    + \lambda \sum_{n=0}^{\infty} a_n x^n = 0,
\]
which is equivalent to 
\[
    2 a_2 + \lambda a_0 = 0, 
\]
\[
    6 a_3 - 2 a_1 + \lambda a_1 = 0,
\]
and 
\[
    (n+2)(n+1) a_{n+2} - n(n-1) a_n - 2 n a_n + \lambda a_n = 0.
\]
So we find 
\[
    a_2 = - \frac{\lambda}{2} a_0, \quad   
    a_3 = \frac{2 - \lambda}{6} a_1, 
\]
and by repeating this we find 
\begin{equation}
    a_{n+2} = \frac{
        n(n+2) - \lambda
    }{
        (n + 2) (n + 1)
    } a_n.
\end{equation}
So we find 
\begin{equation}
    a_{2k} = - \frac{
        \lambda (3 \cdot 2 - \lambda) \cdots ((2k-1)(2k-2) - \lambda)
    }{
        (2k)!
    } a_0,
\end{equation}
and 
\begin{equation}
    a_{2k+1} = \frac{
        (2 - \lambda) (4 \cdot 3 - \lambda) \cdots (2k \cdot (2k - 1) - \lambda)
    }{
        (2 k + 1)!
    } a_1.
\end{equation}

\end{document}