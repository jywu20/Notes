\documentclass[hyperref, a4paper]{article}

\usepackage{geometry}
\usepackage{titling}
\usepackage{titlesec}
% No longer needed, since we will use enumitem package
% \usepackage{paralist}
\usepackage{enumitem}
\usepackage{footnote}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathtools}
\usepackage{bbm}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{physics}
\usepackage{tensor}
\usepackage{siunitx}
\usepackage[version=4]{mhchem}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{underscore}
\usepackage{autobreak}
\usepackage[ruled, vlined, linesnumbered]{algorithm2e}
\usepackage{nameref,zref-xr}
\zxrsetup{toltxlabel}
\usepackage[colorlinks,unicode]{hyperref} % , linkcolor=black, anchorcolor=black, citecolor=black, urlcolor=black, filecolor=black
\usepackage[most]{tcolorbox}
\usepackage{prettyref}

% Page style
\geometry{left=3.18cm,right=3.18cm,top=2.54cm,bottom=2.54cm}
\titlespacing{\paragraph}{0pt}{1pt}{10pt}[20pt]
\setlength{\droptitle}{-5em}

% More compact lists 
\setlist[itemize]{
    %itemindent=17pt, 
    %leftmargin=1pt,
    listparindent=\parindent,
    parsep=0pt,
}

\setlist[enumerate]{
    %itemindent=17pt, 
    %leftmargin=1pt,
    listparindent=\parindent,
    parsep=0pt,
}

% Math operators
\DeclareMathOperator{\timeorder}{\mathcal{T}}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\legpoly}{P}
\DeclareMathOperator{\primevalue}{P}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\res}{Res}
\newcommand*{\ii}{\mathrm{i}}
\newcommand*{\ee}{\mathrm{e}}
\newcommand*{\const}{\mathrm{const}}
\newcommand*{\suchthat}{\quad \text{s.t.} \quad}
\newcommand*{\argmin}{\arg\min}
\newcommand*{\argmax}{\arg\max}
\newcommand*{\normalorder}[1]{: #1 :}
\newcommand*{\pair}[1]{\langle #1 \rangle}
\newcommand*{\fd}[1]{\mathcal{D} #1}
\DeclareMathOperator{\bigO}{\mathcal{O}}

% TikZ setting
\usetikzlibrary{arrows,shapes,positioning}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{calc}
\tikzstyle arrowstyle=[scale=1]
\tikzstyle directed=[postaction={decorate,decoration={markings,
    mark=at position .5 with {\arrow[arrowstyle]{stealth}}}}]
\tikzstyle ray=[directed, thick]
\tikzstyle dot=[anchor=base,fill,circle,inner sep=1pt]

% Algorithm setting
% Julia-style code
\SetKwIF{If}{ElseIf}{Else}{if}{}{elseif}{else}{end}
\SetKwFor{For}{for}{}{end}
\SetKwFor{While}{while}{}{end}
\SetKwProg{Function}{function}{}{end}
\SetArgSty{textnormal}

\newcommand*{\concept}[1]{{\textbf{#1}}}

% Embedded codes
\lstset{basicstyle=\ttfamily,
  showstringspaces=false,
  commentstyle=\color{gray},
  keywordstyle=\color{blue}
}

\lstdefinestyle{console}{
    basicstyle=\footnotesize\ttfamily,
    breaklines=true,
    postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space}
}

% Reference formatting
\newrefformat{fig}{Figure~\ref{#1}}

% Color boxes
\tcbuselibrary{skins, breakable, theorems}
\newtcbtheorem[number within=section]{warning}{Warning}%
  {colback=orange!5,colframe=orange!65,fonttitle=\bfseries, breakable}{warn}
\newtcbtheorem[number within=section]{note}{Note}%
  {colback=green!5,colframe=green!65,fonttitle=\bfseries, breakable}{note}
\newtcbtheorem[number within=section]{info}{Info}%
  {colback=blue!5,colframe=blue!65,fonttitle=\bfseries, breakable}{info}

% Displaying texts in bookmarkers

\pdfstringdefDisableCommands{%
  \def\\{}%
  \def\ce#1{<#1>}%
}

\pdfstringdefDisableCommands{%
  \def\texttt#1{<#1>}%
  \def\mathbb#1{#1}%
}
\pdfstringdefDisableCommands{\def\eqref#1{(\ref{#1})}}

\makeatletter
\pdfstringdefDisableCommands{\let\HyPsd@CatcodeWarning\@gobble}
\makeatother

\newenvironment{shelldisplay}{\begin{lstlisting}}{\end{lstlisting}}

\newcommand{\shortcode}[1]{\texttt{#1}}

\lstset{style = console}

% Make subsubsection labeled
\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}

\title{ODEs}
\author{Jinyuan Wu}

\begin{document}

\maketitle

\section{First order ODEs}

\subsection{Linear ODEs}\label{sec:1st.linear}

An ODE in the form of 
\begin{equation}
    y'(x) + p(x) y(x) = q(x) 
\end{equation}
is considered \concept{linear}.
All linear ODEs can be solved by the following procedure.
First we have 
\begin{equation}
    (y' + py) \ee^{\int p \dd{x}} 
    = q \ee^{\int p \dd{x}},
\end{equation}
and now the LHS is a derivative:
\begin{equation}
    \dv{x} \left(
        y \ee^{\int p \dd{x}}
    \right) = q \ee^{\int p \dd{x}},
\end{equation}
and now we can integrate over $x$ and get 
\begin{equation}
    y \ee^{\int p \dd{x}} = 
    \int q \ee^{\int p \dd{x}} \dd{x},
\end{equation}
\begin{equation}
    y  = \ee^{- \int p \dd{x}}
    \int q \ee^{\int p \dd{x}} \dd{x} .
\end{equation}

\subsection{``Energy-conservation lines'' and exact equations}

Another way to represent the solution of an ODE is 
the form $\phi(x, y) = \const$.
Note that the RHS contains no variables, and we have 
\begin{equation}
    0 = \dv{\phi}{x} = \pdv{\phi}{x} + \pdv{\phi}{y} \dv{y}{x},
    \label{eq:implicit-derivative}
\end{equation}
and thus if 
\begin{equation}
    y' = f(x, y)
\end{equation}
is algebraically equivalent to \eqref{eq:implicit-derivative},
the equation is already solved:
We should find $M, N$ such that 
\begin{equation}
    y' = - \frac{M}{N}, \quad
    M = \pdv{\phi}{x}, \quad
    N = \pdv{\phi}{y},
\end{equation}
and then $\phi(x, y)$ solves the equation.
In this case we say $y' = - M / N$ is \concept{exact}.

To test for exactness, we only have to test whether
\begin{equation}
    \pdv{M}{y} = \pdv{N}{x},
\end{equation}
and if so, the existence of $\phi$ is guaranteed.
(Since we work on a topological trivial space, 
things like cohomology group will not bother us.)
We can now use ``partial integral'' to find $\phi$.

Example: suppose in a calculation we find  
\begin{equation}
    \pdv{\phi}{x} = 2 y^2 + y \ee^{xy}, \quad 
    \pdv{\phi}{y} = 4xy + x \ee^{xy} + 2y.
\end{equation}
After partial integration, we find 
\begin{equation}
    \phi(x, y) = \underbrace{2 xy^2 + \ee^{xy} + h(y)}_{
        \int \pdv{\phi}{x} \dd{x}
    }
    = \underbrace{2 xy^2 + \ee^{xy} + y^2 + g(x)}_{
        \int \pdv{\phi}{y} \dd{y}
    },
\end{equation}
and we have to choose 
\begin{equation}
    h(y) = y^2, \quad g(x) = \const,
\end{equation}
and the solution is 
\begin{equation}
    \phi(x, y) = 2 xy^2 + \ee^{xy} + y^2 + \const.
\end{equation}

Note that even when the decomposition $f = - M / N$
doesn't give an exact equation for us, 
we can still use the method of exact equations:
we can multiply a factor $\mu$ to both $M$ and $N$,
and try to guess the form of $\mu$ so that 
\begin{equation}
    \pdv{(\mu M)}{y} = 
    \pdv{(\mu N)}{x}.
    \label{eq:generalized-exact-condition}
\end{equation}

An example can be found in solving 
\begin{equation}
    y' = - \frac{1}{3x - \ee^{-2y}}.
\end{equation}
We have 
\[
    \pdv{1}{y} = 0, \quad 
    \pdv{(3x- \ee^{-2y})}{x} = 3,
\]
so the equation is not exact if we choose $M = 1$ and 
$N = 3x - \ee^{-2y}$.
However, \eqref{eq:generalized-exact-condition} 
can be fulfilled now: 
it's now 
\[
    \pdv{\mu}{y} = 3 \mu + \left(
        3x - \ee^{-2y}
    \right) \pdv{\mu}{x},
\]
and the most convenient way to solve it 
(we \emph{don't} need to find all solutions of this equation!)
is to let $\mu$ contain $y$ only,
so the tricky term on the RHS disappears,
and thus we choose $\mu = \ee^{3y}$,
and we get 
\[
    \phi(x, y) = \int \mu M \dd{x} 
    = \int \ee^{3y} \dd{x} = x \ee^{3y} + u(y),
\]
\[
    \phi(x, y) = \int \mu N \dd{y}
    = \int (3x \ee^{3y} - \ee^{y} ) \dd{y} 
    = x \ee^{3y} - \ee^{y} + v(x),
\]
so 
\begin{equation}
    \phi(x, y) = x \ee^{3y} - \ee^{y} + \const.
\end{equation}

\subsection{Bernoulli equation}

Consider the following \concept{Bernoulli equation}  
\begin{equation}
    y' + P(x) y = R(x) y^\alpha.
\end{equation}
When $\alpha = 0, 1$, 
the equation can be solved by the standard methods 
for linear first order ODEs.
When this is not the case, 
we may do the substitution 
\begin{equation}
    v = y^\beta,
\end{equation}
and then the equation becomes 
\[
    \frac{1}{\beta} v^{1/\beta - 1} v' + P(x) v^{1/\beta} 
    = R(x) v^{\alpha / \beta},
\]
\begin{equation}
    v' + P(x) v = R(x) v^{1 + \frac{\alpha - 1}{\beta}}.
\end{equation}
The next step is to choose a good beta so that 
the equation gets simplified.
We may want to make to exponent to be zero, 
and this means we should choose 
\begin{equation}
    \beta = 1 - \alpha,
\end{equation}
and the ODE is now 
\begin{equation}
    v' + Pu = R,
\end{equation}
which can then be solved by the method in  
\prettyref{sec:1st.linear}.

\section{Second order ODEs}

\subsection{Linear 2nd order ODE with initial values}

A linear second order ODE has the following form:
\begin{equation}
    y'' + p(x) y' + q(x) y = f(x).
    \label{eq:linear-2nd-ode-1}
\end{equation}
It usually comes with initial value conditions 
\begin{equation}
    y(x_0) = A, \quad y'(x_0) = B.
\end{equation}
This course is about concrete calculations,
but knowing what we are doing makes sense is important.
Here is an existence and uniqueness theorem:
if $p(x), q(x)$, and $f(x)$ are continuous 
over an interval $I$,
and $x_0 \in I$, 
then a unique solution exists for \eqref{eq:linear-2nd-ode-1}
with the initial conditions given above.

Usually, we start by looking at 
the \concept{homogeneous} second order ODE 
\begin{equation}
    y'' + p(x) y' + q(x) y = 0.
    \label{eq:2nd-order-ode-homogeneous}
\end{equation}
The influence of $f(x)$ can be included as the 
``response'' of the LHS.
The full solution of \eqref{eq:2nd-order-ode-homogeneous}
takes the form 
\begin{equation}
    y = c_1 y_1 + c_2 y_2,
\end{equation}
where $c_1, c_2$ are constants to be decided 
by initial conditions,
and $y_1$ and $y_2$ are linearly independent solutions 
of \eqref{eq:2nd-order-ode-homogeneous}.
The \concept{Wronskian} is defined as 
\begin{equation}
    W(x) = 
    \begin{vmatrix}
        y_1 & y_2 \\ y_1' & y_2'
    \end{vmatrix}.
\end{equation} 
By checking if it is non-zero at most points,
we can find whether $y_1$ and $y_2$ are truly linearly independent to each other.

There is a method to arrive at $y_2$ from $y_1$:
we can always take the ansatz 
\begin{equation}
    y_2 = y_1 u, 
    \label{eq:from-y1-to-y2}
\end{equation}
and therefore we get 
\[
    (u'' y_1 + 2 u' y_1' + u y_1'')
    + p (u' y_1 + u y_1')
    + q u y_1 = 0,
\]
and the condition that $y_1$ is a solution to 
\eqref{eq:2nd-order-ode-homogeneous} means 
\begin{equation}
    u'' + \underbrace{\frac{2y_1' + p y_1}{y_1}}_{g(x)} u' = 0,
\end{equation}
which is essentially a first order ODE, 
because we can replace $u'$ by $v$,
and then we find 
\[
    \ln v = - \int g(x) \dd{x},
\]
and 
\begin{equation}
    u(x) = \int \ee^{- \int g(x) \dd{x}} \dd{x}.
\end{equation}

\subsection{Constant coefficients}

The equation 
\begin{equation}
    y'' + A y + By = 0
\end{equation}
can be solved directly by the following construction:
\begin{equation}
    y = c_1 \ee^{\lambda_1 x} + c_2 \ee^{\lambda_2 x},
\end{equation}
where $\lambda_1, \lambda_2$ are solutions of 
\begin{equation}
    \lambda^2 + A \lambda + B = 0.
    \label{eq:character-eq}
\end{equation}

For example, to solve the equation 
\begin{equation}
    y'' - 2 y' + 10 y = 0,
\end{equation}
we just solve 
\[
    \lambda^2 - 2 \lambda + 10 = 0,
\]
which gives us 
\begin{equation}
    \lambda = 1 \pm 3 \ii,
\end{equation}
and therefore a general solution is 
\begin{equation}
    y = \ee^{x} (c_1 \ee^{3 \ii x} + c_2 \ee^{- 3 \ii x}).
\end{equation}
It should be noted that $c_1, c_2$ can be complex,
even when we restrict $y$ in $\mathbb{R}$:
we can let the imaginary part of $y$ vanish 
as long as we impose some constraints over $c_1, c_2$.
If we are determined to work in the real space, 
two alternative linearly independent solutions can be used:
\begin{equation}
    y_1(x) = \ee^{x} \cos(3x), \quad 
    y_2(x) = \ee^{x} \sin(3x).
\end{equation}
Although we can immediately say 
they are linearly independent,
we can use them as a demonstration of the 
Wronskian method: 
now we have 
\begin{equation}
    W(x) = y_1(x) y_2'(x) - y_2(x) y_1'(x) = 3 \ee^{2x},
\end{equation}
which of course isn't constantly zero.

\eqref{eq:character-eq} is faced with the problem 
of having only one solution 
when $A^2 - 4B = 0$.
In this case we need to go back to the standard procedure 
to get $y_2$ from $y_1$.
An example is 
\begin{equation}
    y'' + 6y + 9 = 0,
\end{equation}
for which \eqref{eq:character-eq} only gives 
\begin{equation}
    y_1 = \ee^{-3x}.
\end{equation}
Suppose $y_2 = u \ee^{-3x}$, we have TODO 

\subsection{Euler equation}

A \concept{Euler equation} has the following form:
\begin{equation}
    x^2 y'' + A x y' + B y = 0,
    \label{eq:euler}
\end{equation}
where $A, B$ are constants.
One solution can be immediate found: it always looks like 
\begin{equation}
    y = x^a.
\end{equation}
We then find 
\begin{equation}
    a(a-1) + A a + B = 0.
\end{equation}
If there are two solutions of the equation, 
\eqref{eq:euler} has already been solved.
If not, we can use the trick \eqref{eq:from-y1-to-y2}.

An example: let's solve 
\begin{equation}
    x^2 y'' + 3x y' + y = 0.
\end{equation}
The equation about $a$ is now 
\[
    a(a-1) + 3a + 1 = 0,
\]
and it only has one solution $a = -1$.
Therefore we have 
\[
    y_1 = \frac{1}{x}.
\]
Suppose 
\[
    y_2 = u y_1,
\]
we get 
\[
    x^2 \left(
        \frac{u''}{x^2} - \frac{2 u'}{x} + \frac{2u}{x^2}
    \right)
    + 3 x \left(
        \frac{u'}{x} - \frac{u}{x^2}
    \right)
    + \frac{u}{x} = 0,
\]
which is equivalent to 
\[
    v' x + v = 0, \quad v = u'
\]
the solution of which is 
\[
    \ln v + \ln x = \const,
\]
and therefore 
\[
    v = \frac{C'}{x}, \quad 
    u = C' \ln x + C,
\]
\[
    y_2 = \frac{1}{x} (C' \ln x + C).
\]
This essentially gives \emph{all} solutions we need: 
for $y_1$, we just have $u = 1$,
which corresponds to $C = 1$.
So now the equation is completely solved.

\subsection{Non-homogeneous cases or how to find the linear response}

Now we discuss how to solve 
\begin{equation}
    y'' + p(x) y' + q(x) y = f(x).
    \label{eq:2nd-ode-nonhomogeneous}
\end{equation}
A general solution is 
\begin{equation}
    y(x) = y_{\text{p}}(x) + y_{\text{h}}(x),
\end{equation}
where the subscript p means a particular solution,
and the subscript h means the general solution of the corresponding homogeneous equation.

We need some common sense to find a particular solution.
To solve 
\begin{equation}
    y'' - y' - 2y = 2 x^2 + 5,
\end{equation}
we don't expect $y$ to be, say, $\cos(2x)$:
instead, it's usually the case that $y$ is a polynomial.
An ansatz is 
\[
    y = A x^2 + Bx + C.
\]
We don't want a $x^3$ term because it doesn't appear on RHS.
The equation then becomes 
\[
    2 A - (2 A x + B x) - 2 (A x^2 + Bx + C) = 2 x^2 + 5, 
\]
\[
    -2 A = 2, \quad - 2A - 2 B = 0, \quad 2 A - B - 2 C = 5,
\]
and therefore $A = -1, B = 1, C = - 4$.
Therefore we get a particular solution: 
\begin{equation}
    y_{\text{p}} = - x^2 + x - 4.
\end{equation}

A particular solution may also be determined as 
a ``linear combination'' of homogeneous solutions,
although now the coefficients have temporal variation.
That's to say, we take the ansatz 
\begin{equation}
    y_{\text{p}}(x) = u(x) y_1(x) + v(x) y_2(x).
    \label{eq:2nd-ode-change-coefficients}
\end{equation}
This is quite similar to the procedure introduced in \prettyref{sec:1st.linear}.
After substituting $y$ with \eqref{eq:2nd-ode-change-coefficients} 
in \eqref{eq:2nd-ode-nonhomogeneous},
we get 
\begin{equation}
    u' y_1' + v' y_2' = f.
\end{equation}
Introducing the constraint 
\begin{equation}
    u' y_1 + v' y_2 = 0,
\end{equation}
we have 
\begin{equation}
    \pmqty{y_1 & y_2 \\ y_1' & y_2'} 
    \pmqty{u' \\ v'} = \pmqty{0 \\ f},
\end{equation}
from which we find $u', v'$ and hence $u, v$.
The Wronskian -- the determinant of the matrix on LHS -- 
is non-zero, so the equation always has a solution.

Example: let's solve 
\begin{equation}
    y'' + y = \tan x. 
\end{equation}
We have 
\[
    y_1 = \cos x, \quad y_2 = \sin x,
\]
and therefore 
\[
    W(x) = y_1 y_2' - y_2 y_1' = 1.
\]
So 
\[
    \begin{aligned}
        u' &= - \int \frac{y_2 f}{W(x)} \dd{x} = - \int \frac{\sin^2 x}{\cos x} \dd{x} 
        = - \int \frac{1 - \cos^2 x}{\cos x} \dd{x} \\
        &= - \frac{1}{2} \ln \abs{
            \frac{1 + \sin x}{1 - \sin x}
        } + \sin x,
    \end{aligned}
\]
and similarly we have 
\[
    v = 
\]

\subsection{Analyticity}

The stimulus can be non-analytic.
$f$ is analytic at $x_0$, if we can expand it into a power series around $x_0$:
\begin{equation}
    f(x) = \sum_{n=0}^\infty a_n (x - x_0)^n 
\end{equation}
in a interval around $x_0$.
The function $f(x) = \ln x$, then, is not analytic at $x = 0$ -- 
but $f(x) = \ln (x + 1)$ is analytic at $x=0$,
though not at $x=-1$.

There is a theorem: if $p$, $q$, $f$ are all analytic at $x_0$,
then \eqref{eq:2nd-ode-nonhomogeneous} together with conditions 
$y(x_0) = A$ and $y'(x_0) = B$ 
has a unique solution that is analytic at $x_0$.

An example: 
\begin{equation}
    x'' + y' - xy = 0, \quad y(0) = -2, y'(0) = 0.
\end{equation}
Suppose 
\[
    y(x) = \sum_{n=0}^\infty a_n x^n, \quad 
    y'(x) = \sum_{n=0}^\infty n a_n x^{n-1}, \quad 
    y''(x) = \sum_{n=0}^\infty n (n+1) a_n x^{n-2},
\]
we have 
\[
    \begin{aligned}
        0 &= \sum_{n=0}^\infty n (n - 1) a_n x^{n-2}
        + \sum_{n=0}^\infty n a_n x^{n-1}
        - \sum_{n=0}^\infty a_n x^{n+1} \\
        &= a_1 + 2 a_2 + 
        \sum_{n=1}^\infty x^n (
            (n + 2) (n + 1) a_{n+2} 
            + (n + 1) a_{n + 1} 
            - a_{n - 1}
        ),
    \end{aligned}
\]
and therefore 
\[
    a_1 + 2 a_2 = 0, \quad 
    a_{n+2} = - \frac{a_{n+1}}{n+2} + \frac{a_{n-1}}{(n+1) (n+2)}.
\]
The conditions $y(0) = -2$ and $y' (0) = 0$ means 
\[
    a_0 = -2, \quad a_1 = 0,
\]
and then we can in principle find all $a_n$'s -- 
although it's often hard to see a pattern and write down a closed-form expression for $a_n$.

Now we consider the equation 
\begin{equation}
    P(x) y'' + Q(x) y' + R(x) y = F(x).
\end{equation}
Of course we can divide the equation with $P(x)$ and go back to \eqref{eq:2nd-ode-nonhomogeneous},
but if $P(x)$ is zero at some points, 
$p, q, f$ in \eqref{eq:2nd-ode-nonhomogeneous} are no longer always analytic.
Thus the solution isn't guaranteed to be analytic everywhere. 
In other words, we no longer have a power series solution -- or do we?
We can still try a generalized power series, like 
\begin{equation}
    y(x) = \sum_{n=0}^\infty c_n (x - x_0)^{n + r},
\end{equation}
where $r$ can be a fraction. 
This is guaranteed with
\begin{equation}
    (x - x_0) y'' + Q(x) y' + R(x) y = F(x).
\end{equation}

To demonstrate this, 
consider 
\begin{equation}
    y'' + \frac{1}{2x} y' - \frac{1}{4x} y = 0.
\end{equation}
We plug 
\[
    y(x) = \sum_{n=0}^\infty c_n x^{n + r}
\]
into 
\[
    4 x y'' + 2 y' - y = 0,
\]
and get 
\[
    \sum_{n=0}^\infty \left(
        4 (n + r) (n + r - 1) c_n x^{n + r - 1}
        + 2 (n + r) c_n x^{n + r - 1} 
        - c_n x^{n + r}
    \right) = 0.
\]
The coefficient of the $x^{r-1}$ term is 
\[
    4 r (r - 1) + 2 r = 0,
\]
from which we find $r = 0, 1/2$.
For the rest of the terms, we have 
\[
    4 (n + r) (n + r - 1) c_n
    + 2 (n + r) c_n
    - c_{n-1} = 0,
\]
\[
    c_n = \frac{c_{n-1}}{2 (n + r) (2 n + 2 r - 1)}.
\]
When $r = 0$, this gives 
\[
    c_n = \frac{c_{n-1}}{2n (2n - 1)} , \quad 
    c_n = \frac{c_0}{(2n)!},
\]
and we get 
\begin{equation}
    y(x) = \sum_{n=0}^\infty \frac{x^n}{(2n)!}.
\end{equation}
When $r = 1/2$, this gives 
\[
    c_n = \frac{c_{n-1}}{(2n + 1) \cdot 2n}, \quad 
    c_n = \frac{c_0}{(2 n + 1)!},
\]
and we get 
\begin{equation}
    y(x) = \sum_{n=0}^\infty \frac{x^{n + 1/2}}{(2 n + 1)!}.
\end{equation}
So we have already obtained two independent solutions. 

\end{document}