\documentclass[hyperref, a4paper]{article}

\usepackage{geometry}
\usepackage{titling}
\usepackage{titlesec}
% No longer needed, since we will use enumitem package
% \usepackage{paralist}
\usepackage{enumitem}
\usepackage{footnote}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathtools}
\usepackage{bbm}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{physics}
\usepackage{tensor}
\usepackage{siunitx}
\usepackage[version=4]{mhchem}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{underscore}
\usepackage{autobreak}
\usepackage[ruled, vlined, linesnumbered]{algorithm2e}
\usepackage{nameref,zref-xr}
\zxrsetup{toltxlabel}
\usepackage[colorlinks,unicode]{hyperref} % , linkcolor=black, anchorcolor=black, citecolor=black, urlcolor=black, filecolor=black
\usepackage[most]{tcolorbox}
\usepackage{prettyref}

% Page style
\geometry{left=3.18cm,right=3.18cm,top=2.54cm,bottom=2.54cm}
\titlespacing{\paragraph}{0pt}{1pt}{10pt}[20pt]
\setlength{\droptitle}{-5em}

% More compact lists 
\setlist[itemize]{
    %itemindent=17pt, 
    %leftmargin=1pt,
    listparindent=\parindent,
    parsep=0pt,
}

\setlist[enumerate]{
    %itemindent=17pt, 
    %leftmargin=1pt,
    listparindent=\parindent,
    parsep=0pt,
}

% Math operators
\DeclareMathOperator{\timeorder}{\mathcal{T}}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\legpoly}{P}
\DeclareMathOperator{\primevalue}{P}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\res}{Res}
\newcommand*{\ii}{\mathrm{i}}
\newcommand*{\ee}{\mathrm{e}}
\newcommand*{\const}{\mathrm{const}}
\newcommand*{\suchthat}{\quad \text{s.t.} \quad}
\newcommand*{\argmin}{\arg\min}
\newcommand*{\argmax}{\arg\max}
\newcommand*{\normalorder}[1]{: #1 :}
\newcommand*{\pair}[1]{\langle #1 \rangle}
\newcommand*{\fd}[1]{\mathcal{D} #1}
\DeclareMathOperator{\bigO}{\mathcal{O}}

% TikZ setting
\usetikzlibrary{arrows,shapes,positioning}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{calc}
\tikzstyle arrowstyle=[scale=1]
\tikzstyle directed=[postaction={decorate,decoration={markings,
    mark=at position .5 with {\arrow[arrowstyle]{stealth}}}}]
\tikzstyle ray=[directed, thick]
\tikzstyle dot=[anchor=base,fill,circle,inner sep=1pt]

% Algorithm setting
% Julia-style code
\SetKwIF{If}{ElseIf}{Else}{if}{}{elseif}{else}{end}
\SetKwFor{For}{for}{}{end}
\SetKwFor{While}{while}{}{end}
\SetKwProg{Function}{function}{}{end}
\SetArgSty{textnormal}

\newcommand*{\concept}[1]{{\textbf{#1}}}

% Embedded codes
\lstset{basicstyle=\ttfamily,
  showstringspaces=false,
  commentstyle=\color{gray},
  keywordstyle=\color{blue}
}

\lstdefinestyle{console}{
    basicstyle=\footnotesize\ttfamily,
    breaklines=true,
    postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space}
}

% Reference formatting
\newrefformat{fig}{Figure~\ref{#1}}

% Color boxes
\tcbuselibrary{skins, breakable, theorems}
\newtcbtheorem[number within=section]{warning}{Warning}%
  {colback=orange!5,colframe=orange!65,fonttitle=\bfseries, breakable}{warn}
\newtcbtheorem[number within=section]{note}{Note}%
  {colback=green!5,colframe=green!65,fonttitle=\bfseries, breakable}{note}
\newtcbtheorem[number within=section]{info}{Info}%
  {colback=blue!5,colframe=blue!65,fonttitle=\bfseries, breakable}{info}

% Displaying texts in bookmarkers

\pdfstringdefDisableCommands{%
  \def\\{}%
  \def\ce#1{<#1>}%
}

\pdfstringdefDisableCommands{%
  \def\texttt#1{<#1>}%
  \def\mathbb#1{#1}%
}
\pdfstringdefDisableCommands{\def\eqref#1{(\ref{#1})}}

\makeatletter
\pdfstringdefDisableCommands{\let\HyPsd@CatcodeWarning\@gobble}
\makeatother

\newenvironment{shelldisplay}{\begin{lstlisting}}{\end{lstlisting}}

\newcommand{\shortcode}[1]{\texttt{#1}}

\newcommand*{\mat}[1]{\vb{#1}}

\lstset{style = console}

% Make subsubsection labeled
\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}

\newcommand*{\laplace}{\mathcal{L}}
\newcommand*{\fourier}{\mathcal{F}}
\newcommand*{\zerotoinf}{\int_{0}^{\infty}}
\newcommand*{\inftoinf}{\int_{-\infty}^{\infty}}

\title{Matrices}
\author{Jinyuan Wu}

\begin{document}

\maketitle

\section{Homogeneous linear equation systems}\label{sec:linear-eq}

Consider the set of linear equations 
\begin{equation}
    \begin{aligned}
        &x_1 + x_3 + x_4 = 0, \\
        &x_3 + 2 x_4 = 0, \\
        &x_1 + 2 x_3 + 3 x_4 = 0.
    \end{aligned}
\end{equation}
The equivalent matrix form is 
\begin{equation}
    \underbrace{\pmqty{
        1 & 0 & 1 & 1 \\
        0 & 0 & 1 & 2 \\
        1 & 0 & 2 & 3
    }}_{\mat{A}} \underbrace{\pmqty{x_1 \\ x_2 \\ x_3 \\ x_4}}_{\vb{x}} = \pmqty{0 \\ 0 \\ 0}.
\end{equation}

We use the row reduction method to solve the equations.
First we keep the first row unchanged,
but subtract the first row from the third row, and we get 
\[
    \pmqty{
        1 & 0 & 1 & 1 \\
        0 & 0 & 1 & 2 \\
        0 & 0 & 1 & 2
    }.
\]
Then we can just set the third row to zeros 
because it duplicates with the second line.
Then we can subtract the second line from the first line, and get 
\begin{equation}
    \pmqty{
        1 & 0 & 0 & -1 \\
        0 & 0 & 1 & 2 \\
        0 & 0 & 0 & 0
    }.
    \label{eq:reduced-form-1}
\end{equation}
This is in row echelon form.
Now we get the reduced form of $\mat{A}$,
and from this, we find 
\begin{equation}
    x_4 = x_1, \quad x_3 = - 2 x_4,
\end{equation}
and $x_2$ can be anything.
The general solution is therefore found to be 
\begin{equation}
    \vb{x} = \pmqty{
        x_4 \\ x_2 \\ - x_4 \\ x_4
    } = x_2 \pmqty{0 \\ 1 \\ 0 \\ 0}
    + x_4 \pmqty{1 \\ 0 \\ -2 \\ 1}.
    \label{eq:general-solution-1}
\end{equation}
So we found there are two independent solutions, 
which is expected because \eqref{eq:reduced-form-1} has only two non-zero rows,
so it's rank is 2, 
and the number of independent variables is the number of columns minus the rank,
so we should have $4 - 2 = 2$ independent variables, 
i.e. 2 independent solutions 
(one independent variable controls the weight of one independent solution).
We define the \concept{row space} of a matrix 
as the space of spanned by the non-zero row vectors 
of its reduced matrix.
The dimension of the row space is the rank of the matrix.

We can also get \eqref{eq:general-solution-1} by looking at \eqref{eq:reduced-form-1}.
First we need to switch $x_2$ and $x_3$ so that the non-zero lines of \eqref{eq:reduced-form-1}
have the form of $\pmqty{\mat{I} & \mat{B}}$,
and the non-zero columns of $\mat{B}$ are 
$\pmqty{0 & 0}^\top$ and $\pmqty{-1 & 2}^\top$.
Now we concatenate the opposites of the two columns 
with $\pmqty{1 & 0}^\top$ and $\pmqty{0 & 1}^\top$, respectively,
and we get 
\[
    \pmqty{0 \\ 0 \\ 1 \\ 0}, \quad \pmqty{1 \\ -2 \\ 0 \\ 1},
\]
which are solutions for $(x_1, x_3, x_2, x_4)$,
and then we switch $x_2$ and $x_3$ again and get 
\[
    \pmqty{0 \\ 1 \\ 0 \\ 0}, \quad \pmqty{1 \\ 0 \\ -2 \\ 1}.
\]

\section{Determinant}

We have \concept{Cramer's rule}: 
the solution of 
\begin{equation}
    \mat{A} \vb{x} = \mat{b}
\end{equation}
can be found by 
\begin{equation}
    x_k = \frac{\det \mat{A}_{k}}{\det \mat{A}},
\end{equation}
where 
\begin{equation}
    \mat{A}_k = \pmqty{\vb{A}_1 & \vb{A}_2 & \cdots & \vb{A}_{k-1} & \vb{b} & \vb{A}_{k+1} & \cdots & \vb{A}_n},
\end{equation}
and $\vb{A}_i$ is the $i$th column of $\mat{A}$.
The proof of this rule is simple: 
we have
\[
    \vb{b} = x_1 \vb{A}_1 + x_2 \vb{A}_2 + \cdots + x_n \vb{A}_n,
\]
and therefore 
\[
    \det \mat{A}_i = \sum_{i} x_i \det \pmqty{\vb{A}_1 & \vb{A}_2 & \cdots & \vb{A}_{k-1} & \vb{A}_i & \vb{A}_{k+1} & \cdots & \vb{A}_n},
\]
and when $i \neq k$, 
$\vb{A}_i$ must be the same as one of the other columns of the matrix in the RHS 
and the determinant vanishes, 
and therefore 
\[
    \det \mat{A}_i = x_i \det \mat{A},
\]
and thus we get the Cramer's rule.

\section{Invertible matrix}

Suppose $\mat{A}$ is an $n \times n$ matrix.
The following statements are equivalent:
\begin{itemize}
    \item Columns of $\mat{A}$ are linearly independent;
    \item $\rank \mat{A} = n$;
    \item The reduced form of $\mat{A}$ (\prettyref{sec:linear-eq}) 
        is the identity matrix;
    \item $\det \mat{A} \neq 0$;
    \item $\mat{A} \vb{x} = 0$ only has vanishing solution;
    \item $\mat{A} \vb{x} = \vb{b}$ has unique solution;
    \item $\mat{A}$ has an inverse.
\end{itemize}

\section{Eigenvalue decomposition}

If $n \times n$ matrix $\mat{A}$ has $n$ linearly independent eigenvectors, 
we have 
\begin{equation}
    \mat{A} = \mat{P} \mat{D} \mat{P}^{-1},
\end{equation}
where $\mat{D}$ is the diagonal matrix 
containing all the eigenvalues, 
while $\mat{P}$ is the matrix containing linearly independent eigenvectors 
as the columns. 

If $\mat{A}$ has $n$ distinct eigenvalues, 
then the eigenvectors are linearly independent.

\section{Adjoin structures}

We have 
\begin{equation}
    (\mat{A} \mat{B})^* = \mat{A}^* \mat{B}^*.
\end{equation}
We can also define the \concept{transpose} of a matrix: 
\begin{equation}
    (\mat{A}^\top)_{ij} = \mat{A}_{ji}.
\end{equation}
For matrix transpose, we have 
\begin{equation}
    (\mat{A} \mat{B})^\top = \mat{B}^\top \mat{A}^\top.
\end{equation}
We can also combine the two operations;
indeed we have 
\begin{equation}
    (\mat{A}^*)^\top = (\mat{A}^\top)^*.
\end{equation}
So we can define 
\begin{equation}
    \mat{A}^\dagger = (\mat{A}^*)^\top.
\end{equation}

An astonishing property of the transpose is 
\begin{equation}
    (\mat{A}^{-1})^\top = (\mat{A}^\top)^{-1}.
    \label{eq:transpose-inverse}
\end{equation}
This can be easily proved by noticing 
\begin{equation}
    \mat{A} \mat{A}^{-1} = \mat{I} = \mat{I}^\top = (\mat{A}^{-1})^\top \mat{A}^\top,
\end{equation}
and the final equation proves \eqref{eq:transpose-inverse}.

If $\vb{e}$ is an eigenvector of a unitary matrix $\mat{U}$, 
which means 
\begin{equation}
    \mat{U} \mat{U}^\dagger = \mat{I},
\end{equation}
we have
\begin{equation}
    \vb{e}^\dagger \mat{U}^\dagger = \lambda^* \vb{e}^\dagger.
\end{equation}
So we find 
\begin{equation}
    \abs*{\lambda}^2 \vb{e}^\dagger \vb{e} 
    = \lambda^* \vb{e}^\dagger \cdot \lambda \vb{e}
    = \vb{e}^\dagger \mat{U}^\dagger \mat{U} \vb{e}
    = \vb{e}^\dagger \vb{e} 
    \Rightarrow \abs*{\lambda}^2 = 1.
\end{equation}

\section{System of linear first order differential equations}

We can write a system of linear first order differential equations 
into the following concise form:
\begin{equation}
    \vb{x}' = \mat{A} \vb{x}.
\end{equation}
Inspired by the scalar case 
\begin{equation}
    x(t) = x(0) \ee^{a t},
\end{equation}
and considering the eigenvalue decomposition of $\mat{A}$, 
we have 
\begin{equation}
    \vb{x} (t) = \ee^{\mat{A} t} \vb{x}(0).
\end{equation}
We can put an eigenvector as the initial value, 
and this means 
\begin{equation}
    \mat{x}(t) = \ee^{\mat{A} t} \vb{e} = \ee^{\lambda t} \vb{e}.
\end{equation}
So now we can write the real initial condition as a linear combination 
of eigenvectors of $\mat{A}$
and have 
\begin{equation}
    \vb{x}(0) = \sum_i c_i \vb{e}_i,
\end{equation}
and the solution is 
\begin{equation}
    \vb{x}(t) = \sum_i c_i \vb{e}_i \ee^{\lambda_i t}.
\end{equation}
We can also use a matrix form to represent this: 
\begin{equation}
    \begin{aligned}
        \vb{x}(t) &= \pmqty{
            \ee^{\lambda_1 t} \vb{e}_1 & 
            \ee^{\lambda_2 t} \vb{e}_2 &
            \cdots &
            \ee^{\lambda_n t} \vb{e}_n
        } \pmqty{
            c_1 \\ \vdots \\ c_n
        } \\
        &= \mat{P} \mat{D}(t) \mat{C}, 
    \end{aligned}
\end{equation}
where 
\begin{equation}
    \mat{P} = \pmqty{
        \vb{e}_1 & \cdots & \vb{e}_n
    }, \quad 
    \mat{D} = \pmqty{
        \dmat{
            \ee^{\lambda_1 t}, \cdots, \ee^{\lambda_n t}
        }
    }, \quad 
    \mat{C} = \pmqty{
        c_1 \\ \vdots \\ c_n
    },
\end{equation}
and eventually, $\mat{C}$ can be found by 
\begin{equation}
    \mat{P} \mat{C} = \vb{x}(0),
\end{equation}
so we have 
\begin{equation}
    \vb{x}(t) = \mat{P} \mat{D}(t) \mat{P}^{-1} \vb{x}(0).
\end{equation}
This is just the eigenvalue decomposition of $\ee^{\lambda \mat{A}}$.

It's possible that we want to work in $\mathbb{R}$ 
but $\mat{A}$ doesn't have $n$ independent real eigenvectors.
Suppose there are $m$ independent eigenvectors.
Then we need to find $n - m$ other vectors so that
\begin{equation}
    \vb{x}(0) = \sum_{i=1}^{m} c_i \vb{e}_i + \sum_{i=m+1}^{n} c_i \vb{k}_i.
\end{equation}
Usually we want to find $\vb{k}$'s such that $\ee^{\mat{A} t} \vb{k}$ truncates.
That's to say, we want to find $\lambda$ and $n$ such that 
\begin{equation}
    \begin{aligned}
        \ee^{\mat{A} t} \vb{k} &= 
        \ee^{\lambda t \mat{I}} \ee^{\mat{A} t - \lambda t \mat{I}} \vb{k} \\
        &= \ee^{\lambda t \mat{I}} \left(
            \mat{I} + (\mat{A} - \lambda \mat{I}) t 
            + \frac{1}{2!} (\mat{A} - \lambda \mat{I})^2 t^2 + \cdots 
        \right) \vb{k} \\
        &= \ee^{\lambda t \mat{I}} \left(
            \mat{I} + (\mat{A} - \lambda \mat{I}) t 
            + \frac{1}{2!} (\mat{A} - \lambda \mat{I})^2 t^2 + \cdots + 
            \frac{1}{(n-1)!} (\mat{A} - \lambda \mat{I})^{n-1} t^{n-1}
        \right) \vb{k}.
    \end{aligned}
\end{equation}
Note that since $\mat{I}$ commutes with every matrix, 
the first equation is always true;
the second line is also always true;
the third line comes from the definition of a truncated series;
so what we want is summarized as 
\begin{equation}
    (\mat{A} - \lambda \mat{I})^n = 0.
\end{equation}

\end{document}
