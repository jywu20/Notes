{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Tensorflow operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.data as data\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Date preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_slice(dataset, n):\n",
    "    i = 0\n",
    "    items = []\n",
    "    for item in dataset:\n",
    "        i += 1\n",
    "        if i > n:\n",
    "            break\n",
    "        items.append(item)\n",
    "    return items\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "`from_tensor_slices`(*array*) regards the first axis of *array* as the batch indexing dimension.\n",
    "For example, a (4, 2)-shaped np-array corresponds with a 4-element dataset, in which each element is of (2,)-shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = data.Dataset.from_tensor_slices(np.array([\n",
    "    [1, 1],\n",
    "    [1, 0],\n",
    "    [0, 1],\n",
    "    [0, 0]\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[<tf.Tensor 'IteratorGetNext_46:0' shape=(2,) dtype=int32>,\n <tf.Tensor 'IteratorGetNext_47:0' shape=(2,) dtype=int32>,\n <tf.Tensor 'IteratorGetNext_48:0' shape=(2,) dtype=int32>]"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_slice(dataset1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "`from_tensors`(*array*) creates a dataset with only one element, and the element is just *array*.\n",
    "In this way, a (4, 2)-shaped input results in a dataset with only one (4, 2)-shaped element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[<tf.Tensor 'IteratorGetNext_50:0' shape=(4, 2) dtype=int32>,\n <tf.Tensor 'IteratorGetNext_51:0' shape=(4, 2) dtype=int32>,\n <tf.Tensor 'IteratorGetNext_52:0' shape=(4, 2) dtype=int32>]"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2 = data.Dataset.from_tensors(np.array([\n",
    "    [1, 1],\n",
    "    [1, 0],\n",
    "    [0, 1],\n",
    "    [0, 0]\n",
    "]))\n",
    "dataset_slice(dataset2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The input need not be np-arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[<tf.Tensor 'IteratorGetNext_66:0' shape=(2,) dtype=int32>,\n <tf.Tensor 'IteratorGetNext_67:0' shape=(2,) dtype=int32>,\n <tf.Tensor 'IteratorGetNext_68:0' shape=(2,) dtype=int32>,\n <tf.Tensor 'IteratorGetNext_69:0' shape=(2,) dtype=int32>,\n <tf.Tensor 'IteratorGetNext_70:0' shape=(2,) dtype=int32>]"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_slice(\n",
    "    data.Dataset.from_tensor_slices([\n",
    "        [1, 1],\n",
    "        [1, 0],\n",
    "        [0, 1],\n",
    "        [0, 0]\n",
    "    ]), 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[<tf.Tensor 'IteratorGetNext_72:0' shape=(2,) dtype=float32>,\n <tf.Tensor 'IteratorGetNext_73:0' shape=(2,) dtype=float32>,\n <tf.Tensor 'IteratorGetNext_74:0' shape=(2,) dtype=float32>,\n <tf.Tensor 'IteratorGetNext_75:0' shape=(2,) dtype=float32>,\n <tf.Tensor 'IteratorGetNext_76:0' shape=(2,) dtype=float32>]"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_slice(\n",
    "    data.Dataset.from_tensor_slices([\n",
    "        [1., 1],\n",
    "        [1, 0],\n",
    "        [0, 1],\n",
    "        [0, 0]\n",
    "    ]), 5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "When the input is a tuple of arrays, "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A tuple of lists causes strange behaviors... Why?? TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset3 = data.Dataset.from_tensor_slices((\n",
    "    [1, 1],\n",
    "    [1, 0],\n",
    "    [0, 1],\n",
    "    [0, 0]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[(<tf.Tensor 'IteratorGetNext_54:0' shape=() dtype=int32>,\n  <tf.Tensor 'IteratorGetNext_54:1' shape=() dtype=int32>,\n  <tf.Tensor 'IteratorGetNext_54:2' shape=() dtype=int32>,\n  <tf.Tensor 'IteratorGetNext_54:3' shape=() dtype=int32>),\n (<tf.Tensor 'IteratorGetNext_55:0' shape=() dtype=int32>,\n  <tf.Tensor 'IteratorGetNext_55:1' shape=() dtype=int32>,\n  <tf.Tensor 'IteratorGetNext_55:2' shape=() dtype=int32>,\n  <tf.Tensor 'IteratorGetNext_55:3' shape=() dtype=int32>)]"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_slice(dataset3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[(<tf.Tensor 'IteratorGetNext_97:0' shape=(2,) dtype=int32>,\n  <tf.Tensor 'IteratorGetNext_97:1' shape=() dtype=int32>),\n (<tf.Tensor 'IteratorGetNext_98:0' shape=(2,) dtype=int32>,\n  <tf.Tensor 'IteratorGetNext_98:1' shape=() dtype=int32>),\n (<tf.Tensor 'IteratorGetNext_99:0' shape=(2,) dtype=int32>,\n  <tf.Tensor 'IteratorGetNext_99:1' shape=() dtype=int32>),\n (<tf.Tensor 'IteratorGetNext_100:0' shape=(2,) dtype=int32>,\n  <tf.Tensor 'IteratorGetNext_100:1' shape=() dtype=int32>),\n (<tf.Tensor 'IteratorGetNext_101:0' shape=(2,) dtype=int32>,\n  <tf.Tensor 'IteratorGetNext_101:1' shape=() dtype=int32>)]"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset4 = data.Dataset.from_tensor_slices((\n",
    "    np.array([\n",
    "        [1, 0],\n",
    "        [0, 1],\n",
    "        [1, 1],\n",
    "        [0, 0]\n",
    "    ]),\n",
    "    np.array([\n",
    "        1, 1, 0, 0\n",
    "    ])\n",
    "))\n",
    "\n",
    "dataset_slice(dataset4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[(<tf.Tensor 'IteratorGetNext_84:0' shape=(1, 2) dtype=int32>,\n  <tf.Tensor 'IteratorGetNext_84:1' shape=() dtype=int32>),\n (<tf.Tensor 'IteratorGetNext_85:0' shape=(1, 2) dtype=int32>,\n  <tf.Tensor 'IteratorGetNext_85:1' shape=() dtype=int32>),\n (<tf.Tensor 'IteratorGetNext_86:0' shape=(1, 2) dtype=int32>,\n  <tf.Tensor 'IteratorGetNext_86:1' shape=() dtype=int32>),\n (<tf.Tensor 'IteratorGetNext_87:0' shape=(1, 2) dtype=int32>,\n  <tf.Tensor 'IteratorGetNext_87:1' shape=() dtype=int32>),\n (<tf.Tensor 'IteratorGetNext_88:0' shape=(1, 2) dtype=int32>,\n  <tf.Tensor 'IteratorGetNext_88:1' shape=() dtype=int32>)]"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset5 = data.Dataset.from_tensor_slices((\n",
    "    np.array([\n",
    "        [[1, 0]],\n",
    "        [[0, 1]],\n",
    "        [[1, 1]],\n",
    "        [[0, 0]]\n",
    "    ]),\n",
    "    np.array([\n",
    "        1, 1, 0, 0\n",
    "    ])\n",
    "))\n",
    "\n",
    "dataset_slice(dataset5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Operations on datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = dataset1.shuffle(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating models"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A model is a collection of connected neurons, with their input and output nodes specified.\n",
    "In the code below we create an input node (actually what we created is *a bundle of* input nodes, encapsuled into a single vector in the computational graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "WARNING: Logging before flag parsing goes to stderr.\nW0102 19:31:10.708741  4892 deprecation.py:506] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\n"
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(10,), name='digits')\n",
    "x = layers.Dense(32, activation='relu', name='dense_1')(inputs)\n",
    "x = layers.Dense(32, activation='relu', name='dense_2')(x)\n",
    "outputs = layers.Dense(10, activation='softmax', name='predictions')(x)\n",
    "\n",
    "model1 = keras.Model(inputs=inputs, outputs=outputs, name='3_layer_mlp')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pay special attention to the `layers.Dense(64, activation='relu', name='dense_1')(inputs)` statement.\n",
    "Each `Layer`, `Input` or any other keras objects corresponds to some nodes in the computational graph, \n",
    "and by invoking `x = layers.Dense(64, activation='relu', name='dense_1')(inputs)`, we link `x` to `input`, \n",
    "creating a path in the computational graph.\n",
    "When creating a layer, we specify its size, but do not give any information about the size of its antecedent.\n",
    "This raises a question - since the layer's weights depends on its own size and the size of its antecedent, \n",
    "how can the layer be initialized with only its own size given?\n",
    "The simple answer is: it is not always initialized.\n",
    "If the size of the antecedent (in this case, it is `input`) is already known, \n",
    "then the weights matrix of `x` is also determined, and thus `x` is *built*.\n",
    "On the other side, `x` is not initialized until the first bunch of data arrives, \n",
    "and `x` takes the size of the data as the size of its antecedent.\n",
    "Of course, if the second bunch of data takes a different size, \n",
    "or the first bunch of data takes a shape that is not compatible with `x`, an error occurs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "It is OK to reuse some existing nodes to create another model. In this case, the two models will have shared weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = layers.Dense(2, activation='softmax')(x)\n",
    "model2 = keras.Model(inputs=inputs, outputs=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Now we illustrate the structure of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"3_layer_mlp\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndigits (InputLayer)          [(None, 784)]             0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 64)                50240     \n_________________________________________________________________\ndense_2 (Dense)              (None, 64)                4160      \n_________________________________________________________________\npredictions (Dense)          (None, 10)                650       \n=================================================================\nTotal params: 55,050\nTrainable params: 55,050\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndigits (InputLayer)          [(None, 10)]              0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 32)                352       \n_________________________________________________________________\ndense_2 (Dense)              (None, 32)                1056      \n_________________________________________________________________\ndense (Dense)                (None, 2)                 66        \n=================================================================\nTotal params: 1,474\nTrainable params: 1,474\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We can `summary` these two models because the size of the input node is specified, \n",
    "so the size of everything else is known, and therefore the two models are already `built` when summarized.\n",
    "\n",
    "By default, every `Input` node must have a given size, \n",
    "so models created with `keras.Model` are always built before running.\n",
    "That is not the case for sequential models. We will soon say it.\n",
    "\n",
    "Another notice is the output shape in the summary. \n",
    "They are all in the form of ` (None, xxx) `.\n",
    "All layers we defined have one-dimension inputs, so what is `None` about?\n",
    "Actually it means batch size.\n",
    "When defining the inputs and outputs of a certain node, the batch size dimension is excluded."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Sequential models"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "To create a sequential model(i.e. a model in which neurons are organized as a sequence of layers, and neurons in one layer have no or few connection), use the following method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = keras.Sequential()\n",
    "model3.add(layers.Dense(8, activation='relu'))\n",
    "model3.add(layers.Dense(8, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "You can eliminate `model.add` by "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = keras.Sequential([\n",
    "    layers.Dense(2, activation='relu'),\n",
    "    layers.Dense(2, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Trying to `summarize` the model results as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-615aa5064e88>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36msummary\u001b[1;34m(self, line_length, positions, print_fn)\u001b[0m\n\u001b[0;32m   1502\u001b[0m     \"\"\"\n\u001b[0;32m   1503\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1504\u001b[1;33m       raise ValueError('This model has not yet been built. '\n\u001b[0m\u001b[0;32m   1505\u001b[0m                        \u001b[1;34m'Build the model first by calling `build()` or calling '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1506\u001b[0m                        \u001b[1;34m'`fit()` with some data, or specify '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build."
     ]
    }
   ],
   "source": [
    "model3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Just as we expected, the input shape of `model3` is not specified so it is not built,\n",
    "so it is impossible to summarize the model.\n",
    "Actually `Sequential` uses an input node slightly different from `keras.Input`.\n",
    "To ensure the model gets built from beginning, we mannually set the input shape of the first layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "WARNING: Logging before flag parsing goes to stderr.\nW0102 23:48:19.868268 16272 deprecation.py:506] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 2)                 6         \n_________________________________________________________________\ndense_1 (Dense)              (None, 3)                 9         \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 4         \n=================================================================\nTotal params: 19\nTrainable params: 19\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model5 = keras.Sequential([\n",
    "    layers.Dense(2, activation='relu', input_shape=(2,)),\n",
    "    layers.Dense(3, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model5.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Another way to build a sequential is by invoking `build` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_3 (Dense)              multiple                  6         \n_________________________________________________________________\ndense_4 (Dense)              multiple                  6         \n=================================================================\nTotal params: 12\nTrainable params: 12\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model6 = keras.Sequential([\n",
    "    layers.Dense(2, activation='relu'),\n",
    "    layers.Dense(2, activation='relu')\n",
    "])\n",
    "model6.build((None,2))\n",
    "model6.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TODO: What does multiple mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Details of a layer include\n",
    "- activation function\n",
    "- initializer\n",
    "- regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = layers.Dense(\n",
    "    16, \n",
    "    activation=tf.keras.activations.sigmoid,\n",
    "    kernel_regularizer=tf.keras.regularizers.l1(0.01),\n",
    "    bias_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "    kernel_initializer='orthogonal',\n",
    "    bias_initializer=tf.keras.initializers.Constant(2.0),\n",
    "    input_shape=(2,),\n",
    "    batch_size=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Note: if a string *s* is passed as the regularizer or initializer, \n",
    "`tf.keras.regularizers.__attr__`(*s*) and `tf.keras.initializers.__attr__`(*s*) will be designated as the regularizer or the initializer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1.build((None, 16))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compiling"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Configuration of the model's optimizer, loss function and metrics is called compiling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.binary_crossentropy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.BinaryCrossentropy()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Note that camel-named items are classes, while snake-named items are instances of corresponding classes.\n",
    "That is why we write `BinaryCrossentropy()` but `binary_crossentropy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5.compile(\n",
    "    optimizer='sgd',\n",
    "    loss='mse'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving and loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "W0102 20:03:24.132149  4892 deprecation.py:506] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\nW0102 20:03:24.135138  4892 deprecation.py:506] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\nW0102 20:03:24.509174  4892 hdf5_format.py:221] No training configuration found in save file: the model was *not* compiled. Compile it manually.\nModel: \"3_layer_mlp\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndigits (InputLayer)          [(None, 10)]              0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 32)                352       \n_________________________________________________________________\ndense_2 (Dense)              (None, 32)                1056      \n_________________________________________________________________\npredictions (Dense)          (None, 10)                330       \n=================================================================\nTotal params: 1,738\nTrainable params: 1,738\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model1.save('./my_model.h5')\n",
    "model1_prime = keras.models.load_model('./my_model.h5')\n",
    "model1_prime.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"3_layer_mlp\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndigits (InputLayer)          [(None, 10)]              0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 32)                352       \n_________________________________________________________________\ndense_2 (Dense)              (None, 32)                1056      \n_________________________________________________________________\npredictions (Dense)          (None, 10)                330       \n=================================================================\nTotal params: 1,738\nTrainable params: 1,738\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Illustrating models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"3_layer_mlp\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndigits (InputLayer)          [(None, 10)]              0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 32)                352       \n_________________________________________________________________\ndense_2 (Dense)              (None, 32)                1056      \n_________________________________________________________________\npredictions (Dense)          (None, 10)                330       \n=================================================================\nTotal params: 1,738\nTrainable params: 1,738\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
    }
   ],
   "source": [
    "keras.utils.plot_model(model1)\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/50\n4/4 [==============================] - 0s 47ms/sample - loss: 0.3525\nEpoch 2/50\n4/4 [==============================] - 0s 240us/sample - loss: 0.3510\nEpoch 3/50\n4/4 [==============================] - 0s 748us/sample - loss: 0.3494\nEpoch 4/50\n4/4 [==============================] - 0s 5ms/sample - loss: 0.3478\nEpoch 5/50\n4/4 [==============================] - 0s 738us/sample - loss: 0.3462\nEpoch 6/50\n4/4 [==============================] - 0s 498us/sample - loss: 0.3447\nEpoch 7/50\n4/4 [==============================] - 0s 748us/sample - loss: 0.3431\nEpoch 8/50\n4/4 [==============================] - 0s 499us/sample - loss: 0.3415\nEpoch 9/50\n4/4 [==============================] - 0s 1ms/sample - loss: 0.3399\nEpoch 10/50\n4/4 [==============================] - 0s 498us/sample - loss: 0.3384\nEpoch 11/50\n4/4 [==============================] - 0s 1ms/sample - loss: 0.3368\nEpoch 12/50\n4/4 [==============================] - 0s 249us/sample - loss: 0.3352\nEpoch 13/50\n4/4 [==============================] - 0s 499us/sample - loss: 0.3337\nEpoch 14/50\n4/4 [==============================] - 0s 498us/sample - loss: 0.3321\nEpoch 15/50\n4/4 [==============================] - 0s 499us/sample - loss: 0.3306\nEpoch 16/50\n4/4 [==============================] - 0s 998us/sample - loss: 0.3290\nEpoch 17/50\n4/4 [==============================] - 0s 498us/sample - loss: 0.3275\nEpoch 18/50\n4/4 [==============================] - 0s 498us/sample - loss: 0.3259\nEpoch 19/50\n4/4 [==============================] - 0s 997us/sample - loss: 0.3244\nEpoch 20/50\n4/4 [==============================] - 0s 998us/sample - loss: 0.3228\nEpoch 21/50\n4/4 [==============================] - 0s 498us/sample - loss: 0.3213\nEpoch 22/50\n4/4 [==============================] - 0s 498us/sample - loss: 0.3198\nEpoch 23/50\n4/4 [==============================] - 0s 748us/sample - loss: 0.3183\nEpoch 24/50\n4/4 [==============================] - 0s 2ms/sample - loss: 0.3168\nEpoch 25/50\n4/4 [==============================] - 0s 499us/sample - loss: 0.3153\nEpoch 26/50\n4/4 [==============================] - 0s 499us/sample - loss: 0.3138\nEpoch 27/50\n4/4 [==============================] - 0s 499us/sample - loss: 0.3123\nEpoch 28/50\n4/4 [==============================] - 0s 1ms/sample - loss: 0.3108\nEpoch 29/50\n4/4 [==============================] - 0s 499us/sample - loss: 0.3093\nEpoch 30/50\n4/4 [==============================] - 0s 499us/sample - loss: 0.3079\nEpoch 31/50\n4/4 [==============================] - 0s 748us/sample - loss: 0.3064\nEpoch 32/50\n4/4 [==============================] - 0s 746us/sample - loss: 0.3050\nEpoch 33/50\n4/4 [==============================] - 0s 748us/sample - loss: 0.3035\nEpoch 34/50\n4/4 [==============================] - 0s 250us/sample - loss: 0.3021\nEpoch 35/50\n4/4 [==============================] - 0s 496us/sample - loss: 0.3007\nEpoch 36/50\n4/4 [==============================] - 0s 1ms/sample - loss: 0.2993\nEpoch 37/50\n4/4 [==============================] - 0s 1ms/sample - loss: 0.2979\nEpoch 38/50\n4/4 [==============================] - 0s 499us/sample - loss: 0.2965\nEpoch 39/50\n4/4 [==============================] - 0s 1ms/sample - loss: 0.2951\nEpoch 40/50\n4/4 [==============================] - 0s 249us/sample - loss: 0.2937\nEpoch 41/50\n4/4 [==============================] - 0s 499us/sample - loss: 0.2924\nEpoch 42/50\n4/4 [==============================] - 0s 499us/sample - loss: 0.2910\nEpoch 43/50\n4/4 [==============================] - 0s 2ms/sample - loss: 0.2897\nEpoch 44/50\n4/4 [==============================] - 0s 748us/sample - loss: 0.2884\nEpoch 45/50\n4/4 [==============================] - 0s 748us/sample - loss: 0.2870\nEpoch 46/50\n4/4 [==============================] - 0s 1ms/sample - loss: 0.2857\nEpoch 47/50\n4/4 [==============================] - 0s 496us/sample - loss: 0.2845\nEpoch 48/50\n4/4 [==============================] - 0s 748us/sample - loss: 0.2832\nEpoch 49/50\n4/4 [==============================] - 0s 997us/sample - loss: 0.2819\nEpoch 50/50\n4/4 [==============================] - 0s 498us/sample - loss: 0.2807\n"
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x25643984630>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.array([\n",
    "    [1, 1],\n",
    "    [1, 0],\n",
    "    [0, 1],\n",
    "    [0, 0]\n",
    "])\n",
    "y_train = np.array([\n",
    "    [0],\n",
    "    [1],\n",
    "    [1],\n",
    "    [0]\n",
    "])\n",
    "\n",
    "model5.fit(x_train, y_train, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "When the output is a one-element tensor, it is OK to pass a scalar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/50\n4/4 [==============================] - 0s 2ms/sample - loss: 0.2794\nEpoch 2/50\n4/4 [==============================] - 0s 499us/sample - loss: 0.2782\nEpoch 3/50\n4/4 [==============================] - 0s 748us/sample - loss: 0.2770\nEpoch 4/50\n4/4 [==============================] - 0s 1ms/sample - loss: 0.2757\nEpoch 5/50\n4/4 [==============================] - 0s 997us/sample - loss: 0.2746\nEpoch 6/50\n4/4 [==============================] - 0s 500us/sample - loss: 0.2734\nEpoch 7/50\n4/4 [==============================] - 0s 499us/sample - loss: 0.2724\nEpoch 8/50\n4/4 [==============================] - 0s 748us/sample - loss: 0.2716\nEpoch 9/50\n4/4 [==============================] - 0s 1ms/sample - loss: 0.2707\nEpoch 10/50\n4/4 [==============================] - 0s 748us/sample - loss: 0.2699\nEpoch 11/50\n4/4 [==============================] - 0s 1ms/sample - loss: 0.2691\nEpoch 12/50\n4/4 [==============================] - 0s 998us/sample - loss: 0.2682\nEpoch 13/50\n4/4 [==============================] - 0s 746us/sample - loss: 0.2674\nEpoch 14/50\n4/4 [==============================] - 0s 499us/sample - loss: 0.2666\nEpoch 15/50\n4/4 [==============================] - 0s 748us/sample - loss: 0.2659\nEpoch 16/50\n4/4 [==============================] - 0s 2ms/sample - loss: 0.2651\nEpoch 17/50\n4/4 [==============================] - 0s 748us/sample - loss: 0.2643\nEpoch 18/50\n4/4 [==============================] - 0s 997us/sample - loss: 0.2636\nEpoch 19/50\n4/4 [==============================] - 0s 498us/sample - loss: 0.2628\nEpoch 20/50\n4/4 [==============================] - 0s 4ms/sample - loss: 0.2621\nEpoch 21/50\n4/4 [==============================] - 0s 499us/sample - loss: 0.2614\nEpoch 22/50\n4/4 [==============================] - 0s 249us/sample - loss: 0.2606\nEpoch 23/50\n4/4 [==============================] - 0s 998us/sample - loss: 0.2599\nEpoch 24/50\n4/4 [==============================] - 0s 499us/sample - loss: 0.2592\nEpoch 25/50\n4/4 [==============================] - 0s 749us/sample - loss: 0.2586\nEpoch 26/50\n4/4 [==============================] - 0s 748us/sample - loss: 0.2579\nEpoch 27/50\n4/4 [==============================] - 0s 997us/sample - loss: 0.2572\nEpoch 28/50\n4/4 [==============================] - 0s 499us/sample - loss: 0.2566\nEpoch 29/50\n4/4 [==============================] - 0s 748us/sample - loss: 0.2559\nEpoch 30/50\n4/4 [==============================] - 0s 498us/sample - loss: 0.2553\nEpoch 31/50\n4/4 [==============================] - 0s 748us/sample - loss: 0.2546\nEpoch 32/50\n4/4 [==============================] - 0s 1ms/sample - loss: 0.2540\nEpoch 33/50\n4/4 [==============================] - 0s 499us/sample - loss: 0.2534\nEpoch 34/50\n4/4 [==============================] - 0s 2ms/sample - loss: 0.2528\nEpoch 35/50\n4/4 [==============================] - 0s 499us/sample - loss: 0.2522\nEpoch 36/50\n4/4 [==============================] - 0s 748us/sample - loss: 0.2516\nEpoch 37/50\n4/4 [==============================] - 0s 748us/sample - loss: 0.2511\nEpoch 38/50\n4/4 [==============================] - 0s 748us/sample - loss: 0.2505\nEpoch 39/50\n4/4 [==============================] - 0s 748us/sample - loss: 0.2499\nEpoch 40/50\n4/4 [==============================] - 0s 746us/sample - loss: 0.2494\nEpoch 41/50\n4/4 [==============================] - 0s 997us/sample - loss: 0.2488\nEpoch 42/50\n4/4 [==============================] - 0s 499us/sample - loss: 0.2483\nEpoch 43/50\n4/4 [==============================] - 0s 748us/sample - loss: 0.2478\nEpoch 44/50\n4/4 [==============================] - 0s 748us/sample - loss: 0.2472\nEpoch 45/50\n4/4 [==============================] - 0s 499us/sample - loss: 0.2467\nEpoch 46/50\n4/4 [==============================] - 0s 1ms/sample - loss: 0.2462\nEpoch 47/50\n4/4 [==============================] - 0s 499us/sample - loss: 0.2457\nEpoch 48/50\n4/4 [==============================] - 0s 748us/sample - loss: 0.2452\nEpoch 49/50\n4/4 [==============================] - 0s 498us/sample - loss: 0.2447\nEpoch 50/50\n4/4 [==============================] - 0s 1ms/sample - loss: 0.2443\n"
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x25647d9fcc0>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.array([\n",
    "    [1, 1],\n",
    "    [1, 0],\n",
    "    [0, 1],\n",
    "    [0, 0]\n",
    "])\n",
    "y_train = np.array([\n",
    "    0,\n",
    "    1,\n",
    "    1,\n",
    "    0\n",
    "])\n",
    "\n",
    "model5.fit(x_train, y_train, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "It is also possible to integrate x_train and y_train into one dataset. \n",
    "Note that in this case the batch size should be designated explicitly,\n",
    "since when there is only one dataset, `model.fit` assumes that the dataset will take care of batching,\n",
    "so it takes whatever the dataset gives as the fitting data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "W0103 00:36:06.083314 16272 training_utils.py:1300] Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\nEpoch 1/3\n2/2 [==============================] - 0s 61ms/step - loss: 0.2410\nEpoch 2/3\n2/2 [==============================] - 0s 2ms/step - loss: 0.2407\nEpoch 3/3\n2/2 [==============================] - 0s 4ms/step - loss: 0.2404\n"
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x25649e77da0>"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5.fit(tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(2), epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "W0103 00:38:23.392063 16272 training_utils.py:1300] Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\n"
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_input to have shape (2,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-841975be5db9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m     \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_prepare_feed_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m     \u001b[1;31m# `ins` is a function when a distribute strategy is used in Eager mode.  In\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[1;31m# that case `is_dataset` is True.  The code branches that have requirements\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36m_prepare_feed_values\u001b[1;34m(model, inputs, targets, sample_weights, mode)\u001b[0m\n\u001b[0;32m    499\u001b[0m     inputs, targets, sample_weights = model._standardize_user_data(\n\u001b[0;32m    500\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 501\u001b[1;33m         extract_tensors_from_dataset=True)\n\u001b[0m\u001b[0;32m    502\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m   \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModelInputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2649\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2650\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2651\u001b[1;33m           exception_prefix='input')\n\u001b[0m\u001b[0;32m   2652\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2653\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    383\u001b[0m                              \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m                              str(data_shape))\n\u001b[0m\u001b[0;32m    386\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected dense_input to have shape (2,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "model5.fit(tf.data.Dataset.from_tensor_slices((x_train, y_train)), epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting and evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.635807]], dtype=float32)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5.predict(np.array([[1, 0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.6358069 ],\n       [0.6995458 ],\n       [0.7994131 ],\n       [0.49304864]], dtype=float32)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5.predict(np.array([\n",
    "    [1, 0], \n",
    "    [0, 1],\n",
    "    [1, 1],\n",
    "    [0, 0]\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Some notes on `predict`: \n",
    "- Always pass a dataset or NumPy array to it, not a built-in list or tuple.\n",
    "- Though the input shape is `(2,)`, `predict` actually takes a `(None, 2)`-shaped input.\n",
    "That is because the first dimension is always the batch index.\n",
    "Also, though the output shape is `(1,)`, we get a `(None, 1)`-shaped output. \n",
    "- The shape of the output is `(1,)`, which means each item in the output is a 1-dim *vector*, not a scalar.\n",
    "This should not be surprisal given that inputs and ouputs of a layer is always organized as a vector, \n",
    "even if there is only one neuron in the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5.evaluate(x_test,  y_test, verbose=2)"
   ]
  }
 ]
}