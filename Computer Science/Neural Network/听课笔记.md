# 听俞熹老师讲机器学习的笔记1

许多时候授课者和听课者都以为自己明白了，但是上来一讲，就可以发现有些人实际上并不明白。

## 近些年的主要突破

- 算法的进步：深度学习
- 硬件的进步：GPU接口平台
- 数据量的增加以及虚拟化数字化浪潮
- 机器学习引擎的开源化：TensorFlow，PyTorch

为什么以前不流行深度学习？

这个东西最早被提出来的时候被当成傻子，因为计算量太大，被认为完全不适合模拟，而且也没有专门的硬件支持。

但是，神经网络被证明适合运行在GPU上面。NVIDIA为了支持使用GPU做大规模的向量化计算，专门开发了CUDA框架。

我们可以使用cps，也就是每秒计算次数来衡量一个系统的计算能力。人脑的cps估计为10^16，而天河二号也就相当于六个人脑。

最节约的（一个机箱几万），在一个机箱内部放4块Titan显卡，然后使用多个机箱做一个4U集群；

业界常常将机器学习硬件称为炼丹炉。成本高、温度高、可能出来好东西、产生的东西取决于水平、看运气。

## 什么是人工智能？
简单地说，一个人工智能算法是一个能够通过经验自动改进的算法。因此这东西没有任何神秘之处，就是一个拟合算法。

而所谓机器学习算法就是这样一个算法。通常它要最大化/最小化一个目标函数，并且能够通过大数据分组检验防止过拟合。大数据、算法、拟合，这就是机器学习的核心。写模型、调参数、拟合，成功了，完事。这就是所谓的机器学习工程。当然，这也是一个非常物理的东西。

样本是最重要的。不能预期机器能够给出从来没有出现在训练数据集中的结构。

## 机器学习的基本方法

- 监督学习：每个样本都是特征和标注的对
- 无监督学习：样本只有特征而没有标注，需要自己提取出信息。聚类是一个例子。

## What about the future?
Past industrial revolutions provide extension for muscles. Analogy of muscles in the current AI revolution is cognition, and therefore, automation of control. A direct consequence of this improvement is structural or technological job loss. Solutions include retraining and compensation.

Google has a secret department for preparation for future technologies. One member claimed that automatic driving of Tesla is actually the worst among the industry. He raised the longest distance without human intervention as an estimation of the performance of automatic driving. This being true, Google achieves the highest standard.

# 10.9
机器学习：处理数据的工具。

幂律分布和$x^2$分布都是Gamma分布的特例，所谓Gamma分布指的是相邻两个时间之间的时间间隔的分布情况或者相隔$k$个事件的时间间隔的分布情况。应用例子：μ子的衰变时间预测。

单个事件永远要遇上特例，因此需要做概率推理。这实际上就是开赌场的讨论。

# 10.16
正态分布有什么用呢？

设想有一个基金声称：他们在3年中让一单位的本金变成了3单位。怎么判断他们实际的业绩水平？

我们拿到这个基金每天的数据，做一个分布图；然后再使用上证指数，做一个分布图。然后检查：基金的分布是不是相比上证指数有优势？

做这种统计的时候要尽可能将时间拉得足够长，并且要考虑指数长期的涨和跌。如果有长期的跌，保持空仓就可以跑赢指数；如果有长期的涨，那么正好相反。

---

所谓的机器学习实际上只是人工智能中一个比较小的进路，而其中又有大量不同的分支。


