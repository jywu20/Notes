# 循环神经网络
我们知道处理序列是需要记忆的（比如说人不可能完全不使用上下文无关的算法去听一句话），所以需要使用一些记忆机制，或者说往模型当中引入时间相关的机制。循环神经网络就是一个例子。

## 架构

### 朴素RNN
先前我们定义过前馈神经网络：（我们省略了总输入的计算，而只是展示每一层神经元的输出）
$$
\left\{
    \begin{aligned}
        \bold{a}^{(l)} &= f_l(\bold{W}^{(l)} \bold{a}^{(l-1)} + \bold{b}^{(l)}), \\
        \bold{a}^{(0)} &= \bold{x}, \quad \hat{\bold{y}} = \bold{a}^{(L)}.
\end{aligned}
\right.
$$

当然这个网络是没有记忆功能的，但是在处理序列或者可以转化成序列的数据（例如，一个句法树就可能被表示成一串Merge指令）的时候我们要求神经网络能够接收一系列的输入，并且有记忆，能够根据**先前的**输入给出输出。

满足这个条件的一种常用神经网络架构就是循环神经网络。其定义为：

$$
\left\{
    \begin{aligned}
        \bold{h}^{(l)}_t &= f_l(\bold{U}^{(l)} \bold{h}^{(l)}_{t-1} + \bold{W}^{(l)} \bold{h}^{(l-1)}_t + \bold{b}^{(l)}), \\
        \bold{h}^{(0)}_t &= \bold{x}_t, \quad \hat{\bold{y}}_t = \bold{h}^{(L)}_t.
\end{aligned}
\right.
$$

我们把上一个时刻，每一层神经元的状态保存下来，然后到这一个时刻，将之前同一层神经元的状态加权在输入——这就是循环神经网络一词的由来：某一层神经元某一时刻的历史状态被导出，然后在下一时刻又被输入回这一层神经元，构成一个数据流的循环。经过多层神经元的处理，网络在每一个时刻输出一个向量作为预测结果。这个预测结果通常需要被输入到某个分类器当中，以给出离散的结果。

数学上能够证明，在激活函数满足特定条件的情况下，循环神经网络能够模拟一切图灵机。

事实上，我们可以只使用两层神经元，而且第二层神经元仅仅用于汇总第一层的输出——也就是说，我们只使用一层隐藏层。此时对应的方程是
$$
\left\{
    \begin{aligned}
        \bold{h}_t &= f(\bold{U}\bold{h}_{t-1} + \bold{W} \bold{x}_{t} + \bold{b}), \\
        \hat{\bold{y}}_t &= \bold{V} \bold{h}_t.
    \end{aligned}
\right.
$$

当然，如果我们将“汇总隐藏层输出”视作分类器应该干的事，那么上式还可以进一步简化为
$$
\bold{h}_t = f(\bold{U}\bold{h}_{t-1} + \bold{W} \bold{x}_{t} + \bold{b}).
$$

如同FNN的情况一样，即使这样一个单个隐藏层的RNN也能模拟一切图灵机。为了区分，我们将多层RNN称为层叠RNN，即SRNN。

### 单层单向LSTM
方程为（我们不再引入矩阵$\bold{V}$来汇总输出；分类器什么的直接接在$\bold{h}_t$上）
$$
\left\{
    \begin{aligned}
        \tilde{\bold{c}}_t &= \tanh (\bold{W}_c \bold{x}_t + \bold{U}_c \bold{h}_{t-1} + \bold{b}_c), \\
        \bold{i}_t &= \sigma(\bold{W}_i \bold{x}_t + \bold{U}_i \bold{h}_{t-1} + \bold{b}_i), \\
        \bold{f}_t &= \sigma(\bold{W}_f \bold{x}_t + \bold{U}_f \bold{h}_{t-1} + \bold{b}_f), \\
        \bold{o}_t &= \sigma(\bold{W}_o \bold{x}_t + \bold{U}_o \bold{h}_{t-1} + \bold{b}_o), \\
        \bold{c}_t &= \bold{f}_t \odot \bold{c}_{t-1} + \bold{i}_t \odot \tilde{\bold{c}}_t, \\
        \bold{h}_t &= \bold{o}_t \odot \tanh(\bold{c}_t).
    \end{aligned}
\right.
$$

简单RNN的问题在于没法有效地跟踪远距离的依赖，也就是说记忆不够长——$\bold{h}_t$在每个时刻都会被重写，只能够保存**短期记忆**。为此有必要创建一个更加稳定的记忆存储区域，在LSTM当中这就是$\bold{c}_t$。这就是LSTM名称的来源：长的短时记忆。

使用这一大堆变量是为了创建一个**门机制**。有三个门：
- 遗忘门$\bold{f}_t$，在全0的情况下让上一时刻的记忆被彻底遗忘，在全1的情况下会让上一时刻的记忆完全被保存下来；
- 输入门$\bold{i}_t$，在全0的情况下$\tilde{\bold{c}}_t$完全不会影响$\bold{c}_t$，这代表神经网络不接受新数据输入，在全1的情况下让新数据的输入最大化（$\tilde{\bold{c}}_t$是一个没有上界的变量，它是输入数据的汇总）；
- 输出门$\bold{o}_t$，控制有多少“长的短时记忆”能够被输出给外界。

### 单层单向、门耦合LSTM
还可以定义一个“耦合了遗忘门和输入门”的LSTM，也就是强行要求遗忘门和输入门互补，且涉及到的权值和偏置完全一致，此时方程为
$$
\left\{
    \begin{aligned}
        \tilde{\bold{c}}_t &= \tanh (\bold{W}_c \bold{x}_t + \bold{U}_c \bold{h}_{t-1} + \bold{b}_c), \\
        \bold{i}_t &= \sigma(\bold{W}_i \bold{x}_t + \bold{U}_i \bold{h}_{t-1} + \bold{b}_i), \\
        \bold{o}_t &= \sigma(\bold{W}_o \bold{x}_t + \bold{U}_o \bold{h}_{t-1} + \bold{b}_o), \\
        \bold{c}_t &= (\bold{1} - \bold{i}_t) \odot \bold{c}_{t-1} + \bold{i}_t \odot \tilde{\bold{c}}_t, \\
        \bold{h}_t &= \bold{o}_t \odot \tanh(\bold{c}_t).
    \end{aligned}
\right.
$$

有没有可能使用多层LSTM？

## 使用场景

### 序列分类（序列到类型）
输入一个序列，输出一个类别。这种情况下我们要在RNN的输出端接一个分类器：
$$
\hat{y} = g(\hat{\bold{y}}_T)
$$
这里$T$是序列的总长度。整个序列都被输入之后才能分类。当然这个分类器也可以是一个神经网络。

也有一种做法是将一整个$\hat{\bold{y}}_T$做一个平均，然后再分类，即
$$
\hat{y} = g(\frac{1}{T} \sum_{t=1}^T \hat{\bold{y}}_t).
$$

### 序列标注（同步的序列到序列）

输入一个序列，输出一个长度相同的序列。也就是说，我们根据这一时刻的输入和之前的输入，为这一时刻的输入打上一个标签。通常我们也需要一个分类器：
$$
\hat{y}_t = g(\hat{\bold{y}}_t)
$$

### 编码-解码（异步的序列到序列）

## 训练