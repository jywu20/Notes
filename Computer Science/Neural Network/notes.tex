\documentclass[hyperref, a4paper, 12pt]{report}

\usepackage{geometry}
\usepackage{titling}
\usepackage{titlesec}
% No longer needed, since we will use enumitem package
% \usepackage{paralist}
\usepackage{enumitem}
\usepackage{footnote}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathtools}
\usepackage{bbm}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{simpler-wick}
\usepackage{physics}
\usepackage{tensor}
\usepackage{siunitx}
\usepackage[normalem]{ulem}
\usepackage[version=4]{mhchem}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{underscore}
\usepackage{autobreak}
\usepackage[ruled, vlined, linesnumbered]{algorithm2e}
\usepackage{nameref,zref-xr}
\zxrsetup{toltxlabel}
\usepackage[colorlinks,unicode, bookmarksnumbered]{hyperref} % , linkcolor=black, anchorcolor=black, citecolor=black, urlcolor=black, filecolor=black
\usepackage[most]{tcolorbox}
\usepackage[backend=bibtex,doi=false,isbn=false,url=false]{biblatex}
\addbibresource{probability.bib}
\usepackage{prettyref}

% Page style
\geometry{left=3.18cm,right=3.18cm,top=2.54cm,bottom=2.54cm}
\titlespacing{\paragraph}{0pt}{1pt}{10pt}[20pt]
\setlength{\droptitle}{-5em}

% More compact lists 
\setlist[itemize]{
    %itemindent=17pt, 
    %leftmargin=1pt,
    listparindent=\parindent,
    parsep=0pt,
}

\setlist[enumerate]{
    %itemindent=17pt, 
    %leftmargin=1pt,
    listparindent=\parindent,
    parsep=0pt,
}

% Math operators
\DeclareMathOperator{\timeorder}{\mathcal{T}}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\legpoly}{P}
\DeclareMathOperator{\primevalue}{P}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\res}{Res}
\DeclareMathOperator{\softmax}{softmax}
\newcommand*{\ii}{\mathrm{i}}
\newcommand*{\ee}{\mathrm{e}}
\newcommand*{\const}{\mathrm{const}}
\newcommand*{\suchthat}{\quad \text{s.t.} \quad}
\newcommand*{\argmin}{\arg\min}
\newcommand*{\argmax}{\arg\max}
\newcommand*{\normalorder}[1]{: #1 :}
\newcommand*{\pair}[1]{\langle #1 \rangle}
\newcommand*{\fd}[1]{\mathcal{D} #1}
\DeclareMathOperator{\bigO}{\mathcal{O}}

% TikZ setting
\usetikzlibrary{arrows,shapes,positioning}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{calc}
\tikzstyle arrowstyle=[scale=1]
\tikzstyle directed=[postaction={decorate,decoration={markings,
    mark=at position .5 with {\arrow[arrowstyle]{stealth}}}}]
\tikzstyle ray=[directed, thick]
\tikzstyle dot=[anchor=base,fill,circle,inner sep=1pt]

% Algorithm setting
% Julia-style code
\SetKwIF{If}{ElseIf}{Else}{if}{}{elseif}{else}{end}
\SetKwFor{For}{for}{}{end}
\SetKwFor{While}{while}{}{end}
\SetKwProg{Function}{function}{}{end}
\SetArgSty{textnormal}

\newcommand*{\concept}[1]{{\textbf{#1}}}
\newcommand*{\term}[1]{\textit{#1}}

% Embedded codes
\lstset{basicstyle=\ttfamily,
  showstringspaces=false,
  commentstyle=\color{gray},
  keywordstyle=\color{blue}
}

\lstdefinestyle{console}{
    basicstyle=\footnotesize\ttfamily,
    breaklines=true,
    postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space}
}
\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}
\lstdefinestyle{python}{
        language=Python,
        backgroundcolor=\color{white},
        basicstyle=\footnotesize\ttfamily,
        keywordstyle=\color{blue},
        commentstyle=\color{mygreen},
        stringstyle=\color{mymauve},
        numberstyle=\tiny\color{mygray},
        numbers=left,
        breaklines=true,
        showstringspaces=false
}

% Reference formatting
\newrefformat{fig}{Figure~\ref{#1}}

% Color boxes
\tcbuselibrary{skins, breakable, theorems}

\newtcbtheorem[number within=chapter]{infobox}{Box}{
    enhanced,
    boxrule=0pt,
    %colback=blue!5,
    %colframe=blue!5,
    colback=white,
    colframe=white,
    coltitle=blue!60,
    borderline west={4pt}{0pt}{blue!65},
    sharp corners,
    fonttitle=\bfseries, 
    breakable,
    before upper={\parindent15pt\noindent}}{box}
\definecolor{my-orange}{HTML}{F58123}
\newtcbtheorem[number within=chapter, use counter from=infobox]{theorybox}{Box}{
    enhanced,
    boxrule=0pt,
    %colback=orange!5, 
    %colframe=orange!5, 
    colback=white,
    colframe=white,
    coltitle=my-orange!65,
    borderline west={4pt}{0pt}{my-orange!65},
    sharp corners,
    fonttitle=\bfseries, 
    breakable,
    before upper={\parindent15pt\noindent}}{box}
\newtcbtheorem[number within=chapter, use counter from=infobox]{todobox}{Box}{
    enhanced,
    boxrule=0pt,
    colback=red!5,
    colframe=red!5,
    coltitle=red!50,
    borderline west={4pt}{0pt}{red!65},
    sharp corners,
    fonttitle=\bfseries, 
    breakable,
    before upper={\parindent15pt\noindent}}{box}
\newtcbtheorem[number within=chapter, use counter from=infobox]{perspectivebox}{Box}{
    enhanced,
    boxrule=0pt,
    %colback=red!5,
    %colframe=red!5,
    colback=white,
    colframe=white,
    coltitle=red!50,
    borderline west={4pt}{0pt}{red!65},
    sharp corners,
    fonttitle=\bfseries, 
    breakable,
    before upper={\parindent15pt\noindent}}{box}


% Displaying texts in bookmarkers

\pdfstringdefDisableCommands{%
  \def\\{}%
  \def\ce#1{<#1>}%
}

\pdfstringdefDisableCommands{%
  \def\texttt#1{<#1>}%
  \def\mathbb#1{#1}%
}
\pdfstringdefDisableCommands{\def\eqref#1{(\ref{#1})}}

\makeatletter
\pdfstringdefDisableCommands{\let\HyPsd@CatcodeWarning\@gobble}
\makeatother

\newenvironment{shelldisplay}{\begin{lstlisting}}{\end{lstlisting}}

\newcommand*{\citesec}[1]{\S~{#1}}
\newcommand*{\citechap}[1]{Ch.~{#1}}
\newcommand*{\citefig}[1]{Fig.~{#1}}
\newcommand*{\citetable}[1]{Table~{#1}}
\newcommand*{\citepage}[1]{p.~{#1}}
\newcommand*{\citepages}[1]{pp.~{#1}}
\newcommand*{\citefootnote}[1]{fn.~{#1}}
\newcommand*{\citechapsec}[2]{\citechap{#1}.\citesec{#2}}
\newcommand{\literature}[1]{\textit{#1}}

\newrefformat{sec}{\citesec{\ref{#1}}}
\newrefformat{fig}{\citefig{\ref{#1}}}
\newrefformat{tbl}{\citetable{\ref{#1}}}
\newrefformat{chap}{\citechap{\ref{#1}}}
\newrefformat{fn}{\citefootnote{\ref{#1}}}
\newrefformat{box}{Box~\ref{#1}}
\newrefformat{ex}{\ref{#1}}

\newcommand{\shortcode}[1]{\texttt{#1}}

\lstset{style = python}

% Make subsubsection labeled
\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}

\newcommand*{\abinitio}{\textit{ab initio}}
\newcommand*{\kB}{k_{\text{B}}}
\newcommand*{\epsr}{\epsilon_{\text{r}}}

\title{Modern neural networks}
\author{Jinyuan Wu}

\begin{document}

\maketitle

\chapter{Framework}

\section{Modern neural networks used in deep learning, compared with other algorithms}
\label{sec:modern-network}

Historically, incredibly diverse architectures have been proposed within the family of neural network algorithms.
What proliferates in modern machine learning belongs to a small subset of these algorithms.
What set them apart from ``traditional'' neural network algorithms
(i.e. algorithms before 2010s) include several aspects.

\subsection{How they make predications}

All modern neural networks are feedforward. That's to say -- 
\begin{itemize}
    \item The output of a neuron is a real number%
    \footnote{
        In actual implementations, often a reduced precision float number.
    }, not a time sequence or a spark or something else.
    There are no loops in the structure of the neural network:
    prediction therefore is \concept{forward propagation},
    because data propagates from the input neurons to the output neurons.
    
    \item What happens in a neuron is as simple as possible:
    usually the output is calculated by 
    passing the inputs weighted by weight factors plus a bias
    to the activation function.
    The weights and the biases can be adjusted;
    the activation function contains no learnable parameters.

    \item Since a neuron is very simple, a network contains \emph{layers} of neuron:
    a feedforward network consisting of several layers -- a \concept{multi-layer percepton (MLP)} can therefore be represented as 
    \begin{equation}
        \mathrm{MLP}(\vb{x}) = f_N(\vb{W}_N \cdot (\cdots (\vb{W}_2 \cdot f_1(\vb{W}_1 \cdot \vb{x} + \vb{b}_1) + \vb{b}_2) \cdots) + \vb{b}_N),
        \label{eq:mlp}
    \end{equation}
    where at layer $i$, $\vb{W}_i$ is a matrix containing weights of all neurons at that layer, $\vb{b}_i$ contains the biases of neurons at that layer, and $f_i (\cdot)$ is the activation function that acts element-wise on a vector.
    In actual calculations, the concept of \emph{neuron} does \emph{not} explicitly appear:
    we work solely on vectors and matrices.

    \item Several simple non-linear operations, like multiplication,
    are often used in modern neural networks.
    By the universal approximation theorem, though, they can be reasonably well approximated by a ``pristine'' feedforward neural network within a given range of inputs.
    For instance $xy = ((x-y)^2 - x^2 - y^2) / -2$,
    and $z^2$ can be approximated by e.g. $\mathrm{ReLU}(2(z-1))$ for $0 \leq z \leq 2$.
    Note that the range of $z$ is a trivial instance of an important condition of the \concept{Universal Approxmation Theorem}, which states that neural networks approximate continuous functions on a \emph{compact set}.
    This is also a thing to keep in mind in model designing.
    
    \item Modern neural networks have mechanisms to accept inputs with variable sizes,
    by a sliding window (as in CNNs), by letting the network to have a latent state,
    which is a part of both the output and the input, or other methods.
    Basically this means we are allowed to use bounded \shortcode{for} loops,
    although in practice this is usually done by \emph{tensor} operations (see \prettyref{sec:tensor}).
    The existence of these mechanisms essentially enables us to dynamically increase or decrease the number of neurons to match the input size:
    this further render the neuron-based representation of modern neural networks somehow unnatural.
\end{itemize}

\subsection{Universal approximation}

A modern neural network can approximate any function that is ``well-behaved''.
This conclusion -- actually a family of conclusions of this type --
is known as the \concept{Universal Approxmation Theorem(s)}.
It should be noted that universal approximation is not directly related to Turing completeness.
A theoretical neural network, which works on real numbers,
is \emph{beyond} the capacity of all Turing machines,
because almost all real numbers are not computable.
On the other hand, existing studies on universal approximation generally focus on
neural networks with a fixed number of input variables,
which goes against the usual notion of Turing completeness,
which in theory allows an infinite input.

Therefore, the function classes determined by neural networks and by Turing machines
are both elegant, natural definitions of what computation means,
but they are not the same, and nor is there a subset relation between the two.

In reality, the memory of a computer is never infinite,
and neural networks always have some ``noises'' (either from float number errors or, in certain alternative computing platforms, from thermal noises).
The latter makes neural networks strictly within the class of Turing computable functions,
while the former is less substantial in the sense that
if we can increase the memory allocated to a program in an unbounded manner 
without drastically change the programs 
(for instance, we can write C programs without utilizing the length of the pointer type),
then the concept of Turing completeness is still meaningful.
In this practical sense, universal approximation is a concept that is strictly weaker than Turing completeness.

Whether certain neural network architectures, accompanied by the aforementioned sliding window/latent state/whatever mechanism, can achieve Turing completeness,
is an interesting and not completely explored question.
For instance we may want to know the grammatical complexity class a Transformer is able to recognize and, if it clearly does not cover the attested grammatical complexity of natural languages, then why it is so successfully.

We finally note that universal approximation is about \emph{existence}.
How to train a model is another problem (also see \prettyref{sec:theoretical-aspects}). Below we outline the current training paradigm.

\subsection{Training}

Modern neural networks are all trained by \concept{backpropagation},
which basically is a fancy way to say applying the chain rule of derivatives to the loss function,
which requires going from the output to the inputs (i.e. propagating backwards) in the dependence relations of variables (which is often implemented as computational graphs; see below).
Therefore they are also known as \concept{BP neural networks}.
This means we choose a \concept{loss function} (which can be derived by statistical estimations,
like the maximum a posteriori estimation; see \prettyref{sec:framework.statistics}),
and use it to measure the difference between the training dataset and the prediction.
Then we take the \emph{gradient} of the loss function with respect to parameters in the model,
and update the parameters according to the gradient.
This is known as backpropagation because when taking the gradient of the loss function,
we need to take the gradient of e.g. \eqref{eq:mlp},
and this means we first take the gradient of the last layer,
then the second last layer, etc.,
and the propagation of the derivative operation is backwards.

That modern neural network models are trained by backpropagation matters sometimes when designing models.
One thing we need to avoid is gradient exploding or vanishing.
To see if a model has a gradient exploding or vanishing problem,
we calculate 
\begin{equation}
    \pdv{L}{w} = \pdv{L}{C_1} \pdv{C_1}{C_2} \cdots \pdv{C_n}{w} + \cdots,
\end{equation}
and check if any factor $\pdv{C_i}{C_j}$ is unbounded or has a possibility to fall to zero.

Modern neural networks are trained in \concept{batches},
which enables parallelism and avoids overfitting,
because if samples within a batch are randomly chosen,
then all kinds of noises are seen by the model.

\subsection{Deep learning}

Modern neural networks are usually \emph{deep}:
they are much larger than traditional models and usually contain multiple layers,
and are data thirsty.
Modern neural networks therefore are almost identical to what we call \concept{deep learning}.

It should be noted that in theory, ``deep learning'' by definition means a machine learning paradigm that is large, complex, and scales up well.
It is in theory possible to have non-neural network deep learning methods.
An instance is deep forest, or in other words, a decision tree ensemble;
or consider a large, structured probabilistic graphic model.
Currently, these approaches are less popular than the aforementioned deep neural networks
-- but maybe this will change one day.
In this note, the term \emph{modern neural network} and \emph{deep learning model} will be used interchangeably.

\section{``Tensors'' and PyTorch}\label{sec:tensor}

\subsection{Motivation: ``variables'' in algebra and calculus}
In doing algebra and calculus, it is handy to not explicitly specify the input and output of a function, but do the follows:
we define some \emph{independent variables} like $x, y, z$,
and then write $w = f(x, y, z)$, and say that ``$w$ is a function of $x, y, z$''.
We expect $w$ to \emph{remember} that it is defined in terms of $x, y, z$:
this means we can then talk about the gradient of $z$ with respect to $x$ or $y$,
or talk about the value of $w$ when $x, y, z$ are assigned certain values,
and then evaluate the gradient of $w$ ``at the point $(x, y, z)$''.

For someone who does more formalized math,
what is done here is rather weird, because variables, in a logical sense,
are either bound by quantifiers ($\forall, \exists$),
or are place holders when doing $\forall$- or $\exists$-introduction and elimination:
in this way if $x, y, z$ are declared to be, say, real numbers,
when we write $z = f(x, y, z)$,
$z$ is also just a real number:
we're not supposed to take its derivative or do other things like that!
In a formalized context, we should only talk about the gradient of $f$ with respect to its first or second or third variable,
or the value of $f$ at a certain point:
$x, y ,z$ are merely placeholders used in expressions like $f(x, y, z) = \dots$.

One way to make sense of the concept of variables in informal math is 
to collect the definition of these independent variables at one place,
and say that we have a big ``configuration space'',
on which $x, y$ and $z$ are mappings from points in that space to a number.
This is how we define \concept{random variables}:
in formalized math, a random variable essentially is a function that maps an ``event'' (i.e. an element of a sample space) to a subset of a measurable space (specifying the internal makeup of the event),
which is then measured by a metric (i.e. how probability is define).
This is what is actually going on when we talk about the probability $p(X=x)$ that a random variable $X$ takes the value $x$.
Still this formalized definition doesn't stop us from imagining that a random variable is just a ``variable'' -- just something out there -- that changes itself in a crazy way.

The term \term{variable} may refer to what is to be solved in an equation.
This is similar to the definition of variable in formal logic,
with one important difference:
in everyday math we often treat an equation as an \emph{object}.
Thus an equation can't be a sentence in logic,
because no one defines predicates of sentences.
Instead we can imagine that we formalize the semantics of Mathematica in first-order logic,
and a \emph{variable} in an equation, together with the equation in question,
are objects in that formalized Mathematica,
i.e. an embedded language defined in first order logic
just to make discussion everyday math easier,
without the necessity of being allowed to only talk about zero points of functions.

And finally, a \term{variable} is a concept in procedural programming.
This meaning of variable is completely different from all the other meanings of variable above:
when we write \shortcode{w = f(x, y, z)} in procedural programming,
changing \shortcode{x} doesn't alternate \shortcode{w}.

It is clear that variables in formal logic are a subset of variables in equations in everyday algebra.
On the other hand, with operation overloading,
we can make \shortcode{w = f(x, y, z)} to mean $w = f(x, y, z)$ in the algebraic sense.
A library doing this is essentially making \concept{computational graphs}
-- which is the topic discussed below.

\subsection{Introducing tensors}
A \concept{tensor}, in the context of deep learning,
is just a multidimensional \emph{variable} in function definitions in everyday algebra and calculus:
when we write \shortcode{a = b + c},
where \shortcode{a}, \shortcode{b} and \shortcode{c} are tensors,
\shortcode{a} \emph{remembers} that it comes from \shortcode{b} and \shortcode{c}.
This means we can take the derivative of \shortcode{a} with respect to \shortcode{b} and \shortcode{c}.

Various tensor libraries have been available for quite a while.
In this note we are going to use PyTorch.
In PyTorch, for instance, we can do the follows:

\begin{lstlisting}
import torch

x = torch.tensor(2.0, requires_grad=True)
y = torch.tensor(3.0, requires_grad=True)

u = 3*x + y**2
v = x**3

u.backward()
print(x.grad, y.grad) # Output: tensor(3.) tensor(6.)

# By default the gradient recorded on a PyTorch tensor accumulates
# to enable certain optimization tricks.
# we need to call .zero_() to zero out the gradient. 
x.grad.zero_()
y.grad.zero_()

v.backward()
print(x.grad, y.grad) # tensor(12.) tensor(0.)

\end{lstlisting}

What happens here is that we define 
\[
    u = 3x + y^2, \quad v = x^3,
\]
where $x = 2$, $y = 3$, and therefore 
\[
    \pdv{u}{x} = 3, \quad \pdv{u}{y} = 2y = 6, \quad \pdv{v}{x} = 3x^2 = 12, \quad \pdv{v}{y} = 0.
\]
Both \shortcode{u} and \shortcode{v} remember how they were defined:
this is known as \concept{computational graphs}.
When backpropagation starts at a tensor, say \shortcode{u},
the \shortcode{grad} attribute of all tensors that contribute to the calculation of \shortcode{u}
will be updated.
Sometimes we don't want backpropagation to go to certain tensors,
like the tensors containing training data:
this can be done by removing the \shortcode{requires_grad} parameter in the definition of the tensor.
PyTorch also enables us to define multidimensional arrays,
which then can be used to construct deep learning models that can be trained by backpropagation.

In theory, functions in programming languages can replace by computational graphs.
Probably to make the coding style more uniform,
PyTorch does not provide a straightforward way to reassign values to tensors
and then automatically trigger a forward propagation:
if we want to reassign values to \shortcode{x} or \shortcode{y},
the most idiomatic way to do so is the follows:

\begin{lstlisting}
import torch 

def calculate_u(x, y):
    return 3*x + y**2

x = torch.tensor(2.0, requires_grad=True)
y = torch.tensor(3.0, requires_grad=True)
u = calculate_u(x, y)
u.backward()
print(x.grad, y.grad) # Output: tensor(3.) tensor(6.)

x = torch.tensor(3.0, requires_grad=True)
y = torch.tensor(4.0, requires_grad=True)
u = calculate_u(x, y)
u.backward()
print(x.grad, y.grad) # Output: tensor(3.) tensor(8.)
\end{lstlisting}

As the listing demonstrates, if the value of \shortcode{x} or \shortcode{y} needs to change,
it is probably a good idea to seal the computational graph into a regular Python function,
and create a new tensor whenever we want a reassignment.
This makes sure that doing deep learning with PyTorch is still like writing regular Python code,
not working with an embedded DSL.
Computational graphs in PyTorch also do not support taking second-order derivatives:
therefore optimization can only rely on the first-order derivative of the model with respect to the model parameters.

\subsection{Defining a module}
A model, as is said above, is a function that takes in a tensor (by convention, the first dimension being the batch dimension) and outputs a tensor,
and it contains many learnable parameters.
A model can then be put into another model,
and when doing backpropagation,
\emph{all} parameters of the latter -- including those of the first model -- 
should be exposed to the optimizer.
The \shortcode{torch.nn.Module} is a base class that makes registering parameters easy:
it also provides a uniform coding style for developers to define models.
Below is an example of how to define a PyTorch model:

\begin{lstlisting}
from torch import nn 

class FashionMINSTClassifier(nn.Module):
def __init__(self):
    super().__init__()
    
    self.flatten = nn.Flatten()
    self.linear_relu_stack = nn.Sequential(
        nn.Linear(28*28, 512),
        nn.ReLU(),
        nn.Linear(512, 512),
        nn.ReLU(),
        nn.Linear(512, 10)
    )
    
def forward(self, x):
    x = self.flatten(x)
    logits = self.linear_relu_stack(x)
    return logits
\end{lstlisting}

A subclass of \shortcode{nn.Module} is callable:
predication therefore takes the form of \shortcode{model(X)}.
Assigning a \shortcode{nn.Module} object -- here it is \shortcode{linear_relu_stack} -- to \shortcode{self} triggers the \shortcode{__setattr__} method,
which, according to the definition in the \shortcode{nn.Module} class,
registers all parameters of the \shortcode{nn.Module} object in question.
Note that PyTorch has a \shortcode{nn.Flatten} network component,
which flattens all dimensions of the input besides the first dimension,
intended as the batch dimension.
\shortcode{nn.Sequential} is a subclass of \shortcode{nn.Module},
which assembles several models into a chain.
We can also define a reusable sequential model by writing

\begin{lstlisting}
class LinearReluStack(nn.Sequential):
    def __init__(self):
        super().__init__(
            nn.Linear(28*28, 512),
            nn.ReLU(),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Linear(512, 10)
        )
\end{lstlisting}

\subsection{Hardware acceleration}
Since as is shown in \eqref{eq:mlp}, a neural network model involves a lot of linear algebra operations,
both prediction and training can be drastically accelerated by GPUs.
PyTorch tensors and \shortcode{nn.Module} objects can be sent to GPUs
by the \shortcode{.to("cuda")} method and sent back to the host by \shortcode{.to("cpu")}.
Most users will not need to write CUDA kernels themselves.

\section{What is approximated by neural networks}\label{sec:framework.statistics}

As is implied in \prettyref{sec:modern-network},
modern neural networks are not conventional statistical models:
their inductive biases are not explicitly endowed to them by making assumptions about the statistical structure of the problem.
Still by treating the whole neural network model as a whole as a statistical model,
we can have some insights on the training target. 

\subsection{The two schools}

Machine learning, from the perspective of statistics, is parameter estimation.
There are two main school of thoughts in parameter estimation.
\concept{Frequentists} consider the parameters in the model as unknown constants,
and the training dataset they consider a sampling of a ``real'', physical distribution.
Parameter estimation in the frequentist approach is a \emph{deterministic} algorithm
from a sampling to a deterministic estimation of the parameters.
How fast the estimation of the parameters given by the algorithm
converges to the true parameters as the number of samples increase
and the distribution of the error (which is also a random variable,
because the probabilistic distribution of the sample is a random variable)
decide how good a parameter estimation algorithm is.
Frequentist parameter estimation often employs the method of \concept{maximum likelihood estimation (MLE)}%
\footnote{
    Here we are using a rather sloppy notation.
    As $x$ is essentially a random variable,
    we should have written something like $p(x = x_0)$, where $x_0$ is a deterministic quantity.
    Or we can write $p_{x}(x_0)$.
    The standard in statistics is actually $p(X = x)$,
    where $X$ is the random variable and $x$ is one of its possible values.
    In machine learning, though, we rarely need to make such a distinction.
}
\begin{equation}
    \hat{\theta} = \argmax_\theta p_\theta(x) = \argmax_\theta \log p_\theta(x),
    \label{eq:mle}
\end{equation}
where $x$ is the observation, and we have modeled the hidden distribution of the observation as a parameterized function $p_\theta(x)$.

In \concept{Bayesian statistics}, the parameters are \emph{also} considered as random variables.
This is because in Bayesian statistics, probability is \emph{subjective}.
On how reasoning on uncertain facts naturally results in probability theory, see e.g. Ref.~\cite{jaynes2003probability};
For some reason, the same mathematical discipline seems to be the underlying framework for both reasoning about uncertainty and for modeling a physical system interacting with a ``thermal bath''.
In Bayesian statistics, parameter estimation results in a likely distribution of $\theta$,
and not just a single estimation $\hat{\theta}$.
What is often done is the follows:
\begin{equation}
    p(\theta | x) = \frac{p(\theta)}{p(x)} p(x | \theta) = \frac{p(\theta) p(x | \theta)}{\sum_{\theta'} p(\theta') p(x | \theta')},
    \label{eq:bayesian}
\end{equation}
where we first assume a \concept{prior distribution} of $p(\theta)$,
and build a model by assuming a form of $p(x | \theta)$,
and then calculate $p(\theta | x)$ according the equation above.
When $\theta$ is continuous, $p(\theta)$ and $p(\theta | x)$ are actually probability densities,
and $p(\theta) \dd{\theta}$ is the probability for the random variable $\theta$ to fall between the value $\theta$ and $\theta + \dd{\theta}$.
Since the $\dd{\theta}$ factors on both sides of \eqref{eq:bayesian}, we get 
\begin{equation}
    p(\theta | x) = \frac{p(\theta) p(x | \theta)}{\int p(\theta') p(x | \theta') \dd{\theta'}}.
\end{equation}

\subsection{Independent sampling}

The prior can be viewed as the $p(\theta | x)$ from last inference,
provided that the sample used in the last inference and the sample used in this inference
are independent to each other.
Indeed, we can prove that 
\begin{equation}
    p(\theta | x_2, x_1) = \frac{p(\theta | x_1)}{p(x_2)} p(x_2 | \theta, x_1),
    \label{eq:two-samples}
\end{equation}
if and only if 
\begin{equation}
    p(x_1) p(x_2) = p(x_1 x_2)
    \label{eq:independence}
\end{equation}
and therefore if the prior is determined by past experiences (i.e. $p(\theta | x_1)$),
then the LHS of \eqref{eq:bayesian} is determined by past experiences \emph{completely summarized by the prior distribution} (i.e. $p(x_2 | \theta, x_1)$) and this sample.

Therefore, under the independence assumption \eqref{eq:independence}, 
all we need to build a model is the prior distribution $p(\theta)$ and the likelihood function $p(x | \theta)$.
The posteriori distribution $p(\theta | x)$ is then determined by \eqref{eq:bayesian},
and all other probabilistic inferences, like $p(\theta | x_1, x_2)$, can be made based on these quantities plus \eqref{eq:independence}.
For instance the a posteriori probabilistic distribution of $\theta$ after seeing a whole dataset is, by applying \eqref{eq:two-samples} repeatedly,
\begin{equation}
    p(\theta | \mathcal{D}) =  \frac{
        \prod_{x \in \mathcal{D}} p(x |\theta) p(\theta)
    }{
        \underbrace{\sum_{\theta'} \prod_{x \in \mathcal{D}} p(x | \theta') p(\theta')}_{p(\mathcal{D})}
    }.
    \label{eq:dataset-posterior}
\end{equation}
Further, the axioms of probability do not impose additional constraints on the relation between $p(\theta)$ and $p(x | \theta)$.
Therefore we are allowed to choose the two functions freely,
provided that the positivity, unitarity etc. are satisfied.%
\footnote{
    See \href{../../Math/Statistics/bayesian.pdf}{this note} for more discussions.
    The same note also discusses how $p (x | y, \theta)$ can be understood as $p_\theta(x | y)$ from a frequentist perspective:
    this substitution does not invalidate any probabilistic formula.
    Thus $p_\theta(x | y) = p_\theta(xy) / p_\theta(y)$, for instance.
    These trivial but important lemmas explain why in machine learning,
    people often feel free to switch between frequentist and Bayesian statistics,
    and also never explicitly write the form of $p(\theta | x_1, x_2)$.
    We're going to see a lot of these in the following discussions.

    Another thing to note is $\theta$ in theory can also include ``hard'' priors, 
    like ``the distribution of $y$ should follow a rough pattern parameterized by the soft pare of $\theta$'',
    although these hard priors are typically not updated.
    We discuss this -- and why these hypotheses can be not updated -- in the note linked above as well.
}

\subsection{Parametric estimation and the loss function}

The sum or integral in \eqref{eq:bayesian} usually cannot be evaluated analytically,
and techniques like Monte Carlo sampling have to be used.
As a compromise -- which is what is frequently done when $p(x | \theta)$ is modeled by a deep learning model
-- we may assume that $p(\theta | x)$ has a sharp peak,
and this results in the \concept{maximum a posteriori estimation (MAP)}
\begin{equation}
    \hat{\theta} = \argmax_\theta p(\theta | x) = \argmax_\theta (\log p(x | \theta) + \log p(\theta)).
    \label{eq:map}
\end{equation}
The \concept{loss function} is therefore the opposite of the expression in the RHS.
Note that here $x$ can also be the dataset $\mathcal{D}$ in \eqref{eq:dataset-posterior}.
Given the form of \eqref{eq:dataset-posterior},
the loss function should be additive with respect to different data points.

When the prior is trivial, which means it's the same for all $\theta$,
MAP reduces to MLE \eqref{eq:mle}.
Some thing similar to \eqref{eq:map} can also be derived from \eqref{eq:mle}:
from the perspective of the frequentists,
if we stipulate that certain values of $\theta$ are not allowed in our model in \eqref{eq:mle}
(probably because otherwise the model is to complex),
then we can add a Lagrangian multiplier term (called \concept{regularization}) into the minimization problem \eqref{eq:mle}:
\begin{equation}
    \hat{\theta} = \argmax_{\|\theta\| \leq C} p_\theta(x) = \argmax_{\theta, \lambda} (p_\theta(x) + \underbrace{\lambda (\norm*{\theta} - C)}_{\text{regularization}}).
    \label{eq:mle-lagrangian}
\end{equation}
Here $\|\theta\|$ is a way to measure the complexity of the model.
The equivalence is a consequence of Karush-Kuhn-Tucker conditions.
After maximization finishes, $\lambda$ in \eqref{eq:mle-lagrangian} is zero if $\norm*{\theta} < C$,
and is non-zero when $\norm*{\theta} = C$.
Still, usually we do not know what $C$ actually is,
and we often need to try different values of $C$ to see how well the model works.
This is equivalent to fixing $\lambda$ to a given value in \eqref{eq:mle-lagrangian},
and do maximization with respect to $\theta$ only.
What is done above is equivalent to setting
\begin{equation}
    p(\theta) \propto \ee^{\lambda \norm*{\theta}}
\end{equation}
in \eqref{eq:map}.

Therefore, it seems that MAP in Bayesian statistics (with or without a non-trivial prior)
is equivalent to MLE in frequentist statistics (with or without regularization).
Since MAP is just a specific case of Bayesian statistics,
it seems to follow that frequentist parameter estimation can be seen as a subset of Bayesian parameter estimation.

There are actually more correspondences between the two.
The \emph{estimated} parameter is a random variable (because the input sample is a random variable),
from the perspective of frequentists,
and it gets its value \emph{after} a sampling.
The frequentist distribution of the parameter calculated on a randomly chosen sample
looks quite like the a posteriori distribution of $\theta$ in Bayesian parameter estimation.
This similarity is formalized by the Bernsteinâ€“von Mises theorem and similar ``bridging'' theorems.

So how is this related to neural networks?
Note that \eqref{eq:mle-lagrangian} is a maximization problem -- or a minimization problem,
if we work with $- p_\theta(x)$.
So when the forms of $p_\theta(x)$ and $\norm*{\theta}$ are given 
-- in the language of Bayesian parameter estimation, when the forms of $p(x | \theta)$ and $p(\theta)$ are given --
we have already gotten an optimization target function.
Basically this is how loss functions are derived in \prettyref{chap:loss}.

\subsection{Modules of a model}

$p(x | \theta)$ is a rather vague notion because what's in $x$ and $\theta$ is not clear.
In supervised learning, for instance,
what we want a neural network to know is actually $p(y | x, \theta)$,
while since $(x, y)$ can be seen as a single data point,
by replacing $x$ with $(x, y)$ in the discussions above,
$p(x | \theta)$ becomes $p(x, y | \theta)$.
So there is an additional Bayesian relation to invoke: 
\begin{equation}
    p_\theta(x, y) = p_\theta(y | x) p_\theta(x),
\end{equation}
or in other words,
\begin{equation}
    p(x, y | \theta) = p(y | x, \theta) p(x | \theta).
\end{equation}
That's to say, if we're being serious, we can divide the model into two parts,
a generative part $p(x | \theta)$ and a discriminative part $p(y | x, \theta)$,
each represented by a neural network,
and the two should be trained together using a single loss function.

In real regression problems typically we just ignore dependence between $x$ and $\theta$.
But the above example still illustrates why sometimes a model may contain several subparts
trained together.

\section{Training}

\subsection{The PyTorch training API}
As is mentioned in \prettyref{sec:modern-network}, deep learning models are to be trained by gradient-based optimization,
the simplest being the \concept{gradient descent}
\begin{equation}
    \theta \leftarrow \theta - \eta \grad_\theta \mathcal{R}.
    \label{eq:sgd-example}
\end{equation}
A training procedure usually contains a number of \concept{epoches},
in each of which, we use up all data available in the training dataset.
As is said above in \prettyref{sec:modern-network}, 
we train neural networks in batches, so an epoch consists of several batches.
When the loss function is chosen, 
an epoch usually look like this:

\begin{lstlisting}
optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)
# For each epoch:
for (X, y) in dataloader: # a torch.utils.data.DataLoader object
    model.train() # Entering training mode 
    X = X.to(device)
    y = y.to(device)
    loss_Xy = loss(model(X), y)
    loss_Xy.backward()     # backpropagation
    optimizer.step()       # Apply optimizer once 
    optimizer.zero_grad()  # Avoid gradient accumulation
\end{lstlisting}

Again PyTorch provides us with a very unified API:
we calculate the loss, and the result is a tensor which remembers what parameters are used in the calculation,
and then we do backpropagation.
The optimizer knows all parameters we want to update,
and it changes their values according to the gradients recorded in their \shortcode{grad} attributes
and then zeroes out the recorded gradients.

It is not uncommon to have a \concept{validation dataset} besides the training dataset
as an unbiased evaluation of how well the model performs:
if, as training goes, the loss calculated on the training dataset drops,
but the loss calculated on the validation dataset doesn't,
then usually we're overfitting the data.

As for how to prepare the data,
PyTorch has two classes: \shortcode{Dataset} and \shortcode{DataLoader}.
The first should be the parent class of all user defined dataset classes:
a dataset object is expected to be iterable,
and at each iteration, it should provide a tuple
containing the input and the expected output,
\emph{without} a batch dimension.
A \shortcode{DataLoader} reads a \shortcode{Dataset} object and is also iterable:
at each iteration, it is expected to give a tuple 
containing a \emph{batch} of inputs and a \emph{batch} of expected outputs,
both of which \emph{have} an additional dimension at the first, i.e. the batch dimension.
It is encouraged to define subclasses of \shortcode{Dataset}
for different types of training datasets.

\subsection{Batching from the perspective of parameter estimation}
It is clear that when we have multiple batches -- and not a single, giant batch -- and use \emph{different} batches in different iterations of \eqref{eq:sgd-example},
we are \emph{not} doing any of the parameter estimations in \prettyref{sec:framework.statistics},
the latter requires the training dataset to be the same in the training process.
Still, we can treat each batch as the ``ideal'' sample with some variations,
and since usually the training dataset is shuffled before training (when creating \shortcode{dataloader}),
we are essentially doing parameter estimation (as is outlined in \prettyref{sec:framework.statistics})
with a gradient-based optimizer (e.g. the one described in \eqref{eq:sgd-example}),
and at each iteration, we have \emph{random noises} corresponding to the makeup of the batch being used.
Batching therefore makes training more robust as it avoids the model falling into a local minimum.
How to choose the batch size however is not clear from a statistical perspective, and usually needs empirical testing to decide.

\section{Theoretical aspects}\label{sec:theoretical-aspects}

The \concept{no free lunch theorem} states it is not possible to have a one-size-fit-all machine learning paradigm.
The wild success of deep learning seems to go against it -- or if we sit down and take a deep breath, it seems to indicate that most machine learning problems we are interested in have something deep in common.
This observation has important consequences.

Large language models (LLM) do not have any explicit biases towards possible natural languages.
Does this mean that Chomsky is wrong,
and natural languages are completely results of domain-general cognitive abilities,
and there is no dedicated language faculty in human brain?%
\footnote{
    Note that the success of large language models is a challenge to \emph{all} schools of linguistics,
    and also old-styled NLP (yeah, including not-so-large neural models).
    Proponents of usage-based approaches to linguistics often claim that the success of LLM 
    demonstrate that they are right in emphasizing statistical learning in language acquisition.
    Yet LLMs do not have explicit structural biases for statistical learning as we know it either.
    LLMs know no X-bar syntactic trees, and neither do they know anything about HPSG unification, lambda expressions, constructions or embodied cognition.
    And statistical learning models designed by usage-based linguists suck
    just like minimalist grammars do.
}
Currently nothing substantial can be said on this question:
``human linguisticality'' (to avoid the term Universal Grammar) is likely beyond context-free grammars (see e.g. Swiss German's cross-serial dependencies),
and yet we do not even know whether typical LLMs can learn all context-free languages.
It is not impossible that the relation between neural networks and Turing machines
is also the relation between LLMs and human linguisticality:
there is evidence for LLMs' ability to learn impossible languages better than humans do.%
\footnote{
    Designing the correct metric and interpreting the results is extremely hard here.
    arXiv 2401.06416 observes that LLMs also seem to have difficulties learning languages impossible for humans.
    The impossible language used in the paper has a count-based grammar,
    which shifts the English verbal inflection rightwards by four words.
    The problem with this test, however, is that it is comparing adjacency (i.e. inflectional morphemes being attached to the verb stem) and count-based non-adjacency,
    while Chomskyan generative syntax focuses on more subtle differences,
    like \emph{constituency-based non-adjacency} vs. \emph{count-based non-adjacency}
    (see arXiv 2410.12271).
    arXiv 2502.12317 shows that typologically implausible languages can be learned by LLMs at a slower pace.
    They take this fact to indicate that typological preferences can be explained by domain-general factors.
    Yet if LLMs indeed \emph{learn} the impossible language at the end,
    while certain typologically implausible languages turn out to be \emph{impossible} and \emph{cannot} be learned by humans,
    then the conclusion of the paper should be drastically revised:
    it means the \emph{competence} of LLMs exceed that of humans in certain aspects,
    although their performances show certain similar patterns.
    Similarly, arXiv 2502.18795 demonstrates that human learners show a stronger dislike towards unattested word orders than LLMs,
    although the latter also show weak inductive biases against them.
    (The paper does prove one thing: LLMs like constituency.
    A rather ironic situation is that certain radical usage-based schools actually do not think constituencies exist.)
    Large Language Models and the Argument From the Poverty of the Stimulus by Nur Lan, Emmanuel Chemla, and Roni Katzir also shows that 
    how an initial success story of a LLM learning a subtle syntactic phenomenon
    (and thus possibly refuting the Poverty of Stimulus argument)
    can be turned into a story \emph{supporting} PoS by pointing out that the LLM
    fails at some cleverly constructed (but still simple) examples,
    and how can more data (\emph{ample} stimulus!) partially cure the problem.
    LLMs learn certain impossible languages better than us do,
    and they learn possible languages worse than we do:
    this is a demonstrated fact,
    and whether this means anything to psycholinguistics,
    no one really knows.
}
And the fact that LLMs do have certain behaviors comparable to human psycholinguistics
is quite hard to interpret:
it has been proposed, for instance, that the human brain does store common syntactic structures, and the full combinatorics of grammar is not always on,
and the whole system is related to the working memory or other domain general systems (see https://facultyoflanguage.blogspot.com/2016/09/brains-and-syntax-part-2.html).
This picture of human linguisticality is consistent with LLMs' success
and also consistent with the divergence of LLMs from human psycholinguistics:
this theory is no more complicated than the ``LLM=human psycholinguistics'' theory (which struggles to explain certain fMRI phenomena) or the constructionist theory.
So does the success of LLMs tell us anything about human languages?
Probably, but no one really knows what it is.
And finally, note that LLMs are probably not that domain general, after all,
considering that devils hide in details of fine tuning and minor modifications of the model architecture.

\section{Evaluating the model}

Pure statistic methods sometimes do not best serve our purposes.
Suppose we are doing a binary classification problem,
in which true positives are extremely rare
but we can't afford to miss them.
Now, a naive data scientist may think their model --
which (correctly, by the way) reports that most of the time the incident in question does not happen.
What do we have to say about their model?
Probably ``useless'' -- but it can be a good estimation of the true probabilistic distribution.

Evaluating a model means we have \emph{preference}:
some errors feel more acceptable than others.
Often, we define various \concept{utility funtions} -- often known as \concept{metrics} -- to evaluate models.
One may want to maximize these utility functions in training,
instead of the loss functions defined by MAP.
Depending on one's philosophical stance this may actually the right thing to do in theory%
\footnote{
    Basically, it depends on what you think a ``correct'' theory of decision should be.
}
but it is almost always mathematically hard.
We also note that sometimes it \emph{is} a good idea to try to maximize the utility function,
for instance in reinforced learning.

\section{Summary and notations}

\begin{itemize}
    \item Modern neural networks consist of the follows: matrix multiplication, element-wise activation functions, and occasionally, element-wise nonlinear operations involving \emph{two} inputs.
    The last, by universal approximation theorem, can be mimicked by the first two.

    \item The loss function is to be derived from the MAP 
    \begin{equation}
        \begin{aligned}
            \hat{\theta} &= \argmax_\theta \log p(\theta | \mathcal{D}) = \argmax_\theta \log \frac{\prod_{x \in \mathcal{D}} p(x | \theta) p(\theta)}{p(\mathcal{D})} \\
            &= \argmax_\theta \big(
                \sum_{x \in \mathcal{D}} \log p(x | \theta) + \underbrace{\log p(\theta)}_{\text{regularization}}
            \big).
        \end{aligned}
    \end{equation}
    It can also be understood as MLE with regularization.

    \item The $x$ and $\theta$ in $p(x | \theta)$ may be decomposed into fields and $p(x | \theta)$ may be factorized into several models.
    This can be implemented using \shortcode{torch.nn.Module},
    which registers parameters in it defined by \shortcode{self.x = y}.
    In doing the derivations, $p(A | B, \theta)$ and $p_\theta(A | B)$ can be used interchangeably,
    and all probabilistic equalities like Bayesian formula all hold.

    \item Training of neural network models is typically done by gradient-based methods
    (which are \emph{not} based on e.g. Hessian matrix).
    This is implemented in PyTorch by \shortcode{tensor.backward()}:
    a tensor is a multidimensional array that remembers how it was calculated using other tensors.
    In PyTorch, \shortcode{tensor.grad} stores the derivative of the last tensor on which \shortcode{backward} is called with respect to \shortcode{tensor}.
    
    \item One should design metrics for models depending on what errors they think are acceptable and what are not.
\end{itemize}

\chapter{Tasks, loss functions, and model architectures}\label{chap:loss}

\section{Supervised learning}

\subsection{Probabilistic theoretic structure of the problem}

Suppose our dataset is made of pairs of labels $y$ and features $x$.
People with a strong background in physics may tend to model the relation between $x$ and $y$ as 
\begin{equation}
    p(\theta | x, y) = \frac{p(y | x, \theta) p(x | \theta) p(\theta)}{p(x, y)} 
\end{equation}
and then give $p(\theta)$, $p(x | \theta)$ and $p(y | x, \theta)$ explicit forms.
The problem is that if $\theta$ are parameters in a \emph{physical} process,
then $x$ \emph{of course} depends on $\theta$.
For instance we may imagine that some scenarios are simply easier for experimentalists to handle
so we have more data collected in these scenarios.
Now the parametric estimation problem takes the form of 
\begin{equation}
    \begin{aligned}
        \hat{\theta} &= \argmax_\theta \sum_{x, y} p(\theta | x, y) \\
        &= \argmax_\theta (\log p(\theta) + \sum_{x, y} \underbrace{\log p(x | \theta)}_{\text{???}} + \log p(y | x, \theta)) .
    \end{aligned}
\end{equation}

A more serious treatment of the problem, when $x$ and $\theta$ do show have strong correlation,
will be discussed when we deal with generative models.
For now, we \uline{assume that actually $x$ and $\theta$ do not have strong correlations}.
This means $p(x | \theta) = p(x)$ and $p(\theta) = p(\theta | x)$.
Now this means the $p(x | \theta)$ term is now independent to $\theta$, and the MAP problem becomes 
\begin{equation}
    \hat{\theta} = \argmax_\theta \big(\log p(\theta) + \sum_{(x, y) \in \mathcal{D}} \log p(y | x, \theta) \big).
    \label{eq:y-xtheta-no-generative}
\end{equation}

\subsection{Regression}

When $y$ is a continuous variable,
which is typically the case when the problem is called a \concept{regression} problem,
we often further assume that \uline{when $x$ and $\theta$ are given, the distribution of $y$ is solely defined by its expectation and unknown variables that decide the broadening of the distribution that do not depend on $x$ and $\theta$}.
This is to say 
\begin{equation}
    y = f(x; \theta) + \epsilon,
\end{equation}
where $\epsilon$ is a random variable whose mean vanishes.
In physics, for example, $\epsilon$ may come from Langevin noise.
Thus 
\begin{equation}
    p(y | x, \theta) = p(y - f(x; \theta)).
\end{equation}

So we want \uline{the neural network model to approximate $f(x; \theta)$}.
When \uline{$\epsilon$ is gaussian}, $\log p(y | x, \theta)$ leads to the \uline{mean squared error} loss function.

\subsection{Binary classification}

When the problem is binary classification, \eqref{eq:y-xtheta-no-generative} is often still valid,
but it makes no sense to talk about the expectation of $y$:
the probabilistic distribution of a discrete variable doesn't look like $p(y - f(x; \theta))$ at all.

This is most clear in the case of binary classification, in which the probabilistic distribution always takes the form of 
\begin{equation}
    p(y| x, \theta) = \begin{cases}
        p(y = 1 | x, \theta)     , & y = 1, \\
        1 - p(y = 1 | x, \theta)  , & y = 0,
    \end{cases}
\end{equation}
which means two things.

First, the model architecture now should map $x$ to a scalar,
which is to be interpreted as $p(y = 1 | x, \theta)$.
\uline{We refer to the output of the model as $\hat{p}(x; \theta) = p(y = 1 | x, \theta)$}.
This interpretation introduces and only introduces the constraint $0 \leq p(y = 1 | x, \theta) \leq 1$,
which in turn means \uline{a sigmoid function as the last layer}.

Second, we can equivalently write 
\begin{equation}
    p(y| x, \theta) = p(y = 1 | x, \theta)^y (1 - p(y = 1 | x, \theta))^{1 - y},
\end{equation}
and the parametric estimation problem now takes the form 
\begin{equation}
    \hat{\theta} = \argmax_\theta \big( \log p(\theta) + \sum_{(x, y) \in \mathcal{D}} y \log \hat{p}  + (1 - y) \log (1 - \hat{p}) \big).
\end{equation}
This is often known as the \uline{binary cross-entropy loss}.


\subsection{Multi-class classification}

When we are to assign a discrete label to an input,
the label can be encoded as a one-hot vector:
\begin{equation}
    \vb{y} = (y_1, \dots, y_K), \quad y_k \in \{0, 1\}, \quad \sum_k y_k = 1.
\end{equation}
Each $\vb{y}$ corresponds to a possible label,
and there are $K$ possible values of $\vb{y}$.

Now we link the posterior probability to the neural network architecture.
Naturally we want the model to output $K$ numbers to characterize how likely each label is.
That's to say, the output of the model is \uline{a $K$-dimensional vector $[p_k(x; \theta)]_k$}.
Further, typically, we want \uline{the $k$th output to be $p(\vb{y} | x, \theta)$ where 
the $k$th element of $\vb{y}$ is non-zero}.
This does not introduce any additional assumptions.
So we have 
\begin{equation}
    p(\vb{y} | x, \theta) = \prod_{k=1}^K p_k^{y_k}.
\end{equation}
We emphasize here again that $\vb{y}$ in this section is always a one-hot vector and $y_k$'s here are \emph{not} continuous variables.

The constraints brought by this interpretation of the output of the model are 
\begin{equation}
    0 \leq p(\vb{y} | x, \theta) \leq 1, \quad \sum_{\vb{y}} p(\vb{y} | x, \theta) = 1.
\end{equation}
This is equivalent to say
\begin{equation}
    0 \leq p_k \leq 1, \quad \sum_k p_k = 1,
\end{equation}
which in turn means \uline{adding a softmax layer to the end of the model}.

The parametric estimation problem now reads
\begin{equation}
    \hat{\theta} = \argmax_\theta \big( \log p(\theta) + \sum_{(x, \vb{y}) \in \mathcal{D}} \sum_{k}^{K} y_k \log p_k(x; \theta) \big).
\end{equation}
This, again, gives us the \uline{cross-entropy loss}.


\section{Variational Autoencoder}

Suppose we want to compress a bunch of data into a much smaller \concept{latent} space.
This involves an encoder from $x$ to $z$, and a decoder from $z$ to $x$.
We do not know what exactly $z$.
Still, if we have an approximate form of $p(x)$,
we can do a maximal likelihood estimation.
In the derivation below we mostly ignore the priors of the model parameters.

\subsection{Derivation of the ELBO}

A \concept{variational autoencoder (VAE)} is a compressor which maximizes the \emph{lower bound}
(hence the term ``variational'') of $p(x)$ derived from the non-negativity of the KL divergence.
Suppose we approximate $p(z|x)$ by an \concept{encoder} $q_\theta(z | x)$,
and we approximate $p(x|z)$ by an \concept{decoder} $p_\phi(x | z)$.
Here we adopt a frequentist perspective and treat the parameters $\theta, \phi$ as unknown constants.
Of course, $p(z | x)$ can also be approximated using 
\begin{equation}
    p_\phi(z | x) \coloneqq \frac{p(z)}{p(x)} p_\phi(x | z).
\end{equation}
The two estimations of the real $p(z | x)$ should be consistent.
We calculate their KL divergence, and note that because the KL divergence is always non-negative,
\begin{equation}
    \begin{aligned}
        D_{\text{KL}}(q_\theta(z|x) || p_\phi(z|x)) &= - \int q_\theta(z|x) \log \frac{p_\phi(z|x)}{q_\theta(z|x)} \dd{z} \\
        &= - \int q_\theta(z|x) \log \frac{p_\phi(x|z) p(z)}{q_\theta(z|x) p(x)} \dd{z} \geq 0.
    \end{aligned}
\end{equation} 
Equality occurs when the two distributions are indeed the same.
Manipulating this inequality we get 
\[
    \underbrace{- \int q_\theta(z|x) \log \frac{p(z)}{q_\theta(z|x)} \dd{z}}_{D_{\text{KL}}(q_\theta(z|x) ||p(z))} - \underbrace{\int q_\theta(z|x)  \log p_\phi(x|z) \dd{z}}_{\mathbb{E}_{z\sim q_\theta(z|x)}[\log p_\phi(x|z)]} + \log p(x) \geq 0,
\]
and therefore 
\begin{equation}
    \log p(x) \geq \underbrace{- D_{\text{KL}}(q_\theta(z|x) ||p(z)) + \mathbb{E}_{z\sim q_\theta(z|x)}[\log p_\phi(x|z)]}_{\text{ELBO}}.
    \label{eq:loss.vae.gaussian.elbo}
\end{equation}
The equality occurs when the encoder and the decoder describe the same $p(x|z)$ distribution.
The right hand side is known as the \concept{evidence lower bound (ELBO)}.

\uline{When the neural network architecture used in $p_\phi(x|z)$ and $q_\theta(z|x)$ are flexible enough},
there exists a subset of possible $(\theta, \phi)$ where the inequality is almost an equality,
and in that region, maximizing the ELBO is equivalent to the maximal likelihood estimation.
Praying that our optimization method can bring us there,
we \uline{use the ELBO function as the optimization target}.

\subsection{Closed form VAE loss: Gaussian latents}

We assume that \uline{$p(z)$ is a gaussian distribution}
\begin{equation}
    p(z) = \frac{1}{\sqrt{2\pi \sigma_p^2}} \exp(- \frac{(z - \mu_p)^2}{2 \sigma_p^2}),
    \label{eq:loss.vae.gaussian.z-prior}
\end{equation} 
and \uline{$q_\theta(z | x)$ is also a gaussian distribution parametrized by $x$ and $\theta$}
\begin{equation}
    q_\theta(z | x) = \frac{1}{\sqrt{2\pi \sigma_q^2}} \exp(- \frac{(z - \mu_q)^2}{2 \sigma_q^2}).
    \label{eq:loss.vae.gaussian.q-ansatz}
\end{equation}
The first assumption is a prior, as we assume that $\mu_p$ and $\sigma_p$ are all constants.
Whether it is a good prior can only be checked after we have the training dataset.
The second assumption is of particular importance,
because when making it, we have drastically simplify the structure of the encoder.
The encoder no longer takes both $x$ and $z$ as inputs;
rather, it takes in $x$ and returns $(\sigma_q^2, \mu_q)$
(which means $\mu_q$ and $\sigma_q^2$ are functions of $x$ -- and of course also $\theta$),
which is then given to \eqref{eq:loss.vae.gaussian.q-ansatz} to model the distribution of $z$.
This step is known as the \concept{reparameterization trick} (see arXiv 1312.6114).

The reparameterization trick distills the truly random part of $z$ (as predicted by $q_\theta(z|x)$),
and makes the rest of the model as deterministic as possible.
We note that the reparameterization trick is actually quite generic:
most ``reasonable'' probabilistic distributions in which $z$ is a continuous variable
cna be represented by reparameterization.

Besides making the model structure easier to write within a model neural network framework,
reparameterization solves one problem in training:
the calculation of the gradient of the ELBO.
If $p(z)$ and $q_\theta(z|x)$ were arbitrary probabilistic distributions,
the ELBO would have to be calculated using e.g. Monte-Carlo estimation,
and it is notoriously hard to calculate the gradient of a quantity estimated by Monte Carlo.
After reparameterization, we can write down a closed-form expression of the ELBO,
making training much easier.

The first term in the ELBO in \eqref{eq:loss.vae.gaussian.elbo} can be evaluated by definition
in a straightforward manner, although the calculation details are tedious (see arXiv 1907.08956).
We end up having
\begin{equation}
    - D_{\text{KL}}(q_\theta(z|x) || p(z)) = \log \frac{\sigma_q}{\sigma_p} - \frac{\sigma_q^2 + (\mu_q - \mu_p)^2}{2 \sigma_p^2} + \frac{1}{2}.
\end{equation}
We note that applying a linear transform to $z$ changes $\mu_p$ and $\sigma_p$ however we want,
and preserves the structure of \eqref{eq:loss.vae.gaussian.q-ansatz}.
Thus, without loss of generality, we set $\mu_p = 0$, $\sigma_p = 1$,
and 
\begin{equation}
    \begin{aligned}
        - D_{\text{KL}}(q_\theta(z|x) || p(z)) &= \log \sigma_q - \frac{\sigma_q^2 + \mu_q^2}{2} + \frac{1}{2} \\
        &= \frac{1}{2} \left(
            1 + \log \sigma_q^2 - \sigma_q^2 - \mu_q^2
        \right).
    \end{aligned}
\end{equation}

The second term in \eqref{eq:loss.vae.gaussian.elbo} depends on the architecture of the decoder,
which, given the assumptions \eqref{eq:loss.vae.gaussian.z-prior} and \eqref{eq:loss.vae.gaussian.q-ansatz},
cannot be chosen arbitrarily.
A common choice is to \uline{assume $p(x|z)$ should be gaussian}, which is mentioned in the appendix C of arXiv 1312.6114 without much discussions.
In this way the second term in \eqref{eq:loss.vae.gaussian.elbo} takes a form similar to that of \eqref{eq:loss.vae.gaussian.q-ansatz},
and 
\begin{equation}
    \mathbb{E}_{z \sim q_\theta(z|x)} [\log p_\phi (x|z)] \sim \sum_z
    \text{normalization factor w.r.t. $z$} \cdot \|x - \mu_p(z)\|^2.
\end{equation}
It is not not unusual to ignore the variance of the normalization factor with respect to $z$.
We can interpret $\mu_p(z)$ (now meaning the $\mu$ of $p_\phi(x|z)$, not $p(z)$) as the output of the decoder with a given $z$,
which is $\mu_q(x)$ plus a random noise (of which $\sigma^2 = 1$, $\mu = 0$).

The ELBO now has a closed-form expression, which leads to the following loss function:
\begin{equation}
    \mathcal{L}^{\text{ELBO}}(\theta, \phi) \sim \frac{1}{2} \sum_x 
    \big(
        (1 + \log \sigma_q^2 - \sigma_q^2 - \mu_q^2) + \sum_\epsilon N(\epsilon) \| x - \mu_p(\mu_q(x) + \epsilon) \|^2
    \big),
    \label{eq:loss.vae.gaussian.loss}
\end{equation}
where we may call $\mu_q(x)$ the encoder in the inference mode and $\mu_p(z)$ the decoder.
The neural network architecture has three parts: $\sigma_q^2(x; \theta)$, $\mu_q(x; \theta)$,
and $\mu_p(z; \phi)$.
\uline{$N(\epsilon)$ is typically reduced to a single constant which is adjusted manually.}
Batching of course applies to \eqref{eq:loss.vae.gaussian.loss},
and as \uline{the same $x$ will be fed to the model for many times during training when the training dataset is randomly shuffled},
the summation over $\epsilon$ can be merged into batching:
for a single batch, the loss function is the so-called \uline{VAE KL divergence plus MSE} 
\begin{equation}
    \mathcal{L}^{\text{ELBO}}_{\text{single batch}}(\theta, \phi) 
    = \frac{\beta}{2} \sum_{x}^{\text{batch}} (1 + \log \sigma_q^2 - \sigma_q^2 - \mu_q^2) 
    + \sum_x^{\text{batch}} \| x - \mu_p(\mu_q(x) + \epsilon) \|^2,
\end{equation}
where we have reduced $N(\epsilon)$ into a single hyperparameter $\beta$. 

\section{Sequence generation}

A trivial application of the 

\section{Reinforced learning}

In \concept{reinforced learning}, the model is expected to take a series of \concept{actions} 
that alter the state of the environment step by step,
and how appropriate each step is can be measured by a well-defined, known \concept{reward}.
That's to say, given $s \coloneqq s_n$, 
after the action at step $n$ $a \coloneqq a_n$ is taken,
the next step $s' \coloneqq s_{n+1}$ and the reward $r$ at step $n$ are given by 
\begin{equation}
    s' = \mathrm{next}(s, a), \quad r = \mathrm{reward}(s, a).
\end{equation}
Rewards and state transitions are probabilistic in general.
Obviously this is a \concept{Markov decision process}.

What is left to the model to do is first generating an action depending on the current current.
This is often known as the \concept{policy}.
A stochastic policy takes the form of $\pi(a | s)$.
The second thing, which sometimes is omitted, is modeling of the environment.

\subsection{The loss function}



\chapter{Convolutional neural networks (CNNs)}

\section{Convolutional layers}

In PyTorch tensors being passed between convolutional layers are four dimensional arrays,
with the four dimensions corresponding to the sample in the batch, the channel, the height and the width.

The definition of convolutional layers is mostly straightforward
and can be found in any serious introductions to neural network algorithms so we skip it.

A rule of thumb -- not a strict one -- is that when the CNN kernel size and the dilation make the output tensor small,
it's a good idea to increase the number of channels.

\section{Batch normalization}\label{sec:cnn.batch-norm}

The idea of normalization basically is follows.
A generic neural network layer takes the form of 
\begin{equation}
    z = f(Wx + b),
    \label{eq:z-from-x}
\end{equation}
and $z$ is then fed into the following layers.
This means when calculating the gradient of the loss function, we get something like 
\begin{equation}
    \pdv{L}{W} = \pdv{L}{z} f'(Wx + b) x.
\end{equation}
This means if the probabilistic distribution of $x$ has a large variance,
then the gradient is rather unstable.
To solve the problem we may replace \eqref{eq:z-from-x} by 
\begin{equation}
    z = \frac{f(Wx + b) - \mu}{\sigma},
\end{equation}
where $\mu$ and $\sigma$ are the expectation and the square root of the variance of $f(Wx + b)$.
This means 
\begin{equation}
    \pdv{L}{W} = \pdv{L}{z} \frac{1}{\sigma} f'(W x + b) x.
\end{equation}
The scale of $x$ and $\sigma$ should be comparable so the gradient neither explode nor vanish.

In real world problems, $\sigma$ and $\mu$ cannot be known a priori,
and therefore an alternative is to estimate them using data available in a batch.
This means when we need normalization we can add a batch normalization layer after a $f(Wx + b)$ layer
to calculate the average and the standard error of the outputs in a batch,
and then scale and shift these outputs.

For CNNs, the signal dimension of tensors being passed between layers has interpretable meanings:
therefore $\sigma$ and $\mu$ should be calculus by averaging over the batch, width and height dimensions,
but the channel dimension should not be averaged over.

\section{Dropout}

\chapter{Attention mechanisms and Transformers}

The up-to-date way to do machine learning tasks involving sequences with unknown structures
typically is various Transformer models.
As is said above in the introduction part, it remains theoretically unclear 
whether Transformer models can be seen as some sort of Universal Grammar (or human linguisticality) model for natural languages:
answering what Transformers see exactly is the same as the interpretability problem so we skip the issue for now.

\section{Embedding}


Suppose we have a sequence $(x_1, x_2, \dots, x_T)$.
Models taking sequence input that are not recurrent neural network models typically start with \concept{embedding}.

Embedding for Transformers typically consists of two parts.
The first part is \concept{token embedding}:
each token $x_t$ is mapped to a $d$-dimension vector $E(x_t)$ without considering its position in the sequence.
We refer to the size of the vocabulary of the sequence as $V$,
and therefore $E$ can be represented as a matrix $E \in \mathbb{R}^{V \times d}$.
The embedding process can then be written as 
\begin{equation}
    \vb{e}_t = x_t E \in \mathbb{R}^d,
\end{equation}
where $x_t$ now refers to the one-hot vector specifying which item in the vocabulary $x_t$ is.
Note that the vocabulary can be a list of words in a language,
but can also be a list of characters, or something between the two.

The second part is \concept{positoinal encoding}:
it maps $t$ to a vector $\vb{p}_t \in \mathbb{R}^d$.
This is typically done by passing $t$ to a list of sine and cosine waves.

For each position in the input sequence, we define 
\begin{equation}
    \vb{h}_t^{(0)} = \vb{e}_t + \vb{p}_t.
    \label{eq:transformer.embedding.sum}
\end{equation}
This is the input to the Transformer. Collectively, the $\vb{h}_t$ vectors are assembled into a matrix $H^{(0)} \in \mathbb{R}^{T \times d}$.

One may wonder why we adopt \eqref{eq:transformer.embedding.sum} as the definition of the representation of the sequence,
and not, say, concatenation $[\vb{e}_t ; \vb{p}_t]$.
Concatenation doubles the size of the representation vector,
and element-wise multiplication between $\vb{e}_t$ and $\vb{p}_t$ risks losing information because of zeroes in $\vb{p}_t$.
And the fact that in the nonlinear attention layer, $\vb{e}_t$ and $\vb{p}_t$ are mixed together
may actually be a good thing, as clearly one needs both the position and the content information.

We note that the size of the learnable parameter here -- $E \in \mathbb{R}^{V \times d}$ -- 
does not depend on the length of the input sequence.
This is to say the model can take sequences of arbitrary lengths,
although there is no theoretical guarantee that arbitrarily long correlations are always taken care of.

\section{Attention mechanism}\label{sec:transformer.attention}

A Transformer layer consists of a multi-head self-attention layer and a feed-forward network,
both with residual connections and layer renormalization.
Here we explain the attention mechanism first, before moving on to the structure of the encoder and the decoder modules.

A \concept{single-head self-attention} module in a neural network takes the following form.
It takes a $H \in \mathbb{R}^{T \times d}$ input (output of embedding).
It has three learnable matrix parameters $W_Q, W_K, W_V \in \mathbb{R}^{d \times d_k}$.
Here $d_k < d$ is a hyperparameter that decides how much the three matrices compress the input representation matrix.
We define 
\begin{equation}
    Q = H W_Q, \quad K = H W_K, \quad V = H W_V,
    \label{eq:transformer-projection}
\end{equation}
all of which have shape $T \times d_k$.
The output of the attention module, often known as a \concept{head}, is 
\begin{equation}
    \mathrm{Atten}(Q, K, V) = \softmax\left(\frac{Q K^\top}{\sqrt{d_k}}\right) V.
    \label{eq:attention-module}
\end{equation}
Note that $QK^\top$ is a square matrix of the shape $\mathbb{R}^{T \times T}$; the softmax function applies along its second dimension.
This is to say,
\begin{equation}
    \softmax( \underbrace{A}_{\in \mathbb{R}^{T \times T} } )_{ij} \coloneqq \softmax(A_{i:}).
\end{equation}
The shape of the output of the attention module is therefore $\mathbb{R}^{T \times d_k}$.

What exactly an attention module does is not completely clear,
although some hand-waving arguments can be done.
It's common to have an algorithm to compare a \concept{query} with a series of \concept{keys},
and if there is a match, then return the \concept{value}.
Now if we think about this, what \eqref{eq:attention-module} does is a smoothened version of this:
for each row of $Q$ -- each \concept{query vector} -- its inner product with keys (i.e. rows of $K$),
and if there is a match -- namely, if the query vector and one \concept{key vector} have a large inner product -- 
the softmax function returns a smoothened version of the one-hot vector containing the index of that key vector.
Multiplying the return value of the softmax function to $V$ 
selects the row of the $V$ -- the \concept{value vector} -- 
whose index is the same as the index of the key vector that matches the query vector.
Stacking all the value vectors together we get \eqref{eq:attention-module} in its matrix form.
We also note that the number of queries, keys, and values, is all $T$ in \eqref{eq:attention-module}:
this is again intuitively right,
because this roughly means the querying process 
allows a word (or a character, or whatever else) in the sequence to ask about other words.

The picture depicted above does not explain why a linear transform is enough in \eqref{eq:transformer-projection}.
Again, interpretability of Transformers, just like interpretability of other neural network architectures,
is currently unknown.
The form of \eqref{eq:transformer-projection} and \eqref{eq:attention-module} also say nothing 
about what kind of sequences Transformer models are best in dealing with.
We know for instance RNN does not capture long-range correlations.
On the other hand, there is no clear traceable measure of distance between words 
in embedding (which itself is already rather hard to interpret),
construction of the query, key and value matrices, and self-attention,
so it is rather hard to say anything about what kind of correlations Transformers are capturing.

\eqref{eq:attention-module} involves nonlinearity with respect to \emph{three} inputs,
while in a standard feed-forward neural network nonlinearity is with respect to only one input.
This however does not broaden the expressivity of neural networks, 
as multiplication of two inputs can be reasonably approximated by multiple ReLU functions.

One may also ask why there is a $\sqrt{d_k}$ factor.
$Q K^\top$ sums over the $d_k$ dimension,
so we expect the variance of $Q K^\top \propto d_k$,
assuming that there is little probabilistic correlation between different vector components.
This means $Q K^\top / \sqrt{d_k} \sim O(1)$,
and keeps the order of magnitude of the input to softmax stable.
This is important, as when calculating the gradient of the attention module with respect to $H$,
the $ W_Q W_K^\top$ factor appears outside the softmax function,
which possibly causes gradient explosion or vanishing.

A \concept{multi-head attention} module contains several single-head attention modules.
It calculates 
\begin{equation}
    \mathrm{head}_i = \mathrm{Atten}(Q_i, K_i, V_i),
\end{equation}
where $Q_i, K_i, V_i$ are calculated using different $W$ matrices.
Suppose there are $h$ heads calculated.
The output of the multi-head attention module is 
\begin{equation}
    \mathrm{MHA}(H) = \mathrm{Concat}(\mathrm{head}_1, \mathrm{head}_2, \dots \mathrm{head}_h) W_O,
\end{equation}
where $W_O \in \mathbb{R}^{(hd_k) \times d}$ is a matrix of learnable parameters.
Because the shape of each head is $\mathbb{R}^{T \times d_k}$,
the output of the multi-head attention module therefore is of the same shape of the embedding output, which is $\mathbb{R}^{T \times d}$.

\section{The encoder}

A \concept{encoder layer} consists of a multi-head self-attention layer and a feed-forward network,
both with residual connections and layer renormalization.

For an input $H$ from embedding, the return is 
\begin{equation}
    \mathrm{EncoderLayer}(H) = \mathrm{LayerNorm}(\tilde{H} + \mathrm{FFN}(\tilde{H})),
\end{equation}
where $\tilde{H}$ is given by 
\begin{equation}
    \tilde{H} = \mathrm{LayerNorm}(H + \mathrm{MHA}(H)),
\end{equation}
and FFN refers to a fully connected feed-forward network.
Typically $H$ also has one additional batch dimension.
The shapes of $\mathrm{EncoderLayer}(H)$ and $\tilde{H}$ are both $\mathbb{R}^{T \times d}$.

The $H + \mathrm{MHA}(H)$ and $\tilde{H} + \mathrm{FFN}(\tilde{H})$ operations are clearly residual connections,
which avoids the gradient vanishing problem.
This is because 
\begin{equation}
    \pdv{(h + f(h))}{w} = \pdv{h}{w} \left( 1 + \pdv{f(h)}{h} \right) \not\approx 0,
\end{equation}
where $w$ is a (possibly matrix) parameter and $f$ is the FFN or MHA function.

The layer norm function is similar to batch norm in \prettyref{sec:cnn.batch-norm}.
It also contains a linear layer after normalization containing learnable parameters for better expressivity
(and therefore, if normalization is improper in one place, it can be undone).
The concept and the motivation has already been introduced there;
the main difference is we don't have the concept of channels here,
and in batch normalization, normalization only works along the feature dimension (whose size is $d$).

Note that we do \emph{not} average over the batch dimension:
this is what batch normalization does, and not what is done typically in Transformer models.
Why? One issue is at autoregressive inference, there is simply no batching and hence batch statistics is undefined;
and in this scenario we cannot just rely on the running average
because sequences given to the model in a row are not statistically independent.
Another reason is, sequences in different samples in the same batch perhaps are not of the same length anyway,
so averaging over them makes zero sense.

\section{The decoder}

A \concept{decoder layer} is supposed to read the output of an encoder -- 
with the shape of $\mathbb{R}^{T \times d}$ -- 
and predict what next token to generate given a list of perviously generated tokens
\begin{equation}
    p(y_t | y_{<t}, H_{\text{enc}}).
\end{equation}

The list of perviously generated tokens itself is a sequence,
and is passed to the embedding module first, producing a $H_{\text{dec}} \in \mathbb{R}^{t \times d}$ matrix.
It is then first passed to a masked self-attention module:
\begin{equation}
    \mathrm{Atten}(Q, K, V) = \softmax \left( \frac{Q K^\top + M}{\sqrt{d_k}} \right) V,
\end{equation}
where 
\begin{equation}
    M_{ij} = \begin{cases}
        0, & j \leq i , \\
        -\infty & j > i.
    \end{cases}
\end{equation}
The presence of $M$ enforces the assumption that the $T$ dimension does represent where the token is in the sequence,
because -- based on the interpretation of attention discussed in \prettyref{sec:transformer.attention} -- $i$ corresponds to the position of the query vector and $j$ corresponds to the value vector,
and a token at $i$ is not supposed to query anything about a token coming behind it.

Followed by the masked self-attention module is a round of layer normalization and residual summation.

What follows then is a \concept{cross attention} module.
Basically in this module, $Q$ is from the input to the decoder, and $K$ and $V$ are from the output of the encoder.
Then another round of layer normalization and residual summation.
Then a fully connected feed-forward network layer and layer normalization and residual summation.



\chapter{Metrics}

\section{Classification}

We define a few quantities for discrete classification problems.
First, we define
\begin{equation}
    \text{Confusion matrix} = N [p(\hat{y} = i, y = j)]_{ij},
\end{equation}
where $N$ is the total number of samples.
Often the matrix elements of the confusion matrix are replaced by
numbers of $\hat{y} = i, y = j$ combos in actual samples supplied in the test dataset.

Furthermore,
\begin{equation}
    \mathrm{Recall}_a = p(\hat{y} = a | y = a),
\end{equation}
\begin{equation}
    \mathrm{Precision}_a = p(y = a | \hat{y} = a).
\end{equation}

\subsection{Yes-no models}

For a binary classification problem,
the confusion matrix has only four matrix elements.
We define $y=1$ as ``relevant''. 
We also define $\hat{y}=1$ as ``positive'' (or ``retrieved'') and $\hat{y}=0$ as ``negative''.
Depending on the value of $\hat{y}$, we can then talk about true or false positives and negatives.
We use TP, FP, TN, FN to refer to the four elements of the confusion matrix.

The recall of $y=1$ -- now simply known as the recall, also known as the true positive rate -- is now 
\begin{equation}
    \mathrm{Recall} = p(\hat{y} = 1 | y = 1) = \frac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FN}},
\end{equation}
and the precision of $y=1$ -- now simply known as the precision, also known as the positive predictive value -- is 
\begin{equation}
    \mathrm{Precision} = p(y=1 | \hat{y} = 1) = \frac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FP}}.
\end{equation}
We should note that the second steps in all equations above are only approximate
when the confusion matrix is from a one-shot test dataset.
In machine learning however since the datasets are typically large enough,
these problems are usually ignored.

Comparable to how the recall is defined, we define
\begin{equation}
    \mathrm{Specificity} = p(\hat{y} = 0 | y = 0) = \frac{\mathrm{TN}}{\mathrm{TN} + \mathrm{FP}}.
\end{equation}

The recall measures how large the intersect between relevant elements and retrieved elements is compared with all \emph{relevant elements}.
The precision measures how large the intersect between relevant elements and retrieved elements is compared with all \emph{retrieved elements}.

Obviously, to shrink the recall, we need to decrease FN,
i.e. make the model willing to report a positive case even though it may be a false positive.
This is relevant in e.g. medical diagnosis.
To shrink the precision, we need to decrease FP,
which is the right thing to do when building e.g. spam filters or fraud detection systems.

The \concept{F1 score} is defined as 
\begin{equation}
    \mathrm{F1} = \frac{2}{\frac{1}{\mathrm{Precision}} + \frac{1}{\mathrm{Recall}}},
\end{equation}
which is a harmonic mean of both precision and recall.
We note that it can also be written as 
\begin{equation}
    \mathrm{F1} = \frac{2\mathrm{TP}}{2\mathrm{TP} + \mathrm{FP} + \mathrm{FN}}
    = \frac{2\mathrm{TP}}{
        D + P
    },
\end{equation}
where $D$ refers to the number of reported positives, and $P$ the number of true positives.
Note that changing the model does not change $P$,
so there are only two independent variables in F1, namely TP and $D$.
Now, because 

\printbibliography[title=References]

\end{document}