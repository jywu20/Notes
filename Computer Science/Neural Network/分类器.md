# 分类器

## 线性模型

设想我们有某组向量$\{\bold{x}^{(i)}\}$，我们要使用这个向量内部包含的信息给它分类，总共分成$C$类。最简单的想法大概是使用一系列**超平面**把它们隔开，也就是说，使用超平面把整个空间分割成几块，向量在哪一块就算做属于哪一类。这种方法称为**线性模型**，因为使用线性的判别函数。

比较常用的方法是“一对其余”方法，也就是，总共使用$C$个超平面，每个超平面$\Pi_c$把空间分成两块：属于$c$类的和不属于$c$类的。我们定义判别函数为
$$
f_c(\bold{x}) = \bold{w}_c^\top \bold{x} + \bold{b}_c,
$$
并且约定：判别函数取正值，则$\bold{x}$在$c$类中，否则不在$c$类中。

## 最大值

这样的方法有一个不足之处：$C$个超平面能够划分出多于$C$个互不相交的空间，因此，必定存在这样的情况：某个样本$\bold{x}_0$同时满足$f_{c_1}(\bold{x}_0)>0, f_{c_2}(\bold{x}_0)>0$。这时我们应该把它分为哪一类呢？同样，也必定有这样的可能，就是某个样本没有被分到任何一类——这同样不可接受。

几何上，$f_c(\bold{x})$增大代表$\bold{x}$向着“能够被分到$c$类且不被分到其他类”的方向移动，因此我们可以要求，模型预测的分类为
$$
\hat{y} = \argmax_{c=1}^C f_c(\bold{x})
$$

不过这样的模型还是有问题——$\argmax$是一个不可微的函数，而训练模型的时候我们通常使用标准的优化方法，因此要求损失函数可微，从而要求模型可微。因此，为了方便训练模型，我们希望使用一个比较“软”的函数来代替不可微的$\argmax$：当输入向量中最大值的位置发生变化的时候，这个比较“软”的函数不应该发生太大的跳变，但同时它的输出必须要能够体现输入向量最大值的位置。

## Softmax
定义
$$
\mathrm{softmax} (\bold{x}) = \frac{\exp(\bold{x})}{\sum_i \exp([\bold{x}]_i)} = \frac{\exp (\bold{x})}{\bold{1}^\top \exp (\bold{x})},
$$
显然，如果$x_i$在整个$\bold{x}$中比较大，那么$[\mathrm{softmax}(\bold{x})]_i$也会明显增大。并且，$\mathrm{softmax}(\bold{x})$的各个分量之和为1。

因此，可以使用softmax函数做刚才提到的比较“软”的函数——想象我们有某个向量$\bold{x}$（也许是某个神经网络的输出），对它我们计算出$f_c(\bold{x})$，然后把结果输入到$\mathrm{softmax}$当中，输出的向量中最大值的位置就是模型预测的$\hat{y}$。

softmax的性质意味着可以使用它来表示$\bold{x}$属于类别$c$的“置信程度”，也就是后验概率。我们定义
$$
\hat{y}_c = p(y=c|\bold{x}), \quad \bold{W} = [\bold{w}_1, \bold{w}_2, \ldots, \bold{w}_C], \quad \bold{b} = [b_1, b_2, \ldots, b_C]^\top
$$
则有
$$
\hat{y}_c = \frac{\exp(\bold{w}_c^\top \bold{x} + b_c)}{\sum_{c'} \exp(\bold{w}_{c'}^\top \bold{x} + b_{c'})}
$$
或者说
$$
\hat{\bold{y}} = \{p(y=c|\bold{x})\}_c = \mathrm{softmax} (\bold{W}^\top \bold{x} + \bold{b}).
$$
当然，最终的输出结果还是要做一次argmax，也就是
$$
\hat{y} = \argmax \hat{\bold{y}},
$$
但是正如我们马上就可以看到的那样，训练模型的时候可以直接使用梯度下降法了。