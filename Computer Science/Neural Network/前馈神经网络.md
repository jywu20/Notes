# 前馈神经网络

## 架构
一个前馈神经网络(FNN)是一个明确**分成许多层**的神经网络，每一层都包含有限个神经元。每个神经元将**上一层的所有神经元**的输出作为输入，并将这些输入按照一定的**权值**做**线性叠加**，然后加上一个**偏置值**，将这个结果放进一个**激活函数**当中，最终得到的结果就是这个神经元的输出。

符号约定：
- $L$ 神经网络总层数
- $m^{(l)}$ 第$l$层的神经网络有几个神经元
- $f_l(\cdot)$ 第$l$层的激活函数
- $\boldsymbol{W}^{(l)}\in \reals^{m^{(l)} \times m^{(l-1)}}$ 第$l$层的权重矩阵
- $\boldsymbol{b}^{(l)} \in \reals^{m^{(l)}}$ 第$l$层的偏置
- $\boldsymbol{z}^{(l)} \in \reals^{m^{(l)}}$ 第$l$层的净输入，也就是上一层神经元的输出乘上权重后叠加再加上偏置值后的结果
- $\boldsymbol{a}^{(l)} \in \reals^{m^{(l)}}$ 第$l$层的输出，也就是净输入放到激活函数中计算后的结果


于是可以得到前向传播方程
$$
\boldsymbol{z}^{(l)} = \boldsymbol{W}^{(l)} \boldsymbol{a}^{(l-1)} + \boldsymbol{b}^{(l)}, \quad \boldsymbol{a}^{(l)} = f_l (\boldsymbol{z^{(l)}}).
$$
或者可以写出合并后的方程
$$
\boldsymbol{a}^{(l)} = f_l (\boldsymbol{W}^{(l)} \boldsymbol{a}^{(l-1)} + \boldsymbol{b}^{(l)}).
$$
给定输入$\boldsymbol{x} = \boldsymbol{a}^{(0)}$，我们可以按照上式做迭代计算，重复$L$次之后就得到了预测结果：
$$
\boldsymbol{x} = \boldsymbol{a}^{(0)} \rightarrow \boldsymbol{a}^{(1)} \rightarrow \cdots \rightarrow \boldsymbol{a}^{(L)} = \varphi(\boldsymbol{x};\boldsymbol{W}, \boldsymbol{b})
$$

现在我们手头上有一个样本$(\boldsymbol{x},\boldsymbol{y})$，我们把$\boldsymbol{x}$提供给神经网络，计算出$\hat{\boldsymbol{y}}$。在神经网络当中可以变动的参数有$\boldsymbol{W}^{(i)},\boldsymbol{b}^{(i)}$，我们希望能够合理调节这两个参数，从而使计算出来的$\hat{\boldsymbol{y}}$和$\boldsymbol{y}$能够足够接近，这样一来，在我们不能直接知道$\boldsymbol{y}$的时候，可以使用神经网络得到$\boldsymbol{y}$的一个近似值。现在的问题是，这样是否是可能的，也就是说，前馈神经网络的表现力是不是足以拟合任何性质足够好的函数？

根据通用近似定理，对于具有线性输出层和至少一个使用“挤压”性质的激活函数的隐藏层组成的前馈神经网络，只要其隐藏层神经元的数量足够，它可以以任意的精度来近似任何从一个定义在实数空间$\reals^d$中的有界闭集函数 [Funahashi and Nakamura, 1993, Hornik et al., 1989]。所谓“挤压”性质的函数是指像Sigmoid函数的有界连续递增函数，但神经网络的通用近似性质也被证明对于其它类型的激活函数，比如ReLU，也都是适用的。

需要注意的是，只能保证前馈神经网络可以很好地拟合**一定范围内**的函数值，超出这个范围的函数值可能能够被拟合，也可能不能。在超出范围的函数值不能被很好地拟合的时候，神经网络就是**过拟合**的——它在训练数据集上工作良好，但是并不能推广到真实的数据集上。

## 训练

### 梯度计算——反向传播
使用一个**损失函数**$\mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})$度量拟合值和真实值的差别。在已知一组样本$(\boldsymbol{x}^{(i)}, \boldsymbol{y}^{(i)})$合计$N$个的情况下，风险函数为
$$
\mathcal{R}(\boldsymbol{W}, \boldsymbol{b}) = \frac{1}{N}\sum_i \mathcal{L}(\boldsymbol{y}^{(i)}, \hat{\boldsymbol{y}}^{(i)}) + \frac{1}{2} \lambda \| \boldsymbol{W} \|_F^2.
$$
其中，
$$
\| \boldsymbol{W} \|_F^2 = \sum_{i=1}^L \| \boldsymbol{W}^{(i)} \|_F^2 = \sum_{l=1}^L \sum_{i=1}^{m^{(l)}} \sum_{j=1}^{m^{(l-1)}} (W_{ij}^{(l)})^2.
$$

所谓“训练”就是要调整权值和偏置的大小，让风险函数尽可能小。当然，这样会得到一系列极小值点，而我们需要的其实是一个最小值点。

为了获得极小值点我们通常需要计算风险函数随着权值和偏置的变化情况。
$$
\begin{aligned}
    \frac{\partial \mathcal{R}}{\partial \boldsymbol{W}^{(l)}} &= \frac{1}{N} \sum_i \frac{\partial \mathcal{L}(\boldsymbol{y}^{(i)}, \hat{\boldsymbol{y}}^{(i)})}{\partial \boldsymbol{W}^{(l)}} + \lambda \boldsymbol{W}^{(l)}, \\
    \frac{\partial \mathcal{R}}{\partial \boldsymbol{b}^{(l)}} &= \frac{1}{N} \sum_i \frac{\partial \mathcal{L}(\boldsymbol{y}^{(i)}, \hat{\boldsymbol{y}}^{(i)})}{\partial \boldsymbol{b}^{(l)}}.
\end{aligned}
$$
所以重点是损失函数的导数。

分别计算损失函数对单个权重和偏置的导数。
$$
\frac{\partial \mathcal{L}}{\partial W_{ij}^{(l)}} = \frac{\partial \boldsymbol{z}^{(l)}}{\partial W_{ij}^{(l)}} \frac{\partial \mathcal{L}}{\partial \boldsymbol{z}^{(l)}}, \quad \frac{\partial \mathcal{L}}{\partial \boldsymbol{b}^{(l)}} = \frac{\partial \boldsymbol{z}^{(l)}}{\partial \boldsymbol{b}^{(l)}} \frac{\partial \mathcal{L}}{\partial \boldsymbol{z}^{(l)}} .
$$
分别计算两个式子，
$$
\frac{\partial \boldsymbol{z}^{(l)}}{\partial W_{ij}^{(l)}} = \frac{\partial \boldsymbol{W}^{(l)}\boldsymbol{a}^{(l-1)}}{\partial W_{ij}^{(l)}} = \mathbb{I}_{i}(a_j^{(l-1)})^\top \in \reals^{1\times m^{(l)}},
$$
$$
\frac{\partial \boldsymbol{z}^{(l)}}{\partial \boldsymbol{b}^{(l)}} = \frac{\partial \boldsymbol{b}^{(l)}}{\partial \boldsymbol{b}^{(l)}} = \boldsymbol{I} \in \reals^{m^{(l)}\times m^{(l)}},
$$

通过链式法则，我们成功地将关于$\boldsymbol{W}^{(l)}, \boldsymbol{b}^{(l)}$的损失函数导数转化为了关于$\boldsymbol{z}^{(l)}$的损失函数导数。因此现在要计算损失函数对$\boldsymbol{z}^{(l)}$的导数。很容易看出，由于神经网络层和层之间是紧密联系的，不可能找到一个单一的表达式来计算这个导数。而且$\boldsymbol{z}^{(l)},\boldsymbol{z}^{(l+1)}$还多半不是独立的。因此，我们退而求其次，希望找到一个将两个相邻的层对应的导数联系起来的公式。同样，这又是通过链式法则做到的。

我们知道$\boldsymbol{z}^{(l)}$确定了$\boldsymbol{a}^{(l)}$，$\boldsymbol{a}^{(l)}$又确定了$\boldsymbol{z}^{(l+1)}$，这样就有公式
$$
\begin{aligned}
    \frac{\partial \mathcal{L}}{\partial \boldsymbol{z}^{(l)}} &= \frac{\partial \boldsymbol{a}^{(l)}}{\partial \boldsymbol{z}^{(l)}} \frac{\partial \boldsymbol{z}^{(l+1)}}{\partial \boldsymbol{a}^{(l)}} \frac{\partial \mathcal{L}}{\partial \boldsymbol{z}^{(l+1)}} \\
    &= \frac{\partial f_l(\boldsymbol{z}^{(l)})}{\partial \boldsymbol{z}^{(l)}} \frac{\partial (\boldsymbol{W}^{(l+1)} \boldsymbol{a}^{(l)} + \boldsymbol{b}^{(l+1)})}{\partial \boldsymbol{a}^{(l)}} \frac{\partial \mathcal{L}}{\partial \boldsymbol{z}^{(l+1)}} \\
    &= \mathrm{diag} (f'_l(\boldsymbol{z}^{(l)})) (\boldsymbol{W}^{(l+1)})^\top \frac{\partial \mathcal{L}}{\partial \boldsymbol{z}^{(l+1)}} \\
    &= f'_l(\boldsymbol{z}^{(l)}) \odot \left((\boldsymbol{W}^{(l+1)})^\top \frac{\partial \mathcal{L}}{\partial \boldsymbol{z}^{(l+1)}}\right).
\end{aligned}
$$
若定义误差项
$$
\boldsymbol{\delta}^{(l)} = \frac{\partial \mathcal{L}}{\partial \boldsymbol{z}^{(l)}},
$$
就得到公式
$$
\boldsymbol{\delta}^{(l)} = f'_l(\boldsymbol{z}^{(l)}) \odot \left((\boldsymbol{W}^{(l+1)})^\top \boldsymbol{\delta}^{(l+1)}\right).
$$
此外，
$$
\frac{\partial \mathcal{L}}{\partial W_{ij}^{(l)}} = \delta^{(l)}_i a^{(l-1)}_j, \quad \frac{\partial \mathcal{L}}{\partial \boldsymbol{b}^{(l)}} = \boldsymbol{\delta}^{(l)}.
$$

这样一来，只需要计算出最后一层神经元的误差项，使用上式就可以反向逐层计算出各层的误差项，然后就能够计算出每一层损失函数的导数了。这就是反向传播这个名字的由来：误差从最后一层逐层往回传播。

而最后一层的误差项为
$$
\begin{aligned}
    \boldsymbol{\delta}^{(L)} &= \frac{\partial \mathcal{L}}{\partial \boldsymbol{z}^{(L)}} = \frac{\partial \boldsymbol{a}^{(L)}}{\partial \boldsymbol{z}^{(L)}} \frac{\partial \mathcal{L}}{\partial \boldsymbol{a}^{(L)}} \\
    &= \frac{\partial f_L(\boldsymbol{z}^{(L)})}{\partial \boldsymbol{z}^{(L)}} \frac{\partial \mathcal{L}}{\partial \boldsymbol{a}^{(L)}} = \mathrm{diag}(f'_L(\boldsymbol{z}^{(L)})) \frac{\partial \mathcal{L}}{\partial \boldsymbol{a}^{(L)}} \\
    &= f'_L(\boldsymbol{z}^{(L)}) \odot \frac{\partial \mathcal{L}}{\partial \boldsymbol{a}^{(L)}}.
\end{aligned} 
$$

于是我们得到计算损失函数导数的公式，也就是所谓的反向传播公式
$$
\left\{
    \begin{aligned}
        \boldsymbol{\delta}^{(L)} &= f'_L(\boldsymbol{z}^{(L)}) \odot \frac{\partial \mathcal{L}}{\partial \boldsymbol{a}^{(L)}}, \\
        \boldsymbol{\delta}^{(l)} &= f'_l(\boldsymbol{z}^{(l)}) \odot \left((\boldsymbol{W}^{(l+1)})^\top \boldsymbol{\delta}^{(l+1)}\right), \\
        \frac{\partial \mathcal{L}}{\partial \boldsymbol{W}^{(l)}} &= \boldsymbol{\delta}^{(l)} (\boldsymbol{a}^{(l-1)})^\top, \\
        \frac{\partial \mathcal{L}}{\partial \boldsymbol{b}^{(l)}} &= \boldsymbol{\delta}^{(l)}.
    \end{aligned}
\right.
$$

### 梯度更新——训练

在已经获得了梯度下降的公式的情况下就可以通过计算梯度来最小化损失函数从而进行机器学习。现在我们有两个数据集，训练集$D$和验证集$V$。对于训练集中的每一个样本，我们可以执行下面的操作来最小化风险函数：
1. 使用样本特征$\boldsymbol{x}$前馈计算$\boldsymbol{a}^{(l)},\boldsymbol{z}^{(l)}$
2. 使用样本标签$y$得出损失函数$\mathcal{L}(\boldsymbol{y}, \cdot)$
3. 反向传播计算$\boldsymbol{\delta}^{(l)}$
4. 计算$\partial \mathcal{L}/\partial \boldsymbol{W}^{(l)}, \partial \mathcal{L}/\partial \boldsymbol{b}^{(l)}$
5. 梯度下降调整$\boldsymbol{W}^{(l)}, \boldsymbol{b}^{(l)}$，即做这样的操作：$\boldsymbol{W}^{(l)} \leftarrow  \boldsymbol{W}^{(l)} - \eta \partial \mathcal{R} / \partial \boldsymbol{W}^{(l)}$, $\boldsymbol{b}^{(l)} \leftarrow  \boldsymbol{b}^{(l)} - \eta \partial \mathcal{R} / \partial \boldsymbol{b}^{(l)}$。

TODO：后处理，即$\boldsymbol{a}^{(L)}$还需要经过一步变换才能够得出$\boldsymbol{y}$的情况怎么办