\documentclass[../main.tex]{subfiles}

\begin{document}

\section{Existing descriptive frameworks}\label{sec:descriptive-framework}

\subsection{Infeasibility of using derivational syntax as a descriptive tool}\label{sec:generative-no-good}

Though Minimalism is the most prevalent framework in the generative enterprise, 
we should acknowledge that the framework is not a good choice for descriptive means \citep{dryer2006descriptive}, 
and more surface-oriented grammatical theories (``descriptive theory'' in Dryer's terms) are required. 
The ultimate reason is Minimalism tends to work with rather abstract and fine-grained features 
with lots of movements and spell-out related post-syntactic morphological processes, 
which is just the purpose of generative research but proves not suitable for language describing from sketch.
When faced with real-world data, 
it is technically impossible for linguists to immediately work out the right derivation process. 
To list a few representative cases:
\begin{itemize}
    \item  How to decide the correct derivation procedure of an ergative language, since 
    linguists are still debating about several possible mechanisms?
    \item How to describe subcagegorization? By selectional features as in Minimalist Grammar,
    or by \ac{dm}-like spellout-based mechanisms\citep{siddiqi2009syntax}?
    \item How to account for different NP word orders? Should we accept them as they are, or should we introduce 
    some movements without clear motivation \citep{cinque2005deriving} to derive them from a universal supine?
\end{itemize}
A lot more questions can be added to this list. We see that surface-based ``shallow'' analysis and fine-grained 
analysis are conflicting, and hence it is reasonable to adopt other frameworks for language description.

Compared to Minimalism, GB is less fine-grained and we can expect it may be a better descriptive theory,
and there are GB grammars about underdocumented languages, for example \citet{holmer1996parametric}. 
This work, however, has poor reputation in linguists 
who care more about grammar writing than so-called in-depth analysis \citep{van2007holistic}.
These linguists contribute the most to our knowledge about particular languages, 
and they tend to be anti-generativism. Their words should definitely be taken seriously.
The viewpoint taken in this book is we do need descriptive theories in lieu of Minimalism,
but this is for practical reasons discussed above.
The formal sides of these descriptive theories, as I discuss later in this chapter, 
are essentially the same as Minimalism.

The other theory frameworks -- for example TAG -- are much better at grammar writing. 
We do not need much explanation for the descriptive power \ac{blt} or \ac{cgel}. 
For dependency grammar, we have the Universal Dependency project, 
which uses a unified annotation rule to build a large, multi-linguistic treebank \citep{ud}.
For \ac{tag}, we also have the XTAG project. % TODO: more citation

So now we have a list of a few existing structure-based frameworks that are relevant to our discussions:
\begin{itemize}
    \item Minimalism, which works on 
    \item The traditional GB-style X-bar scheme
    \item \ac{tag}  % TODO
    \item Dependency grammars, which is frequently used in computational works.
    \item \ac{blt}, the framework used in most contemporary descriptive works.
    \item The grammatical framework used in \ac{cgel} \citep{cgel,pullum2008expressive}, which is generative-informed and yet remaining 
    context-free and insists some analysis quite different from contemporary Minimalism (e.g. 
    what is a head -- I will discuss these apparent disagreements later).
\end{itemize}
Their differences can be roughly summarized as \prettyref{tbl:framework-comparison}. 
The ``surface-based partition'' and ``find-grained analysis'' rows 
explain why Minimalism fails as a descriptive theory. 
Minimalism is optimized for the latter, 
which other grammatical frameworks simply ignore, and as a consequence, 
Minimalism fails in the former task.

An obvious question is why there are so many frameworks, each of which seems to make some sense. 
In the rest of \prettyref{sec:descriptive-framework}, 
I explain items in \prettyref{tbl:framework-comparison}, 
the strength and weakness of each framework, 
and how these differences are mostly just notational differences 
and are more about methodology instead of worldview. 
The grammatical framework I use in this book for descriptive works is a mixture of all these framework,
with plenty of discussions on how these frameworks are related.
This is a reasonable choice in my own point of view. 
\citet{jacques2021grammar}, for example, is a typical \ac{blt} grammar, 
but when it comes the coordination strategies in NPs, 
some GB-style X-bar trees are used (Figure 9.1 and 9.2).

\input{tables/framework-comparison.tex}

\subsection{Equivalent formalism of Minimalism}

\begin{figure}
    \centering
    \input{axes/minimalism-equivalent.tex}
    \caption{Coordinates of equivalent Minimalism formalisms} 
    \label{fig:coordinate-minimalism}
\end{figure}

\prettyref{fig:coordinate-minimalism} gives a coordinate system which classifies possible equivalent formalisms
of Minimalism. \prettyref{fig:minimalism-mapping} illustrates how to explicitly build correspondence between 
different formalisms. In the following sections, I will talk about the three axes in these two figures. 

\begin{figure}
    \centering
    \input{axes/minimalist-equivalence-mapping.tex}
    \caption{Mapping between equivalent formalisms}
    \label{fig:minimalism-mapping}
\end{figure}

There is yet a final coordinate of formalisms equivalent to Minimalism 
beside what is shown in \prettyref{fig:minimalism-mapping}: 
pre-compiled trees (read: ``construction''). 
I will not talk about this topic too much in this section, 
because there is no single way to routinize Minimalism derivations, 
so routinization cannot be easily represented 
by adding a more coordinate into \prettyref{fig:coordinate-minimalism}.

\subsubsection{Dependency vs constituency}\label{sec:minimalist-dependency}

Dependency grammars are often said to be alternatives to constituency grammars.
This is a quite weird claim, since they are better described as \emph{dual} to each other.
It has already been demonstrated that there are dependency structures in Minimalism \citep{boston2009dependency},
and Minimalism, under certain adaptation, is equivalent to a dependency grammar \citep{osborne2011bare}.
This should not surprise anyone, since there have long been tools converting one into another in old-fashioned NLP.
It is actually also possible to mix constituency grammar and dependency grammar 
\cite{kahane-mazziotta-2015-syntactic}, in which a dependent ark may point to a constituent and not necessarily a word.

\subsubsection{Divergent standards of constituency: how to divide a large unit}\label{sec:divergent-standard-constituency-segmentation}

Note that in Distributed Morphology, a word -- a bundle of several features spelt-out together -- 
is a \emph{span} and not a constituent in the generative sense, 
and yet in lexicalist Minimalism, people recognize it as a construction. 
This helps to settle the bracketing dilemma in immediate constituency analysis.

So now we can see there is no conflict 
between the binary branching definition of \term{verb phrase} as Aux plus V plus NP_{\text{object}} 
and the \ac{blt} definition of verb phrase, which does not include the object NP. 
A good choice is to use the term \term{verb complex} to denote the verb phrase in the \ac{blt} sense \citep[among others]{Wilbur2014,Friesen2017}.

\subsubsection{Divergent standards of constituency: how flat the tree is}

The fact that the relative height of dependency relations are never considered 
in standard dependency grammars raises yet another issue about 
how flat the syntax \emph{constituency} tree should be,
since we can map a dependency grammar without any marking of relative height of dependency relations 
(i.e. a dependency grammar which does not have labels 1, 2, \dots on dependency arcs as in \prettyref{fig:minimalism-mapping})
into a constituency grammar with flat trees. 
It is kind of strange to talk about flat trees in a theory equivalent to Minimalism, 
but recall that in lexicalist Minimalism, 
a terminal node in a syntactic tree can have unrestrictedly large feature bundles i.e. unrestricted branching, 
and since we know lexicalist Minimalism can be seen as an equivalent formalism of DM-like Minimalism, 
it is nature to ask whether unrestricted branching for \emph{all} kinds of nodes 
-- not just feature nodes -- also gives an equivalent grammar class to the grammar class allowed in Minimalism.

Take the tree structure of a simple clause as an example. 
In \ac{blt}, Dixon accuses the binary division between 
the subject and the predicate as harmful (without providing much argumentation about it), but when discussing 
topic and pivots, he distinguishes the subject (in accusative languages, of course) from other arguments and regards it as having a somehow more 
prominent position. The arguments are licensed by the verb and have dependency relations with the verb. 
Introducing a position of pivot (or subject in accusative languages) besides the notion of deep S, A and O 
of course means introducing a layer of construction built \emph{upon} the argument structure layer
about ``the obligatory and syntactically codified topic'', 
which, in generative terms, is just the TP layer. It is of course possible to draw a syntactic tree 
with a quite flat clause structure and explain that ``the subject somehow has a higher position'', 
but this is eventually the same as the binary branch approach in Minimalism.

\begin{figure}
    \centering
    \input{trees/i-have-completed-the-task-minimalism.tex}
    \caption{The derivation of \emph{I have completed my task} in Minimalism}
    \label{fig:complete-my-task-minimalism}
\end{figure}

Here I give an example of multi-branch trees that are equivalent to Minimalist ones.
Since it is hard to define Merge-like fine-grained primitive operations with multiple branching,
I will present a tree in a grammar working with routinized pre-compiled constructions, 
not Merge and a list of syntactic atoms. I will talk about routinization in detail in \prettyref{sec:routine}. 

Consider, for example, the derivational process shown in \prettyref{fig:complete-my-task-minimalism}.
This seems complicated because we are only allowed to use Merge and spellout, 
but we can certainly find three routinized processes: 
the first is to assemble the $v$P structure and to build the argument structure, 
the second is to add tense and aspect information and to spell out them as auxiliary verbs
(hence forming the ``verb phrase'' in the \ac{blt} sense), 
and the third is the A-movement of the subject.
If we work with these routines instead of Merge and spellout, 
we get the dependency arcs in \prettyref{fig:complete-my-task-dep}.
Dependency relations created in the three layers are labeled by three colors.
The deep argument structure is realized in the $v$P layer in the Minimalism derivation.
The clausal non-spatial settings (this is the terminology in \ac{blt}) are created in the TP layer.
The fact that in active clauses the A argument is the pivot is realized by the Spec-$v$P to Spec-TP A-movement.%
\footnote{One question here might be that layer 2 seems to be the T' projection, 
while layer 3 seems to be the Spec-TP position and EPP feature, 
but it seems strange that a construction routinizes an intermediate projection. 
But for the sake of conciseness, 
\prettyref{fig:complete-my-task-minimalism} is \emph{not} a true cartographic syntactic tree, 
and just as is the case with \ac{cgel} where 
a projection corresponding to N' in the classical X-bar theory is actually a maximal projection, 
so is T' in \prettyref{fig:complete-my-task-minimalism}.}

\begin{figure}
    \centering 
    \input{trees/i-have-completed-the-task-dependency.tex}
    \caption{The dependency relation created when deriving \emph{I have completed my task} in \ac{blt}}
    \label{fig:complete-my-task-dep}
\end{figure}

Now the problem is how can \prettyref{fig:complete-my-task-dep} be rephrased into a constituency tree 
which is multi-branch. After some reflection, it is easy to find that 
the \ac{blt} version of tree diagram shown in \prettyref{fig:complete-my-task-blt-1} is exactly what is wanted.
Every syntactic relation in the \ac{blt} tree diagram 
traces back to a dependency relation created by Merge. 
The grammatical relation between the auxiliary verb and the past participle is shown 
by combining these two into one unit, called the \emph{verb phrase} (in the \ac{blt} sense).
This is a \emph{span} in generative terms, being the newly added elements in the TP layer,
representing the non-spatial setting of the clause -- in this case it is ``present perfect''.
The argument structure \ac{blt} tree is realized by putting agent and patient labels to the arguments.
The pivot status of the agent is shown by the A label, 
which marks the agent NP and marks its pivot status at the same time.

Readers need additional knowledge put aside 
the tree diagram in \prettyref{fig:complete-my-task-blt-1}, which, essentially, 
means we need dependency relations not shown in the tree diagram.
This is the cost of rejection of fine-grained constituent structure: 
coexistence of flat trees and dependency relations is needed.

\begin{figure}
    \centering
    \input{trees/i-have-completed-the-task-blt.tex}
    \caption{The tree diagram representation of \emph{I have completed my task} in \ac{blt}}
    \label{fig:complete-my-task-blt-1}
\end{figure}

A similar case where the loss of structural information in less fine-grained formalisms
is remended by adding additional dependency relations can be found in \ac{cgel} \citesec{5.14.1}, 
where the authors highlight the existence of \emph{indirect} complements, 
which are licensed by another dependent in the matrix phrase, 
and the dependency relation between an indirect complement and its licensor is not that 
transparent in \ac{cgel}'s phrase structure grammar.
In this case, additional dependency relations arise from rejection of invisible movements.

A further issue about the flatness of tree diagrams is how to describe the constituency order.
The flat-tree approach, which is used in \ac{blt} and also known as the functional theory of syntax,
views constituent order as a mean to mark the constituents,
in the same way case markers mark NPs.
For a SVO language like Chinese or English, for example, 
the \ac{blt} perspective is that the O argument is marked by 
the fact that it follows the predicator.
As Dixon points out, the constituent order is about surface realization and is largely arbitrary,
easily variable just like case marking strategies.
Thus, the generative papers trying to somehow ``derive'' the constituent order
seem to be all meaningless.%
\footnote{
    There is a widespread myth in functionalists that 
    generative linguistics try to demonstrate 
    all human languages have English-like underlying constituent orders.
    This myth is sometimes expressed via claims like 
    ``Chomsky is a language colonizer'' or 
    ``Chomsky tells us that all languages are European languages.''
    This myth has factual errors as well as misunderstanding of what generativism tries to do.
    The factual errors include 
    that in Minimalism, the S-structure v.s. D-structure distinction is canceled,
    and there is simply no such thing as English-like D-structures in contemporary generative syntax,
    that though some linguists assume a universal left-branching derivational order 
    or cartographic supine,
    which captures the essential idea of the old-fashioned D-structure,
    not all generative linguists insist on this universal supine \citep{wiltschko2014universal},
    and that even for those accepting the cartographic syntax completely,
    due to the Distributed Morphology-like lexical decomposition,
    the verb root may be in the rightmost position in the universal supine,
    which moves to its surface position by head movements in SVO languages,
    and hence the ``underlying constituent order'' (as functionalists call it)
    is better described as Japanese-like rather than English-like.
    In the Chomsky-as-language-colonizer logic, then,
    if a linguist does not a lexical decomposition approach, he or she is a language colonizer,
    and otherwise, he or she embraces diversity.
    This line of thinking is as absurd as it seems.
    The misunderstanding of what generativism is discussed hereafter.
}
The problem is this does not goes against the generative idea:
in generative syntax, movements which cause deviation 
from the trivial tree structure created by external Merge 
are driven by features, 
in the same way case marking is driven by features.
The movement-free template-based approach of syntax,
despite its apparent discrepancy with the contemporary generative approach,
describes a largely identical grammatical complexity class with the generative one:
``a feature attracts S to its surface position''
has nothing practically different from ``a feature is marked by inserting S to its surface position''.
What, then, does the invisible and chaotic features and derivations add to our understanding of language?
What they add to our understanding of language 
is that the variation of constituency order is \emph{not} that free:
certain antisymmetric effects can be observed,
which do not have clear functional explanation \citep{cinque2014typological}.
A right-branching Merge-based analysis extracts these observations in the notation,
and hence is helpful when doing both internal and cross-linguistic analysis.

\subsubsection{The notion of \emph{head}}\label{sec:headedness}

What is the head of a constituent is also a topic causing lots of disputation. 
Dixon argues in \ac{blt} that prepositions should not be taken as the head of a constituent,
because otherwise we will end up in ridiculously calling the case affix of a noun the head of ``case phrase''.
The point here is it is \emph{not} ridiculous.
What tears linguists apart is what they expect a head to do. 
It is often said that 
``a head is what determines the properties of a constituent''.
If this definition is to be followed strictly and in a surface-oriented manner,
then in a NP like \corpus{[big [grey bears]]},
the adjectives \corpus{grey} and \corpus{big} have to be heads:
we do not have \corpus{*[grey [big bears]]},
which means \corpus{grey} modifies a bare NP but not a NP modified by \corpus{big},
and hence \corpus{big} changes the property of the NP and has to be a head, 
and we have \corpus{[_{<} big [_{<} grey bears]]}.
% TODO: specifier-less syntax

So here we see the root cause of the disagreement between Minimalism and the \ac{blt}-and-\ac{cgel}-definition 
of head. 
The former is the bridge between a newly introduced element (i.e. the specifier) and the complement, 
while the latter is about the the overall syntactic function of the whole constituent. 
% TODO: 没说清楚

\subsubsection{Information contained in a Minimalism derivational tree}

% TODO: 语法关系，高低位置，vP-domain还是TP-domain这些
% 高的layer粗粒化给出的构式在低的layer对应的构式之后被引入，例如，先有论元关系，再有时态这些，然后再有话题化
% 这个序列和依存关系的产生、government and binding这些是完全一样的

\subsection{Deriving more surface-oriented formalisms}

\subsubsection{Minimalism as constructivism}\label{sec:routine}

Now it is time to discuss routinization in Minimalism, 
and how this reconciles Minimalism with more surface-oriented theories, 
or in Dryer's terms, ``descriptive theories'' \citep{dryer2006descriptive}.
All descriptive theories work on more % TODO: 总结常见的表示构式的方式

One reason to use pre-compiled trees is derivational theories are sometimes 
practically hard to use when doing computational researches. For example, \citet{liter2020modeling}
is about how fine-grained feature hierarchy simplifies learning, but it has to explicitly solve out what 
a derivational grammatical constraint allows, and then work with the not-so-derivational possibilities.

Another reason to keep pre-compiled trees is it makes the theory more psychologically plausible --
attempts trying to localize Merge in the brain are generally not that successful, and assuming that 
human brains do store constructions, while on the other hand these constructions are still analyzable 
in terms of Merge seems a plausible answer to what is really going on in our brains
\citep{brain-syntax-1,brain-syntax-2}. There are also acquisitional evidences supporting the claim 
that we learn words one by one and not in terms of abstract features \citep{white2022lexicalization},
but abstract features are useful anyway as one method to narrow down possible languages \citep{liter2020modeling}.

A third reason -- which, in my opinion, is the main reason that 
typologists like Croft, Bybee, etc. reject generative syntax -- 
is that when describing a poorly documented language, 
it is impossible to ``tell syntax from other factors'', 
as most generative linguists do.
Even when investigating syntax,
current mainstream Minimalism is just not handy (\prettyref{sec:generative-no-good}).
What can be segmented from raw data are \emph{always} routines,
or in other words, \term{constructions}.
This explains the name of \citet{croft2001radical}.
All field linguists, including those with generative backgrounds,
are construction grammarian in some senses.

It should be noted, however, that the typological, inductive and \term{constructivist} approach
is by no means the \emph{opposite} of the generative approach, as many may suggest.
(see, for example, discussion on distinction between 
``scientific and cumulative linguistics'' and ``formal theories'' in \ac{blt}). 
Rather, these two approaches are largely \emph{orthogonal}.
Field linguists and typologists investigate constructions,
while generative linguists investigate the \emph{grammatical complexity} of these constructions.
The depiction of generative linguists as 
people rephrasing observed linguistic phenomena in highly technical metalanguages \citep{haspelmath2021}
is therefore not that wrong, 
but it fails to see what this is all about.
What generative linguists do is to verify 
whether the newly observed constructions are in the same grammatical complexity class 
hinted by previously observed constructions.
If we can identify a stable grammatical complexity class 
which contains all possible human languages and excludes the impossible ones,
this tells a lot about the human language faculty.

What generative linguists do, then, is actually 
formalizing the intuition of a field linguist when dealing with syntactic constructions.
Or maybe semi-formalizing: strict formalizing sometimes leaves loopholes 
that allows very weird and unintended syntactic derivations,
because some implicit rules that everyone follows but never thinks of are not included \citep{whyformalize}.
Though many (anti-generativism for reasons discussed above) field linguists insist that 
languages vary in an unlimited manner, 
this is not what their reference grammars tell us.
The grammars are all written in terms of constituency and dependency relations, 
which have a rough correspondence but also some subtle mismatches 
(read: ``syntactic structures, be them constituency-based or dependency-based, 
are created by a single Merge operation, but internal Merge is possible'')
argument structures (read: $v$P), 
non-spatial settings and syntactic obligatory topic (read: TP),
syntax-pragmatics interface (read: CP), etc.
which fit perfectly in the generative framework. 
Occasionally, there are disputations like 
``the definition of a \term{verb phrase} should not include the object
so the generative binary branching is wrong'' 
(\prettyref{sec:divergent-standard-constituency-segmentation}),
but in the same section I will show this has no conflict with generative syntax.
The motto of ``describing languages without prejudice'' therefore makes more sense rhetorically:
field linguists actually do have some bias (in the machine learning meaning) when describing a new language,
but this bias is not negative and hence not really \term{prejudice}:
there is bias not because linguists support Eurocentrism, but because languages \emph{themselves} have bias.

So now the question is when these two approaches come together.
This is predictably rare.
The problem is the bias is in the \emph{implicit} knowledge passed in the typology world, 
orally from older generations to younger generations,
and subconsciously in modern reference grammars.
In the same way, the implicit knowledge about constructions and routinization
passes in the generative circle.
Both worlds work well without acknowledging explicitly the findings of ``the opposite camp'',
blocking beneficial mutual conservations.

There are several approaches to carry out a grammar in terms of routines. 
The most radical approaches are probably construction grammars.
Construction grammarians tend to think that constructions vary unboundedly.
This is quite a claim, since it is in principle possible for a construction to involve the notion of prime number,
which is not demonstrated in any natural languages.
The standard construction grammar explanation for this is 
that humans are not sensitive intuitively to abstract number theory concepts,
and the concepts do no good to communication.
But arguments in this way obviously \emph{do} set some bounds for possible human languages:
they are just about domain general principles.
Since 

A large type of grammars that are in terms of pre-compiled trees but in a less radical manner
is the class of \concept{lexicalist} grammars, where 
a word is actually a pre-compiled tree, with placeholders in its complement positions, and words 
are the \emph{only} kind of structure-building primitives. 
\ac{tag} is the most famous example.

Note that this notion of \emph{lexicalism} has subtle differences 
with \emph{lexicalism} as in \emph{lexicalist Minimalism}, 
because although the two kind of lexicalist grammars all work with words, 
not sub-word or even sub-morpheme features, 
what the former term emphasizes is the status of words as pre-compiled treelets, 
while the second term emphasizes the inward organization of words is different from syntactic rules. 
If, for example, we routinize the $v$P structure of a verb in DM
while still keeping the notion of functional projections, 
we can still get a somewhat lexicalist grammar in the former sense, 
but the existence of functional heads means the grammar is not lexicalist at all in the latter sense.
\prettyref{fig:hurt-dm-routine} is an example of a lexical entry in a lexicalist version of DM, 
and since it involves Merge of features, it is by no means lexicalist in the sense of lexicalist Minimalism.
What is emphasized by the former \term{lexicalism} is that the grammar is determined by the lexicon 
and a separate set of phrase structure rules (as in early generative theories)
is not needed.
What is emphasized by the latter \term{lexicalism} is that there is a morphological grammar engine 
strictly before syntax happens.

\begin{figure}
    \centering
    \input{trees/routine-hurt-dm.tex}
    \caption{Routinized DM $v$P of the verb \emph{hurt}}
    \label{fig:hurt-dm-routine}
\end{figure}

% TODO： 给下面的内容做一个目录

\subsubsection{Constituency grammar and labeling}\label{sec:phrase-label}

Let us start from \prettyref{fig:hurt-dm-routine}. 
It still contains many invisible functional heads 
and is not that surface-oriented and will not please a field linguist. 
One possible way to throw away these functional heads is to notice that 
the only purpose of these functional heads is to create grammatical relations \citep{hornstein2021extended}: 
all grammatical relations (which are more transparently represented as dependency relations 
-- see \prettyref{sec:minimalist-dependency}) are supposed to have something to do with Merge, 
and only Merge (not Adjoin or something else), and hence the invisible functional head hierarchy.
To throw away functional heads while keeping their functions, 
what we will get is \prettyref{fig:hurt-constituent-tree}. 
Actually, throwing away functional heads while keeping the grammatical relations they create 
is not just a procedure of routinization: 
there is already research on the possibility of so-called ``specifier-less syntax'',
in which the functional head is deemed as the label of a maximal projection, 
with the previously called specifier and complement being merged in an exocentric manner \citep{osborne2011bare}.

In \prettyref{fig:hurt-constituent-tree}, (a) is the syntactic tree in which 
we use the dependency relation between the left argument slot, the verb \emph{hurt} and the right 
argument slot to decide the constituency relations:
\begin{itemize}
    \item By the patient DP being the specifier of the Trans head, there is a Trans-DP dependency relation between them; since the Trans head is realized on the verb \emph{hurt}, there is a Trans-DP dependency relation between the patient DP and the verb. % TODO: 和equivalent formalism of Minimalism中的head moving哪些东西建立关系
    \item Similarly, there is a $v$-DP relation between the agent DP and the verb.
    \item The $v$-DP relation is built in a higher position than the Trans-DP dependency relation.
    \item Thus we get (a) in \prettyref{fig:hurt-constituent-tree}.
\end{itemize}
We can go further to eliminate the occurrence of functional heads by replacing $v$-DP dependency relation 
with predicate-agent relation, which is shown in (b). The final result, as we see, is just a \ac{cgel}-like 
tree. So the phrase structure grammar formalism in \ac{cgel} is just routinized constituent-based Minimalism.

Note that (b) in \prettyref{fig:hurt-constituent-tree} separates 
\emph{syntactic form} (i.e. the inner structure of a constituent) 
and \emph{syntactic function} (i.e. the external surrounding of the Constituent).
This traces back to the derivational tree: 
the functional layer introducing a constituent (the \emph{function} of the constituent) 
is logically independent from the layers 
that define the inner structure of the constituent (the \emph{form} of the constituent).
These two are of course still related, which is reflected by the fact that 
the patient position selects NPs and not other syntactic objects.
We will see many syntactic forms and functions in \prettyref{sec:descriptive-terms}.

\begin{figure}
    \centering
    \input{trees/routine-hurt-functional-name.tex}

    \caption{Routinized DM $v$P of the verb \emph{hurt}, with functional heads removed and the grammatical relations reserved}
    \label{fig:hurt-constituent-tree}
\end{figure}

The fact that derivational trees can be effectively routinized into pre-compiled trees 
raises the question about whether Minimalism is \emph{needed} at all.
Indeed, the framework used in \ac{cgel} is context-free \citep{pullum2008expressive}, 
and even after including the whole expressive power of Minimalism, 
what we get is just a mildly context sensitive grammar, or to be precise, 
a multiple context-free grammar \citep{clark2014introduction}.
Both CFGs and MCFGs are close to Standard Theory without movements, 
or to be frank, \emph{formalized American structuralist grammars},
and the necessity of all the fuss around Merge and its constraints is then questioned.
What is even worse for Minimalism is that certain types of CFGs and MCFGs 
seem to be easily learned via a quite domain-general learning algorithm,
and thus Minimalism may just be a good descriptive tool:
it shows the tendency of the human statistical learner,
but there is no domain-specific language faculty 
which is said to be described by Minimalism.
What we have in this circumstance is therefore a construction grammar with Minimalism flavor.

Note, however, that a typical Minimalist grammar is often mapped to a rather large MCFG, 
because of the expansion of all possible movements, feature configuration, etc. 
The fact that human languages are easier to describe in the Minimalist lens 
and not the strictly no-feature-no-movement MCFG lens may hint something about the human language faculty.
Even the actual case of human linguistic capacity is Minimalism-inspired construction grammar, 
the Minimalist flavor will still be very strong,
and what today's generative linguists do is still useful, 
even though their story about human language faculty needs adaption.

The routinized (and quite \ac{cgel}-like) constituency grammar formalism, 
especially after accompanied by the notion of complements and modifiers (\prettyref{sec:sub-cat}), 
is like the good old X-bar theory, where a lexical word 
(not a functional word, especially not an invisible functional head) heads a constituent 
with multiple complements, specifiers and adjuncts (all in generative terms). 
The main differences between the GB-like X-bar scheme and the \ac{cgel} scheme are 
that in the former terms like ``complement'' is defined strictly in structural terms 
(e.g. ``the sister of the head''), 
while in the latter these terms are defined in terms of dependency relations 
(e.g. ``complements are more explicitly licensed by the head than modifiers'') 
without any guarantee that complements are necessarily lower than modifiers \citep{payne2007fusion},
and that in the \ac{cgel} framework there is no intermediate projections: 
nominals in \ac{cgel} corresponds to N' projections in X-bar theory, 
but the former can appear as a complete unit (e.g. as the modifier of another NP). 

We can summarize that the framework in \ac{cgel} is a little more flexible than X-bar theory. 
From the Minimalism perspective, it is easy to explain why: 
the criteria \ac{cgel} uses to draw the line between complements and modifiers are related 
more to subcagegorization information that is shown in the fine-grained Minimalist tree 
but not the coarse-grained \ac{cgel} tree (\prettyref{sec:sub-cat}) 
and hints nothing about whether a constituent is a maximal projection or not. 
Purely tree structure-based notion of ``complements'' 
based on coarse-grained GB or \ac{cgel} trees 
is bond to fail. 
It is possible to define \term{complement} purely in terms of tree structure in Minimalism,
because all functional projections are present, 
and thus we can say, for example, 
that ``the specifier of the Trans projection is the O argument, etc. 
and A and O arguments are complements of a transitive verb'',
but in coarse-grained formalisms, 
these functional projections are blurred as in \prettyref{fig:hurt-constituent-tree}.
So-called intermediate projections in X-bar theory, like N', 
are actually maximal projections according to more fine-grained analysis, 
and hence they indeed can appear as a complete constituency in some occasions.

Another feature of the framework in \ac{cgel} that is more flexible than the old-fashioned
is that it displays the segmentation of different functional layers of an XP,
with X being a lexical head, 
while the old-fashioned X-bar theory does not shows the fine structure of the dependents 
between the lexical head and the specifier.
This is illustrated in \ac{cgel} \citesec{5.2} [11]:
the dependents introduced with a \emph{nominal} parent node 
belong to an inner functional domain, 
while dependents introduced with a \emph{NP} parent node 
belong to an external functional domain.
The functional domains catch the essential intuition of ``maximal projection'' in the X-bar scheme,
but if we are to implement the idea of two functional domains in terms of X-bar theory,
an asymmetry occurs: 
the lexical head (here it is the noun \term{salary}) cannot be the head of both the nominal and the NP,
so we have to find something else -- say the determiner -- as the head of the maximal projection 
corresponding to the external functional domain, and now we have both a lexical head and a functional head,
while the ways dependents are related to the lexical head and the functional head are different,
so the two heads are not ``heads'' in the same sense.
There are only two roads forward: 
one is to give up the idea of lexical heads altogether and use functional heads,
which leads to Minimalism,
while the other is to keep the notion of the lexical head 
but allow fine-grained segmentation of functional domains surrounding the head,
which leads to the \ac{cgel} -- and essentially \ac{blt} -- approach,
which is dual to the Minimalism approach (\prettyref{fig:cgel-minimalism-dual}).

\begin{figure}
    \centering
    \input{trees/cgel-minimalism.tex}
    \caption{Functional domains in \ac{cgel} and Minimalism}
    \label{fig:cgel-minimalism-dual}
\end{figure}

The X-bar theory is therefore just a transitional version of generative syntax 
between the more surface-oriented formalisms 
and the more fine-grained formalisms.

\subsubsection{Movements}\label{sec:movement-in-theory}

Whenever dependency relations that cannot be transparently reflected by a phrase structure grammar occur
(i.e. ill-nestedness occurs), movements occur \citep{boston2009dependency}. 
Let us consider the Spec-$v$P-to-Spec-TP A-movement of the subject in \prettyref{fig:coarse-grained-hurt-the-dog-badly}. We see the notion of this movement is just reflection of the fact 
that the subject both has an ``obligatory topic'' position (in the terms of typology) in the whole clause 
and an agent position in the argument structure. There are two dependency relations involving it, and 
the movement-free phrase structure grammar can only easily reflect one, and thus we need a movement 
if we want to show all dependency relations with constituency structure. Some formalisms, like HPSG,
reject the notion of movement, but introduce an ``agent'' feature in the feature structure of 
VP, which is merely movement in feature structure's disguise. 

\begin{figure}
    \centering
    \input{trees/have-hurt-the-dog.tex}
    \caption{The coarse-grained version of the Minimalist derivation of \emph{The man has hurt the dog badly}}
    \label{fig:coarse-grained-hurt-the-dog-badly}
\end{figure}

If the dual status of the subject is not reflected by movement or by feature structure, 
it either has to be reflected by something outside the tree, 
or has to be reflected by enumerating all possible construction types. 
It is easy to see that although surface-oriented grammarians are mostly more pragmatic than generative linguists, 
sticking to the surface form sometimes creates unnecessary burdens. 
Rejecting obligatory A-movements or equivalent notions totally means the linguist has to 
describe the TP structure and the argument structure always at the same time, 
which is tiresome both for the linguist and for the reader. 
But this does not mean A-movement appears in the name of \term{movement}, either:
enumerating all \emph{obligatory} grammatical categories in the TP
is still feasible,
and thus we can first describe a minimal canonical construction 
and then introduce adjunctions and transformations to non-canonical constructions.
% TODO: diagram

\subsubsection{Selection and subcagegorization, complements and modifiers}\label{sec:sub-cat}

\prettyref{fig:coarse-grained-hurt-the-dog-badly} contains an adverb, 
traditionally considered as an \emph{adjunct} in generative terms, 
which is optional and is largely invisible to multiple syntactic constraints. 
In the terms of \ac{cgel}, it is a \emph{modifier}, 
while \emph{the dog} and \emph{the man} are \emph{complements}.%
\footnote{\ac{cgel} uses the term ``adjunct'' for clausal modifiers.}
The main differences between complements and modifiers 
is that the former is licensed or even selected by the \ac{cgel} head of a constituent, 
which is also an important classification criterion 
of the lexical category of the head (known as \concept{subcategorization}),
while the latter can occur in a freer manner. 
In \ac{tag}, therefore, modifiers are introduced via adjunction, 
while complements are introduced via substitution.
This distinction has some theoretical backgrounds.
It may be assume that licensors of complements are essential for the spellout of the lexical head word
(which is the approach to subcagegorization in \citet{siddiqi2009syntax}).

Still, the distinction between complements and modifiers are opaque with 
the above argument about licensing and selection, 
and this opaqueness is again theoretically rooted. 
Languages with a relatively fixed word order often impose 
a word order constraints on clausal adjuncts and adjectives in NPs, 
which is well explained by the cartographic approach in which modifiers 
are introduced by a fixed hierarchy of functional heads. 
Tt follows that adjectives about different properties fill different ``slots of modifiers'' of the head noun, 
and similarly adjuncts about different properties fill different ``slots of modifiers'' of the head verb, 
in exactly the same way arguments fill argument slots in \prettyref{fig:hurt-constituent-tree}. 

Note that I say ``licensed \emph{or} even selected'' just before.
If a dependent has strong correlation with the head, it is a complement,
otherwise it is a modifier.
Logically, whether a dependent is required is independent of whether a dependent is a complement or a modifier.
The complement position may not be filled for some heads.
In English, for example, nouns take complements,
but these complements are rarely \emph{required} (\ac{cgel} \citesec{5.1.2}).
Obligatory dependents can be understood as generated in core projection layers,
while others can be understood as generated in optional projection layers \cite{pan2022deriving}.
This also justifies the \ac{tag} distinction between substitution and adjunction:
we may view the core projection layers as a prototypical and minimal example of the construction type in question,
and optional projection layers can be said to be adjoined into the minimal construction.
The existence of the adjunction operation means it is unnecessary
to routinize the whole Cinque hierarchy into something like \prettyref{fig:hurt-constituent-tree}.
Rather, we can first describe a minimal clause, 
and then go on to introduce possible optional dependents via adjunction. 
An obligatory dependent, however, is always considered as a complement, 
because if a dependent is obligatory, 
it is almost always the case that it has some close licensing relation with the lexical head,
though not all complements are obligatory.

Another remark here is that 
though complements are usually in lower positions in the phrase structure than modifiers,
this is not always the case.
\ac{cgel} \citesec{5.2} [11] is an example. 
If this analysis is considered controversial, 
then I have a definitely uncontroversial example: the subject.
If the $v$P layer and the TP layer are considered as forming a single construction (i.e. the nucleus clause),
and we ignore the inner Spec$v$P copy of the subject,
then the subject -- a complement -- is obviously on a higher position than the adjuncts, which are modifiers.
Whether a dependent is a complement or a modifier hints nothing about its syntactic position. 
What is -- and is not -- a complement is decided by the above listed tests, 
not by anything else.
And that is why in \ac{cgel}, though \emph{kindly} in \emph{He treats us kindly} is a complement,
it is, however, discussed together with clausal modifiers (or ``adjuncts'' in \ac{cgel} terms):
because \emph{kindly} is in the adverbial functional domain, anyway.
It makes sense to separate dependents in terms of functional domains:
this is how \ac{cgel} discusses clausal dependents, 
where \citechap{4} is about the prototypically ``complement'' domain,
but a kind of predicative -- which turns out to be adjunct -- 
is discussed in this chapter, 
and \citechap{8} is about prototypically ``adjunct'' or ``adverbial'' domain,
while manner complements, etc. are discussed in this chapter.

The conclusion that a clear distinction between complements and modifiers -- 
or in the case of clause structure, arguments and adjuncts -- 
is of limited purely formal interest and 
is better viewed as a language-specific descriptive concept \citep{haspelmath2014arguments}. 

\subsubsection{More discussion on dependents}\label{sec:dependents-types-enumerate}

Some dependents do not fit into the complement-modifier dichotomy, 
including the determiner, the classifier, etc.
Many of them are filled by functional words,
wile prototypical complements and modifiers are filled by lexical (phrasal) categories.
But there is no strict relation 
between being neither complement nor modifier 
and being filled functional words.
A determiner in English, for example, 
can be a genitive NP, 
so what fills a position that is neither complement nor modifier 
is not necessarily a functional word.
On the other hand, 
as is shown in \prettyref{chap:non-argument-complement},
certain types of non-argument complements have limited variation 
and it is reasonable to consider them as filled by functional words,
so functional words can fill complement positions. 

\subsubsection{Categories, heads and the notion of ``word''}

A \concept{category}\index{category} is defined as a type of constructions with similar distributions.
I will first discuss basic syntactic constructions and identify positions that can be filled in them, 
and then search possible constructions in these positions. This is how categories can be recognized.

A construction that is small enough is said to be a \concept{word} or more precisely, 
a \concept{grammatical word}\index{word!grammatical}.
I emphasize \emph{grammatical} because it is quite common to use the term \emph{word} 词 to denote 
a \concept{prosody word}\index{word!prosody}. Prosody is important in Chinese, which is discussed in 
\prettyref{sec:prosody-intro} and \prettyref{chap:prosody-overview}. 

In traditional grammars concerning Latin, 
a common practice is to roughly define word classes (nouns, verbs, etc.) 
according to their meaning and then discuss where they can be used. 
In this book I do not take this approach. 
Though I will review a lot of work based on the meaning-first approach, 
the way I distinguish word classes is mainly distributional. 
If two words can appear in similar positions, 
they are classified into one \concept{word class}\index{word class} or \concept{part of speech}.
A word class is just a category about words.  

In other words, I define concepts like \emph{noun-like} and \emph{verb-like} \emph{before} listing criteria of 
what is a noun and what is a verb. Criteria for word classes are always language-specific, but we have more 
confidence that at least some \emph{features} -- like the nominal feature \textit{n} or the verbal feature 
\textit{v} -- are cross-linguistic and may be attributed to the language faculty in the broad sense. 

\subsubsection{Dependency relations as in dependency grammars}\label{sec:dependency}



\subsubsection{Diverging standards of constituency revisited}

So now it is easy to see what is really going on in the flat trees in \ac{blt}: a constituent
is a large \emph{layer} in generative syntax, with grammatical relations inward being shown as 
dependency arcs instead of constituency hierarchy. 

\subsubsection{Empty categories and fusion-function constructions}

\subsubsection{Feature structures}


\section{Descriptive terms}\label{sec:descriptive-terms}

I have paid length discussions of \emph{structures} 
and demonstrated that the superfluously diverging frameworks are actually almost the same. 
But the formalism of generative syntax is probably not what dissatisfies field linguists the most.
Having worked out a \emph{formal} grammatical framework, 
a list of \emph{building blocks} are still required for fully describing a language. 
The former is purely about how structures are built, 
while the latter is about what raw materials are fed into the structure building machine. 
Universals about the former and the latter are called formal and substantial universals,respectively. 
Now readers of \ac{blt} will find 
what Dixon complains most about is not about the binary branching trees, 
but the tendency of ``formalists'' to find things equivalent to English in newly documented languages. 
In his depiction, formalists assume there are infinitives, 
inflection-derivation distinction, etc. in all languages, which he mocks several times. 
This depiction is actually not quite true for most generative works 
but is alarming for people seeking universal glossing and annotation rules.

What is discussed in this section is to find a set of grammatical relations 
as in \prettyref{fig:hurt-constituent-tree} (especially in (b), 
since most people in the descriptive community rejects 
all the functional hierarchies proposed in generative syntax).
This has already be done for English in \ac{cgel}. 
We already know some examples of these grammatical relations: 
\prettyref{sec:sub-cat}, for example, discusses complements and modifiers.
\ac{cgel} contains more grammatical relations: 
readers will find prenucleus positions in the clause, 
which are roughly specifier positions in the CP layer,
and also ``external modifiers'' in NPs. % TODO: 这些位置对应什么？ 

\subsection{Clause structure}

\subsubsection{Predicate}

We use the term \concept{predicate} to denote the constituent containing the verb complex, 
complements that are not extracted out (most frequently, the object), some adverbials, etc.
This notion is problematic for the surface analysis of a VSO language, 
where head movement analysis is unavoidable, 
or otherwise clumsily enumerating all possible clause constructions 
(i.e. ``expanding the phrase structures'') is needed
but fortunately this is not the case in Chinese. 
The predicate is roughly TP minus the layer in which the subject is introduced. 
Note that the term \emph{predicate} is about syntactic function 
and has nothing to do with its inner structure.

A predicate, if with a lexical verb, is a \concept{verb phrase (VP)}.
The concept of VP is about syntactic form, not function. 
Though a cartographic grammarian may assume a $v$P in every clause, 
but it is then possible that the $v$ head is not realized on something that looks like a verb. 
It is, then, also possible that the predicate is not a typical VP.
This is indeed the case in some languages where a NP functions as the predicate \citep[\citesec{10.1.2}]{Friesen2017}.
\ac{blt} calls this ``verbless clause''.

\subsubsection{Complement in a clause, argument or not}\label{sec:complement-argument-clause}

It should also be noted that the term \emph{argument} is not the same as the term \concept{complement}, 
although the term \emph{argument-adjunct distinction} 
is much more frequent than the \ac{cgel}-style \emph{complement-modifier distinction}. 
In \ac{blt} terms, arguments include \emph{peripheral arguments},
like locative NPs or instrumental NPs, or even PPs 
(\ac{blt} insists that PPs are actually NPs with syntactic rather than morphological case markers), 
so some arguments are modifiers with the definition in \ac{cgel}. 
Even when we limit ourselves to core arguments, 
there is still a question whether all complements can be said to be arguments. 
Nominal complements are of course arguments, 
but complement clauses are less nominal 
since across languages they usually do not have person, number, gender and case 
(or so-called \concept{$\phi$-features} in generative terms). 
Some say it is still kind of nominal, 
and the complementizer is argued by some people to implicitly carry $\phi$-features \citep{complement-clause}.
Indeed, in traditional grammars, complement clauses are often named as ``nominal clauses''.

% TODO: more disucssion
The classification of clausal dependents in typical European languages is shown in \prettyref{fig:clause-dependent-classification}.

Telling arguments from the general notion of complements is important in Chinese grammar 
because in Chinese there are obviously non-argument particles which are licensed by the verb, 
commonly called 补语 ``complement'' in Chinese School Grammars (\prettyref{sec:school-grammar}).
Now there is a terminology confusion: we have two kinds of \ac{cgel} complements:
the first covers the arguments, and the second covers the non-argument complements, i.e. 补语 (\prettyref{chap:non-argument-complement}).
What makes things even more complicated is that certain so-called non-argument complements 
look like grammatical words, which have a finite number and can be enumerated (\prettyref{sec:complement-name}).
Therefore, in the Chinese version of \prettyref{fig:clause-dependent-classification},
we need a third axis about how close a constituent is to a grammatical word.

\begin{figure}
    \centering
    \input{axes/argument-adjunct.tex}
    \caption{Classification of clause dependents in typical European languages}
    \label{fig:clause-dependent-classification}
\end{figure}


\subsection{Noun phrases}

The term \concept{nominal}\index{nominal} is used mostly as an adjective. Since Chinese does not have 
explicitly the determiner position, we do not need a separate term for NP-like phrases without a determiner.
So, despite the fact \ac{cgel} uses the term \emph{nominal} to denote constituents like \emph{red apple}, 
this book uses the term \emph{nominal} to denote anything that is noun-like.

\section{How to analyze data}

\subsection{Data source}

% 关于语料来源和分析方法

\subsection{Organization of grammar}

\end{document}