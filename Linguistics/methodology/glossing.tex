\documentclass[UTF8, a4paper, oneside, scheme=plain]{ctexart}

\usepackage{geometry}
\usepackage{titling}
\usepackage{titlesec}
\usepackage{paralist}
\usepackage{float}
\usepackage{footnote}
\usepackage{marginnote}
%\usepackage{enumerate}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{gb4e}
\noautomath
\usepackage{bbm}
\usepackage{soul}
\usepackage{graphicx}
\usepackage{siunitx}
\usepackage[table,xcdraw]{xcolor}
\usepackage{tikz}
\usepackage[ruled, vlined, linesnumbered, noend]{algorithm2e}
\usepackage{xr-hyper}
\usepackage[colorlinks]{hyperref} % linkcolor=black, anchorcolor=black, citecolor=black, filecolor=black
\usepackage[most]{tcolorbox}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage[figuresright]{rotating}
\usepackage{acro}
\usepackage[round]{natbib} 
\usepackage{nameref,zref-xr}
\zxrsetup{toltxlabel}
\zexternaldocument*[cgel-]{../English/cambridge}[cambridge.pdf]
\zexternaldocument*[chinese-]{../Chinese/main}[main.pdf]
\zexternaldocument*[zhu-]{../Chinese/reading-note-zhudexi}[reading-note-zhudexi.pdf]
\zexternaldocument*[latin-]{../Latin/latin-notes}[latin-notes.pdf]
\zexternaldocument*[alignment-]{../alignment/alignment}[alignment.pdf]
\zexternaldocument*[exercise1-]{../Exercise/2021-3}[2021-3.pdf]
\usepackage{prettyref}

\geometry{left=3.18cm,right=3.18cm,top=2.54cm,bottom=2.54cm}
\titlespacing{\paragraph}{0pt}{1pt}{10pt}[20pt]
\setlength{\droptitle}{-5em}

\DeclareMathOperator{\timeorder}{\mathcal{T}}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\legpoly}{P}
\DeclareMathOperator{\primevalue}{P}
\DeclareMathOperator{\sgn}{sgn}
\newcommand*{\ii}{\mathrm{i}}
\newcommand*{\ee}{\mathrm{e}}
\newcommand*{\const}{\mathrm{const}}
\newcommand*{\suchthat}{\quad \text{s.t.} \quad}
\newcommand*{\argmin}{\arg\min}
\newcommand*{\argmax}{\arg\max}
\newcommand*{\normalorder}[1]{: #1 :}
\newcommand*{\pair}[1]{\langle #1 \rangle}
\newcommand*{\fd}[1]{\mathcal{D} #1}

\newcommand*{\citesec}[1]{\S~{#1}}
\newcommand*{\citechap}[1]{chap.~{#1}}
\newcommand*{\citefig}[1]{Fig.~{#1}}
\newcommand*{\citetable}[1]{Table~{#1}}
\newcommand*{\citefootnote}[1]{footnote~{#1}}
\newcommand*{\citepage}[1]{pp.~{#1}}

\newrefformat{sec}{\citesec{\ref{#1}}}
\newrefformat{fig}{\citefig{\ref{#1}}}
\newrefformat{tbl}{\citetable{\ref{#1}}}
\newrefformat{chap}{\citechap{\ref{#1}}}
\newrefformat{infobox}{Box~\ref{#1}}
\newrefformat{fn}{\citefootnote{\ref{#1}}}

\usetikzlibrary{arrows,shapes,positioning}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{decorations.markings}
\tikzstyle arrowstyle=[scale=1]
\tikzstyle directed=[postaction={decorate,decoration={markings,
    mark=at position .5 with {\arrow[arrowstyle]{stealth}}}}]
\tikzstyle ray=[directed, thick]
\tikzstyle dot=[anchor=base,fill,circle,inner sep=1pt]


\tcbuselibrary{skins, breakable, theorems}

\newtcbtheorem[number within=section]{infobox}{Box}%
  {colback=blue!5,colframe=blue!65,fonttitle=\bfseries, breakable}{infobox}

\newcommand*{\concept}[1]{\textbf{#1}}
\newcommand*{\term}[1]{\emph{#1}}
\newcommand*{\corpus}[1]{\emph{#1}}
\newcommand*{\translate}[1]{`#1'}

\newcommand*{\vP}{\textit{v}P}

\DeclareAcronym{blt}{short = BLT, long = Basic Linguistic Theory}
\DeclareAcronym{cgel}{short = CGEL, long = The Cambridge Grammar of the English Language}
\DeclareAcronym{dm}{short = DM, long = Distributed Morphology}
\DeclareAcronym{tag}{long = Tree-adjoining grammar, short = TAG}
\DeclareAcronym{sfp}{long = sentence final particle, short = SFP}
\DeclareAcronym{vp}{long = verb phrase, short = VP}
\DeclareAcronym{np}{long = noun phrase, short = NP}
\DeclareAcronym{adjp}{long = adjective phrase, short = AdjP}
\DeclareAcronym{advp}{long = adverb phrase, short = AdvP}
\DeclareAcronym{pp}{long = preposition phrase, short = PP}
\DeclareAcronym{cls}{long = classifier, short = CLS}
\DeclareAcronym{dist}{long = distal, short = DIST}
\DeclareAcronym{prox}{long = proximate, short = PROX}
\DeclareAcronym{dem}{long = demonstrative, short = DEM}
\DeclareAcronym{dur}{long = durative, short = DUR}
\DeclareAcronym{neg}{long = negative, short = NEG}
\DeclareAcronym{tam}{long = {Tense, Aspect, Mood}, short = TAM}
\DeclareAcronym{tame}{long = {Tense, Aspect, Mood, Evidentiality}, short = TAME}
\DeclareAcronym{pie}{long = Proto-Indo-European, short = PIE}

% Disable unsupported commands in bookmark titles 
\pdfstringdefDisableCommands{%
  \def\\{}%
  \def\texttt#1{<#1>}%
  \def\mathbb#1{#1}%
}
\pdfstringdefDisableCommands{\def\eqref#1{(\ref{#1})}}

\makeatletter
\pdfstringdefDisableCommands{\let\HyPsd@CatcodeWarning\@gobble}
\makeatother

\newcommand{\cgel}{\href{../English/cambridge.pdf}{my notes about CGEL}}
\newcommand{\chinesenote}{\href{../Chinese/main.pdf}{my notes about Chinese syntax}}
\newcommand{\zhudexi}{\href{../Chinese/reading-note-zhudexi.pdf}{my notes about Zhu's book}}
\newcommand{\latin}{\href{../Latin/latin-notes.pdf}{my notes about Latin}}
\newcommand{\alignment}{\href{../alignment/alignment.pdf}{my notes about alignment}}
\newcommand{\exerciseone}{\href{../Exercise/2021-3.pdf}{this exercise}}

\newcommand{\ala}{Ã  la}
\newcommand{\classify}[1]{{\sc #1}}

\title{Glossing of descriptive terms, and how to read a grammar}
\author{Jinyuan Wu}

\begin{document}

\maketitle

TODO list:
\begin{itemize}
    \item unaccusative/unergative
    \item Details about aspect 
    \item Semantic classification of verbs and argument structures
\end{itemize}

\section{Theoretical orientation}\label{sec:theory}

\subsection{The surface-based approach}

\subsubsection{Frameworks}

The contemporary generative syntax is usually in a lexical-decomposition manner:
that is, sub-word and even sub-morpheme features are the basic units of morphosyntactic operations.
The features are fed into Merge to construct a binary tree with possible multi-domains
(or a binary tree with chains recording copying or Internal Merge),
and then the tree is linearized and post-syntactic operations are applied 
so features are assembled together to form words.
This approach is not appropriate for language description,
the latter requiring a more surface-oriented approach.

There have already been several relatively stable descriptive frameworks.
Modern descriptive grammars of underdocumented languages 
are usually carried out within the framework of \ac{blt},
which is theorized in \citet{dixon2009basic1,dixon2010basic2,dixon2012basic3},
and, according to Dixon, deviates striking from the bond-to-fail generative approach.
The \ac{blt} approach assumes no fine-grained constituency hierarchy,
represents dependency relations not by constituency structure,
and views constituent order as mostly driven by pragmatic reasons.
In a word, \ac{blt} is the lay version of functional syntax \ala{} Simon Dik.

On the other hand, already well-known languages are often described in the framework proposed in 
\ac{cgel} \citep{cgel}.
This framework is also taken in \citet{abeille2021grande,huang2016reference},
and its varieties are used in \citet{demonte1999gramatica1,munoz2000gramatica2,bosque1999gramatica3}.
The \ac{cgel} approach is more informed by generative syntax:
it maintains a largely binary-branching analysis,
and dependency relations are still highly bounded to the surface-oriented constituency tree,
though certain dependency relations that involve movements that are hard to find by looking at the surface form 
require special treatment:
and thus the subject in a clause is recognized as a complement of the verb,
but since it is outside the \ac{vp} and the A-movement to SpecTP is not recognizable
in a surface-oriented,
a new syntactic function label \term{external complement} 
-- essentially a way to represent a dependency relation --
is invented to cover the relation between the subject and the verb.
In a word, the \ac{cgel} approach
is the surface-oriented version of the mutual consensus of Minimalism (and GB) and HPSG.

Though the \ac{blt} approach, and the contemporary generative approach, or the Minimalist one,
differ in several important aspects,
the two frameworks roughly describe the same grammatical complexity class:
I will show this in the rest of this note. 
For abstract discussion, see \citechap{\ref{chinese-chap:theory}} in \chinesenote. 
The take-home message is
\ac{cgel} may be viewed as the surface-oriented dual theory of the constituency structure of Minimalism,
while \ac{blt} may be viewed as the surface-oriented dual theory of the dependency-based formalism of Minimalism.
The relation between the three is visualized in \prettyref{fig:three-formalism}.

In the literature, the constituency-based approach is frequently called the \emph{structuralist} one,
while the dependency-based approach is frequent called the \emph{functionalist} one.
The latter doesn't necessarily look like more theoretical `functionalist' approaches,
like the Systematic Functional Grammar.
We see both extremes aren't suitable for surface-oriented analysis:
the surface-oriented constituent tree is not enough
and additional function labels like \term{subject} and \term{internal complement}
are required,
while the surface-oriented dependency tree 
can't reveal what commonly attributed to constituency relations,
like extraction properties.
The \ac{cgel} approach is closer to the so-called structuralist approach,\footnote{
    Actually, there is no wide gap between the old-fashioned structuralist approach 
    and the modern \ac{cgel} approach:
    if we look closer to structuralist grammars in the age of Bloomfield,
    e.g. \citet{chao1965grammar},
    we will still find primary function labels like \term{subject} and \term{predicate}.
    The main differences include function labels are sometimes mixed with category labels i.e. form labels,
    for example in terms like \term{verb-object construction},
    and argumentation in support of function labels (see \prettyref{sec:why-argumentation-function-label})
    is often in lieu.
    But using category labels in place of function labels sometimes is acceptable 
    (\prettyref{sec:form-based-function-name}),
    and argumentation may be omitted in the grammar (\prettyref{sec:argumentation-amount}),
    so the differences are merely about writing styles rather than theoretical divergence.
}
and the \ac{blt} approach is closer to the so-called functionalist approach.
We know there is no substantial divergence between the two.
Indeed, people often say 
``modern typology is based on functionalist syntax''
and ``descriptive grammars used in typology are strongly influenced by structuralism \ala{} Bloomfield''
at the same time,
and as we see, both claims are correct.

Note that it is frequent 
for comprehensive grammars about well-known languages to be carried out in term of \ac{blt}.
Examples include \cite{batchelor2010reference,batchelor2011reference}.
This is possibly because these languages fit in the framework of traditional grammar well,
and since \ac{blt} may be regarded as ``traditional grammar informed by theories'',
that certain grammars of well-documented languages are \ac{blt}-like is expected.

\begin{infobox}{\ac{blt} in the generative community}{blt-generative-people}
    Indeed, contrary to the prevalent idea in the so-called functional-typological community,
    there \emph{are} generative linguists participating in language documentation and description,
    though in the generative community, 
    the \ac{blt} approach is usually said to be simply \emph{descriptive},
    in contrast with \emph{theoretical} works, i.e. generative works.
    Here is a list of some linguists participating in 
    both descriptive and theoretical enterprises (as they call them)
    and their descriptive works:
    \begin{itemize}
        \item David Adger: \citet{adger1997vso,adger2006dialect,harbour2012information}.
        \item Chris Collins: \citet{collins2014plural,miller2007sounds}, 
        as well as videos with subtitles that can be found with a Google search.
        \item Daniel Harbour: \citet{watkins2010linguistic}, and his collaboration with David Adger.
        \item Roberta D'Alessandro: \citet{andriani2022documenting,frasson2021subject}.
    \end{itemize}
    There is no qualitative boundary between their ``descriptive'' and ``theoretical'' works:
    the descriptive part reveal grammatical categories, dependency relations,
    and constraints on constituent order demonstrating functional domains,
    and all these things are put together in the theoretical i.e. generative part.
    Their workflow differs from with functional-typologists only in 
    how they deal with the output of the \ac{blt} part:
    the generative approach shows the construction in question is possible 
    in the sense that it can be derived with processes observed before,
    while the typological approach measures how probable it is 
    by comparing with other languages.
    These two approaches are in fact complementary and not contradictory.
\end{infobox}

\begin{figure}
    \centering
    \input{line-box/three-formalisms.tex}
    \caption{The generative formalism and two surface-oriented formalisms used in language description.
    The hierarchy information of the Minimalist formalism lost when deriving the surface-oriented formalisms 
    is remended by grammatical functions 
    like \term{indirect complement} and \term{function fusion} in the \ac{cgel} formalism,
    and by grammatical dependency relations like ``slot 1 is the syntactic topic''
    and ``the dependency relation between the lexical head and the F1 slot is coded by the F1 marker'' 
    in the \ac{blt} formalism.}
    \label{fig:three-formalism}
\end{figure}

\subsubsection{Some caveats}\label{sec:caveats}

Before going on, I need to list some caveats.
The first is many terms in this note may be confusing in the context of generative syntax:
I use the term \term{lexicon} in a more traditional way (\prettyref{sec:lexicon-content}),
and use the term \term{semantic role} in place of \term{thematic role}.
Sometimes, it's said that language description would better be carried out in a semantic-first approach
\citep{dixon2005semantic}.
Here the notion of \term{semantics} is also not the same as 
semantics in formal semantics or cognitive grammar:
it means things like the tendency between the semantic classification of verbs and its argument structure.
Indeed, in the light verb theory in generative syntax,
certain aspects of semantics are of course involved.

The second is about \concept{gradience}.
Generative linguists usually assume the I-language is discrete,
while functionalists insist that they are by nature gradient.
However, gradience in cognitive research is different from gradience in language description:
for a population in which each person has a discrete (but not necessarily identical) grammar,
then a sketch of the grammar of the group is gradient as an effective theory.
And if the grammar of each person is gradient,
discrete models -- like those used in historical linguistics --
are still quite useful.
Therefore, in this note I will talk about ``one construction is somehow between the two'',
even if this makes no sense in the corresponding theory.
I may talk about a word being both a lexical word or a function word,
though in Distributed Morphology,
the first originates from the root of a functional hierarchy,
and the first originates from span spellout of a functional hierarchy.
The two mechanisms are in conflict with each other,
but it's obviously possible for a person to have competing grammars in 
his or her mind,
and for a population to have competing -- but still largely mutually understandable -- grammars.
The gradient description is therefore acceptable as a first approximation.

The third is language description itself doesn't make claims on the cognitive nature of languages
-- but the theory framework used may be of cognitive significance.
\ac{tag}, for example, has been used in psycholinguistic researches.
But when a constituency tree-based approach or a dependency tree-based approach 
is picked up,
the linguist usually doesn't think too much about its cognitive foundation,
though there are almost certain aspects in which it does hint something about cognition.

A final remark is the term \term{language description} itself.
Some people use the term \term{language documentation} to denote 
the activity to gather as much as original data 
(they should be roughly annotated, 
but the most important point is to record data as they are,
avoiding never details).
The term \term{language description} means the activity happening after language documentation,
i.e. surface-oriented analysis, grammar writing, and typological research.
This note is mainly about the latter, not the former.
Accumulating texts requires 

\subsubsection{About semantics and the function-oriented analysis}

A semantic concept may have one of the following statuses in the morphosyntax:
\begin{itemize}
    \item Morphological or syntactic (by constituent order and/or auxiliaries) marking.
    \item No explicit marking, but active in some morphosyntactic tests.
    \item Purely semantic, ``simulated'' by other constructions in the morphosyntax.
\end{itemize}

The boundaries between these cases are blurred.
Differences in morphosyntactic tests may be view as a specific morphosyntactic marking strategy,
a ``zero-realization'' of constructions in question,
so there is no clear boundary between the first and the second.
Morphosyntactic tests always require a certain amount of semantic information,
and if a construction ``may be stably interpreted into'' several meanings,
maybe it's not a single construction but a group of constructions with the same appearance.
So it's also hard to draw a clear line between the second and the third.
As for periphrastic marking, sometimes it's hard to say 
whether something is a part of the grammar or from the lexicon:
In syntactic cartography,
it's sometimes said that \corpus{allegedly} is an evidentiality marker.
It occupies roughly the same position in the syntactic tree with a ``real'' evidentiality category,
and carries the same meaning,
and when we are doing morphosyntactic tests of adverbs,
it's found that the position of \corpus{allegedly} can't 
be easily replaced by a large group of adverbs.
But still, \corpus{allegedly} just looks too much like a normal adverb with nothing special.
So is \corpus{allegedly} a periphrastic marking of evidentiality,
or is it just a way to mimic evidentiality using an ordinary adverbial modification?
The answer may be different in different historical stages or even with different groups of speakers.

Another parameter is how dependent is this concept to the lexicon.
Some verbs simply don't work with the punctual temporal extent;
some verbs never appear in any perfect construction.
Note that this parameter is independent of the parameter of morphosyntactic status:
a verb that only appears in the plain aspect 
can still receive morphosyntactic marking for that,
while it's also possible that the aspectual information is implied purely from the verb stem 
without any morphosyntactic marking strategy.
Whether a category is compatible only with a limited number of lexical items,
in morphology, is a factor in deciding whether a process is 
already historical or still has synchronic values,
which some authors use as a criterion of the derivation-inflection distinction
(\prettyref{sec:history-or-current-morphology}).

Often, typologists use a otherwise morphosyntactic term to refer to a semantic concept.
The term \term{causative}, for example, 
is widely used for all constructions with the same meaning of ``make something to happen''.
The so-called causative constructions are highly heterogeneous.
A ``lexical causative'' -- the correspondence between \corpus{rise} and \corpus{raise},
or \corpus{die} and \corpus{kill} -- 
may have morphological marking and depend on the lexeme (in the \corpus{rise}/\corpus{raise} case)
or have no morphosyntactic status and depend on the lexeme (in the \corpus{die}/\corpus{kill} case).
A ``analytic causative'' 
may be a periphrastic construction, 
or a complement clause construction like \corpus{make sb. to do sth.},
both of which don't depend on the verb describing the caused action.

\subsection{Questions to ask about the underlying theory}

Beside the discrepancy between the \ac{blt} and the \ac{cgel} approaches,
there are still further divergences within the \ac{blt} approach.
So here I list some dimensions of divergence.

\subsubsection{Pre-compiled phrasal templates}

Is the theory purely lexicalist, 
or are there syntactic templates?%
\footnote{
    Sometimes the term \term{lexicalist} means the syntax works on words 
    and not sub-word units.
    This is not the meaning intended here. 
    The meaning intended here by \term{lexicalist} is 
    ``all grammatical rules can be reduced to how to use certain lexicon entries (lexical or functional)'',
    which may be words or morphemes or features.
    In other words, a lexicalist theory has no or few ``global'' phrase structure rules,
    as opposed to early generative grammars.
    This usage of the term \term{lexicalist} is attested in \citet{matchin2020cortical}. 
}\label{fn:lexicalist-1}
Though in a quick glance, it seems the lexicalist approach agrees with the Minimalist syntax 
while the templatic approach agrees with the constructionism,
things aren't that simple:
remember, a Minimalist syntax runs on features which aren't directly visible,
and words and morphemes are just quirky reflections of them.
The corresponding surface-oriented version of a Minimalist syntax with lots of features 
that are used to guide the syntactic derivation (e.g. the EPP feature), then,
inevitably contains syntactic templates that are hard to place under any lexicon entry.
The Cinque hierarchy of clause structure, for example, contains tons of invisible functional heads,
and once we ``integrate out'' these functional heads,
the resulting grammar has a clause template.
The linguist has to consider whether to introduce 
a chapter named ``the structure of noun phrases''
or a chapter named ``the clausal structure''.

\subsubsection{The morphology model}

How is morphology dealt?
This parameter has strong association with the previous parameter, 
since there is no clear distinction between a morpheme and a word.
In morphology the lexicalist extreme is the Item-and-Arrangement approach,
while the templatic extreme is the Word-and-Paradigm approach.
The Item-and-Process approach is somehow in the middle, 
maybe in a position closer to the former and further from the latter. 
What brings in more complexity in morphology is 
there are post-syntactic operations:
even when the features do spellout into morphemes,
the Distributed Morphology-style post-syntactic operations 
blur the correspondence between features and morphemes,
and hence the idea that words are built up by morphemes 
doesn't lead to any constraints on the form of the word,
raising doubts about whether in a surface-oriented analysis,
morphemes are of any theoretical significance at all \citep{anderson2017words}.
The linguist needs to pick up a specific way to show how words are built up.

\subsubsection{Where to find grammatical relations}

How are grammatical relations (in other words, dependency relations) introduced? 
Together with morphemes that bear them, or words, or constituents, or with separate chapters and sections?
This parameter has certain correlation with the top-down/bottom-up parameter,
because in a top-down analysis,
the grammatical functions of constituents in a larger construction 
are obviously introduced before what fill the constituent slots are discussed.
On the other hand, 
a bottom-up grammar tends to introduce grammatical relations when discussing the smallest unit that bear them,
for example talking about the case marking of various complements in the noun morphology chapter.

\subsubsection{Notion of head, complement and modifier}\label{sec:what-is-head}

What is the relation between a phrase and words contained in it?
What is the head? What are the complements? What are the modifiers?
In Minimalist syntax, all functional categories serve as heads,
but lexical categories are never heads.
This may appear strange but has underlying consistency 
(see \citesec{\ref{chinese-sec:headedness}} in \chinesenote).
This approach, however, is not acceptable for a surface-oriented grammar,
and here another concept -- what determines the ``overall'' property of a constituent -- 
is accepted as the standard to decide what is the head. 
Thus a \term{n}P and a DP are all headed by the central noun in the surface-oriented analysis,
because both of them are built surrounding the core noun stem,
and since the core noun stem is phonetically realized as the central noun -- a lexical word --
the latter is recognized as the head.
Disagreements then arise when whether a word is functional or lexical is not that certain.
Should the preposition be considered as a head? 
The preposition in a peripheral argument may be seen as the marker of a syntactic case system 
(so in the generative analysis, we have PP and CaseP),
and under this analysis, the preposition is not a head.
But in many languages like English, 
the preposition category has certain predicative properties,
making it appear like a lexical category, 
and then it seems an \ac{np} with a preposition is no longer an \ac{np} -- 
it is a \ac{pp} headed by the preposition.

\subsubsection{Constituent levels}

Are there fine-grained constituency structures, or are there just \acl{np}s and clauses?
Some grammars, like the \ac{cgel}, 
posits an anatomy of \ac{np}s with the following functional domains:
head noun -- nominal -- minimal \ac{np} with a determiner -- external modifiers.
Others just list possible \ac{np} dependents or clausal dependents,
without discussing which is closer to the head. 
If the latter approach is taken,
the linguist has to introduce effects due to the relative position of constituents in another way,
like ``the O argument in ergative languages is more topic-like''.
The main reason to take the latter approach -- which is the approach advocated in \ac{blt} --
is only \ac{np}s and clauses have complete semantic significance.
See \ac{blt} \citesec{1.11}, (33) and (34):
Dixon doesn't like the binary-branching (Minimalist) approach (33),
because it doesn't illustrate the fact that the function words are different from lexical ones. 
But this is more a problem of terminology:
the term \term{phrase} in \ac{blt} corresponds to a maximal domain like DP or CP in generative syntax,
while a generative \term{phrase} -- like \term{v}P or AdvMannerP -- 
corresponds to a grammatical construction in \ac{blt}.

\subsubsection{The syntax-morphology distinction}

Whether to adopt a clear distinction between syntax and morphology.%
\footnote{
    This parameter involves the term \term{lexicalist} in another sense (compare \prettyref{fn:lexicalist-1}).
    If there is a clear distinction between syntax and morphology,
    i.e. there is a stable notion of \term{word},
    then the theory is \term{lexicalist}.
}
From a Distributed Morphology perspective,
the only distinction between what is traditionally regarded as syntax 
and what is traditionally regarded as morphology 
is that the latter involves more post-syntactic operations.
Once these operations are undone,
\ac{cgel} and \ac{blt} analysis for syntax is applicable to the morphemes
(or grammatical categories in the Word-and-Paradigm approach):
the lexical stem of a word may be regarded as with no definite category,
and hence lexical category labels are in essence function labels (\prettyref{fig:noun-function}).

\subsubsection{Constituent order}

How is constituent order (often called \term{word order}) introduced?
Is there a separate chapter devoted to constituent order?
Constituent order can be understood as a manifestation of constituent hierarchy,
while in more functionalist approaches, 
it is understood as a method parallel to morphological marking that 
marks the constituent positions in a larger construction.
Note that the second claim doesn't go against the first one:
certain features, e.g. EPP, 
are indeed reflected by the surface constituent order in generative syntax.
Languages may also have macroparameters directing how to linearize generative syntactic trees,
which doesn't involve features (which are syntactic objects) 
and are rules on the interface between PF and the syntax proper.
Indeed, there are linguists claiming that constituent order is merely morphology 
and should not be taken too seriously when discussing the syntax proper.%
\footnote{
    In \ac{blt}, Dixon says constituency order is a way 
    to mark constituents' function label and is of secondary interest.
    He contrasts this with the approach taken by ``formalists'',
    who allegedly want to use an English-informed framework to catch all things 
    of a largely unknown language.
    The point here is Dixon's approach \emph{is} in fact 
    the approach taken by formalists in the real world. 
}
Certain mechanisms that do not involve features (e.g. Antisymmetry)
can't be translated transparently back into the second approach, though,
but they can be framed in the second approach as 
``the human language faculty just rejects certain constituent orders anyway''.%
\footnote{
    One controversy here is the generative feature-driven constituent order often involves movement,
    while functionalists accept constituent order variations ``as they are''.
    This controversy is false, because for many generative linguists, 
    movements can be unmarked, and what movement means is simply 
    dual syntactic function of a constituent 
    or the imperfect relation between constituent order and dependency relations
    (e.g. cross-serial dependencies).
}

\subsubsection{The notion of canonical constructions and transformation rules}

Whether a set of canonical constructions is established.
Viewing non-canonical constructions as transformed from canonical ones (or by adjunction, etc.) 
is a powerful descriptive tool,
but it is often the case that certain constructions 
that are uncontroversially deemed non-canonical do not have a canonical counterpart.
This is one of the reason transformational rules are finally abandoned in generative syntax.
Transformational rules (and adjunction, etc.) are still handy when doing description, though:
no one wants to read a grammar that treats positive clauses and negative clause in the same way.

\begin{figure}
    \centering
    \input{line-box/noun-function.tex}
    \caption{The lexical category \term{noun} as a function label:
    in a generative grammar, a nP (nominalizer phrase) is placed in a functional projection FP,
    and a noun stem is placed as the complement of the nominalizer.
    After coarse-graining the generative syntax tree 
    and using function labels to replace functional heads,
    the F functional head in the FP projection is replaced by the F label on the right side.
    Then, by the same logic, 
    the nominalizer should be coarse-grained into a ``Noun-stem'' function label,
    and the nP projection is coarse-grained into the ``Noun'' category label.
    In practice, the orange box on the right side
    is contracted into one single word, i.e. the noun,
    and thus we can see the category label of a word also has a hidden function label inside.}
    \label{fig:noun-function}
\end{figure}

Both \ac{blt} and \ac{cgel} 
have strong tendencies in the values of some (though not all) parameters listed above.
The parameters that are largely fixed by the overall descriptive framework 
include whether fine-grained constituency hierarchies appear,
how is constituent order introduced,
and the definition of \term{head}.
It is of course possible to mix the \ac{blt} and the \ac{cgel} approaches:
\citet{Friesen2017}, for example, despite being a largely \ac{blt}-like grammar,
uses the term \term{verb phrase} in the way of \ac{cgel},
while the parts about \ac{np}s are typically \ac{blt}-like.
On the other hand, \citet{munoz2000gramatica2}, 
despite it employment of \ac{cgel}-like terms,
doesn't emphasizes on tree diagrams and 
constituency structure, nor its possible constraints on the constituent order,
and is much more \ac{blt}-like than the \ac{cgel} itself or the great French grammar \citep{abeille2021grande}.
Another example of mixing of the \ac{cgel} approach and the \ac{blt} approach
can be found in \ac{cgel} itself:
ditransitive clauses can be analyzed in Minimalism,
but the analysis inevitably involves lots of movements
to make the dependency relations correct,
so \ac{cgel} just gives up binary branching in exchange of a much clearer surface-oriented analysis,
where the syntactic properties of the direct and indirect objects 
(two grammatical functions not found in monotransitive clauses)
are manually stipulated.
This is just how \ac{blt} works when the constituency relations are complicated.

Choosing between the approaches in \ac{cgel} and \ac{blt} 
and the rest of the parameters are to be decided by the grammarian,
and some best practices are introduced in \prettyref{sec:best-practice}.
In principle, the above parameters are free to choose.
In practice, they have to be fine-tuned or otherwise the grammar will be hard to read.

\section{Common practices of grammar writing}\label{sec:best-practice}

\subsection{What influences the organization of the grammar}\label{sec:grammar-factors}

Parameters in \prettyref{sec:theory} have to be fine-tuned according to the following factors:
\begin{itemize}
    \item The properties of the language. 
    If a linguist unfortunately decides to write a Chinese grammar 
    in a bottom-up manner
    in which a grammatical relation is introduced in the chapter about the lexical category about its head,
    a reader will soon be stuck in questions like 
    what are the possible linear order between object(s), directional complement, and aspectual markers.
    On the other hand, it is okay -- and even desirable to write a Latin grammar in this way,
    because Latin is much more free-order than Chinese 
    and grammatical relations are mostly marked by morphology.
    \item Whether the language is already well-known. 
    In \prettyref{sec:theory} we have already seen that
    comprehensive grammars of well-known European languages are usually \ac{cgel}-like,
    while newly documented languages are usually captured by \ac{blt}-like grammars.
    This is again expected, because in-depth partition of constituents 
    is never the priority when describing a newly encountered language.
\end{itemize}

% TODO: a table about famous grammars

It should be noticed that what is in the final version of the grammar 
has no absolute relation to what is done to reveal the grammar.
A chapter in a grammar about verb morphology 
may list all attested \ac{tame} categories marked on the verb,
and then show morphemes corresponding to these categories,
while the workflow to investigate the paradigm 
is likely to be carried out in reverse order:
grammarians often first record all attested forms of a verb,
and then try to find the morphemes,
and finally link them to \ac{tame} categories.
The morphemes attested, actually, 
are the strongest evidences for or against certain analyses of 
what \ac{tame} categories are coded.
This explains why many authors love the bottom-up approach.
But again, they inevitably need some top-down partition:
at least they have to be able to identify \ac{np}s and clauses!
These scaffolds are removed once the building of the grammar is finished.
They may still leave traces in the finished work:
This is the topic I am going to discuss in \prettyref{sec:argumentation}.

\subsection{Introductory chapters}

\subsubsection{The introduction chapter}\label{sec:introduction-chapter}

In modern grammars, the first chapter of a grammar is usually about 
its genetic affiliation, its population, 
geographic and cultural information,
typological significance, and similar topics.
Grammars focusing on raw fieldwork data 
may also list available texts and corpora.

\subsubsection{The grammar sketch}\label{sec:grammar-sketch}

It is common -- but not necessary, counterexamples including \citet{Grimm2021} 
-- to have a grammar sketch chapter then.
This brings some (though not all, for reasons below) benefits of the top-down approach
to a grammar that is not carried out strictly in the top-down manner,
and thus makes it easier both for readers to understand the language 
and for the author to describe what he or she knows.
In many grammars, though, the grammar sketch chapter is just 
the zipped version of the following chapters organized in a bottom-up manner
instead of something like \prettyref{sec:top-down},
and readers in a hurry 
-- possibly readers who want to know what syntactic categories are marked in the clause,
or readers reading a grammar to give a construction a generative analysis  --
are unlikely to find what they want in the chapter without reading it from the beginning to the end.
They should not expect what they need is fully summarized in a single section.

Take \citet{jacques2021grammar} as an example.
The morphosyntactic part of \citechap{2} is thoroughly bottom-up,
with attested lexical categories being introduced first,
then nominal morphology,
then verbal morphology,
then argument indexation and flagging,
and finally constituent order and subordination.
Readers need to dive deep into \citesec{2.4.3.2} to see what \ac{tame} categories are marked in the clause
-- since they are marked on the verb complex 
and the author writes the grammar in a bottom-up way,
introducing grammatical categories in the sections about smallest constructions 
that manifest them,
a comprehensive list of \ac{tame} categories can only be found 
in a small corner in the section about verbal morphology.

There is one particular benefit of having a grammar sketch chapter.
A grammar needs argumentation to support a function label.
When the author says ``the subject is available for more extractions than internal complements'',
the readers are expected to know where these movements happen,
which, unfortunately,
are usually introduced after the concept \term{subject} appears,
in the chapter about information structure.
Having a grammar sketch means non-canonical constructions 
can be used to support a particular analysis of a canonical construction.

Another benefit of the grammar sketch chapter is necessary top-down information can be collected there.
If you really want, you can first enumerate all grammatical categories in a clause 
and then introduce each of them.
In practice,
finding all grammatical categories can be done by looking at the name of each section,
so this probably is not necessary.

\subsubsection{Phonetics and phonology}

The following chapter(s), if any, is usually about phonetics and phonology,
and possibly about the preferred writing system.
This chapter may be omitted for languages already well known, like English.
There is no phonology chapter in \ac{cgel}, for example.
This note is not about phonology, so I will just skip this part.
It should be noted that readers interested in a largely unknown language 
would better know something about the phonology,
even if their primary interest is about morphosyntax,
because phonological rules can blur otherwise clear constructions.

What happens next observes more variations.
In the following I discuss frequent strategies to arrange chapters about morphosyntax.
These strategies aren't mutually contradictory:
good grammars mix them to meet the need in \prettyref{sec:grammar-factors}.

\subsection{Order of chapters}

\subsubsection{Top-down or bottom-up}

Top-down (i.e. structuralist partition-based), 
or bottom-up (i.e. based on the usage of smaller units)? 
In PSGs there is a clear correspondence between the two, 
but for actual language description things are often complicated:
a top-down grammar is awkward to write 
because the author has to enumerate all possible configurations in a construction 
to fully characterize it
(``a clause is either coordination of clauses or a subject-predicate construction''
-- oh no, supplementation and pre-nucleus constructions are forgotten),
while a bottom-up grammar is awkward to read 
because the reader has to infer all possible configurations in a construction 
(``the verb is the prototypical content of the predicate slot''
-- any other possibilities? Nobody knows).
This parameter is in principle orthogonal to the parameter about how constituent order is introduced,
but a bottom-up grammar without a chapter (or several chapters) devoted to constituent order 
will be extremely hard to read:
the reader may find a sentence like ``the object follows the verb'' in the chapter about verbs.
Alright, can an adverb intervenes between the verb and the object? No answer.

A bottom-up grammar is about item and arrangement.
A top-down grammar is about anatomy.
It is possible to mix the two approach.
For example, there may first be several chapters about morphology,
and then syntax is discussed,
so the morphology-to-syntax narrative order is bottom-up,
but in the syntax part,
first the subject-predicate relation is discussed 
and then the inner structure of the predicate,
so the inner structure of the syntax part is top-down.

\subsubsection{The \ac{np}-clause division}\label{sec:np-clause-chapter}

Some grammars can be divided into two parts:
one is about \ac{np}s,
the other clauses.
Since \ac{blt} tolerates using names of lexical categories in place of the naming of their prototypical functions,
sometimes we say these grammars are divided into 
a part about nouns and a part about verbs.
Other grammars mix the noun chapters and the verb chapters together.

\subsubsection{From canonical constructions to non-canonical ones}

If two constructions are of the same type,
but one is much simpler than the other,
whether to place the simpler version together with the less simple version,
or to place the simpler construction with other relatively simple constructions.
Examples include whether the serial verb construction should be discussed 
together with the simple verb-complement construction 
-- the alternative arrangement is to place the verb-complement construction 
together with alignment and argument indexation in a chapter named, say, ``simple clauses'' -- 
and whether the relative clause construction 
should be introduced in the chapter about \ac{np} dependents,
or in the chapter about clause types,
or in a separate chapter.
This chapter is related to the parameter about canonical constructions:
\ac{np}s without relative clauses may be viewed as canonical \ac{np}s,
and non-canonical \ac{np}s containing relative clauses 
can be obtained by substituting attributive modifiers with relative clauses.


\subsection{Terminology}

\subsubsection{Form-based grammatical relation terms}\label{sec:form-based-function-name}

Some people tend to use the lexical or phrasal category 
that prototypically fills a grammatical function slot 
as the name of that grammatical function.
Here is a list of relevant terms:
\begin{itemize}
    \item \term{Nominal} and \term{verbal}: the exact meanings of the terms vary from person to person.
    \item \term{Adverbial}, which is actually peripheral argument (\prettyref{infobox:clausal-dependent}).
    \item \term{Serial verb construction}, which is actually serial predicator construction.
    \item \term{Verb complex}, which is actually predicate (in the \ac{blt} sense) 
    or the predicate minus complements (in the \ac{cgel} sense).
\end{itemize}

\ac{blt}-oriented grammars tend to use more form-based terms about function,
possibly because these grammars are usually results of language description projects,
in which linguists do not have much time to decide which is the best analysis of a phenomenon,
and a large corpus with at least some annotation 
-- despite possible quirks -- 
is better than a few grammaticality judgement tests.

Confusing terms about form and function in practice does \emph{not} cause much problems
-- see \citesec{\ref{latin-sec:form-function}} in \latin.

\subsection{Argumentation}\label{sec:argumentation}

\subsubsection{The amount and position of linguistic argumentation}\label{sec:argumentation-amount}

This section is about how to make linguistic analysis and argumentation,
and how it is shown in a grammar.
A grammar without argumentation will be confusing:
readers will ask ``why does the author say there is clear distinction between nouns and verbs'',
and the answer can only be found by comparing the distribution of nouns and verbs by themselves.
To a certain extent, a grammar without enough argumentation is not truly finished.
It is just an \emph{legend} about the ``shape'' of a language,
and all the reader need to do is to accept the story as it is and never ask why.

A grammar with too much argumentation will be messy.
Consider a grammar in which there are discussions both 
on how to do a top-down partition of a clause 
and on how to assemble verbs and \ac{np}s into a clause. % TODO: ç»ä¸ä¸ªè¿ç§æ¹æ³ååºæ¥çè¯­æ³çreference
It is often the case that the reader has to switch between the two parts frequently 
because a grammar point is introduced in one 
but a highly relevant grammar point can only be found in the other.

Even when the amount of argumentation is appropriate,
where to place them is still quite an art.
Argumentation for distinguishing ditransitive verbs and monotransitive verb with an extended argument,
for example, may be carried out in a chapter mainly about canonical clauses,
but it inevitably involves something about non-canonical constructions 
(passivization, extraction, etc.)
which are still largely unknown for readers reading the grammar from the beginning to the end.
Adding a grammar sketch chapter may ease the problem (\prettyref{sec:grammar-sketch}).

Sometimes argumentation is is mixed with description.
It is possible to talk about what argument can be passivized 
in the chapter about clausal dependents in canonical clauses,
and then shun the topic in the actual chapter about passivization.
This makes the grammar concise but also harder to read,
because readers interested in passivization 
will then have to switching between the two chapters.

\subsubsection{Constituency partition}

In \ac{blt}, the constituency relations are relatively simple,
because there are only two basic types of constituents: 
\ac{np}s and clauses,
while in the \ac{cgel} approach,
the constituency relations are much more complicated
(see \prettyref{infobox:term-predicate}). 

\subsubsection{Identifying function labels and dependency relations}\label{sec:why-argumentation-function-label}

The importance of function labels and dependency relations 
is exemplified well in \prettyref{sec:complement-type-necessary},
and there is no need to repeat here.


\subsubsection{Distinguishing lexical and phrasal categories}

Lexical and phrasal categories can be classified by their internal makeup as well as 
their external distribution.
Similarly to argumentation about function labels,
if a category label is established,
then certain implicational rules about the category exist in the grammar.
The claim that 
``Latin verbs conjugate, not decline, and they head clauses''
can be equivalently formulated without mentioning \term{verb} as 
``if the inflectional endings of a word attested fit in the so-called conjugation table,
not the declension table,
then it is likely that the word prototypically appears in the predicator position of clauses''.

\subsection{Corpus data}

\subsubsection{Deeply annotated or shallowly annotated}

How corpus examples are given -- fully bracketed and labeled, or represented as is?
In the first case, 
an example is a demonstration of the grammatical \term{rules} generalized in the grammar,
while in the second case,
an example is just there as a piece of observation.
In practice, no reference grammar is truly generative 
in the sense of listing all permitted forms and excluding ungrammatical forms.
Thus, examples are never presented purely as examples of grammatical rules.
But whether they are ``deeply'' or ``shallowly'' annotated still observes lots of variations.
This parameter has relation with the top-down or bottom-up parameter,
but the relation is not absolute:
in general, the more bottom-up a grammar is, the more observation-based it is, 
i.e. the examples are provided as is without much tree diagrams or bracketing expressions.
But it is of course possible to have a top-down grammar 
which doesn't make much generalization about grammatical rules 
that generate grammatical utterances 
and exclude ungrammatical ones.
Indeed, this is exact the case for the structuralist Immediate Constituent Analysis:
partition of corpus data is made,
but there is no generalization about what is permitted and what is not.
The correlation between the bottom-up narrative order and the observation-based approach 
is likely due to it is the easiest way 
to document something about a language without mistakes.

\subsubsection{Full or segmental utterance}

Another parameter about how to give examples: as a bracketed constituent in a full utterance,
or taken out of the context?
Structuralist works, like \citet{chao1965grammar}, tend to employ the latter approach,
while modern grammars are the opposite.
Despite being concise and easy to formalize,
the first approach is not easy to use (for obvious reason) 
and not easy to update:
since the scaffolds of the generalizations are removed once the structuralist work is finished,
readers are unable to find why the author decided to take a particular analysis 
when he or she was writing the grammar,
and just like all (semi-)formalized theories,
changing one rule may cause unexpected overgeneralization.

\subsection{Workflow}

It is now time to summarize the basic workflow to carry out a grammar.
Though most modern grammars are bottom-up (just like their traditional antecedents),
top-down constituency and dependency analysis of clauses is still important
and is likely to be the first step in understanding the language.
A toy example can be found in \citesec{\ref{exercise1-sec:clause-partition}} in \exerciseone.

\prettyref{sec:top-down} summarizes how to do top-down analysis.
Details of constructions that may be met aren't discussed in the section.
They are discussed in the bottom-up sections after \prettyref{sec:top-down}.
The organization of sections below implies our workflow:
to first give a sketch of the language (where top-down analyses are important),
and then dive in the details (and carry out the grammar in a bottom-up way).

\section{Top-down analysis of syntax}\label{sec:top-down}

Even for a largely bottom-up grammar,
top-down analyses are sometimes necessary,
both as scaffolds or as argumentation,
which is discussed at the end of \prettyref{sec:grammar-factors}.
In this section, I discuss what to expect when analyzing and annotating clauses.

\subsection{A sketch of the constituency tree of clauses}\label{sec:ica-clause}

This section starts with a purely form-based analysis of clause structure.
That is to say, distinction between grammatical labels like ``subject'' or ``topic''
is not touched.
We first show possible syntactic devices 
-- in other words, constituent analysis and ``movements'' --
and then discuss what grammatical categories are marked by these devices,
or in other words, 
how to assign function labels like ``subject'' and ``predicator''
to the nodes of the constituency tree obtained.

\subsubsection{A clause is built up by one or more nuclei with certain syntactic processes}\label{sec:nucleus-to-clause}

The top-level partition of a clause is given as the follows:
\begin{exe}
    \ex\label{ex:clause-def-1} A \concept{clause} is
    \begin{itemize}
        \item the coordination of two clauses (\prettyref{sec:clause-coord}),
        which may involve ellipsis in and/or movement out of the conjuncts, or
        \item a clause with supplementation (\prettyref{sec:clause-supp}), or
        \item a clause without the two.
    \end{itemize}
    \ex\label{ex:clause-def-2} A clause without coordination or supplementation is 
    \begin{itemize}
        \item a clause with pre- or post-nucleus constructions
        (the residue of the nucleus clause undergoing relevant syntactic processes 
        is named the \concept{nucleus}), 
        like the English subject-auxiliary verb inversion or \term{wh}-movement, or 
        \item a nucleus clause (see \eqref{ex:nucleus-def}).
    \end{itemize}
\end{exe}
Note that the distinction between 
coordination and adjunct clause construction (a type of subordination)
may be not so clear for some languages, 
for example Latin (see \latin, \citesec{\ref{latin-sec:subordination-abs}}).
Also, there is no strict application order 
between coordination, supplementation, and pre- and post-nucleus constructions:
in the English question
\corpus{on that particular day -- I mean the day when the unfortunate incident happened -- 
did you pass that site or hear anything usual in that direction},
first a coordination construction is used, 
followed by a subject-auxiliary verb inversion (a pre-nucleus construction)
and then supplementation 
and finally another pre-nucleus construction (topicalization of the time adjunct).
Another remark here is the syntactic processes from nucleus clauses to more complicated ones 
may only work for certain inputs:
in English, for example, the supplementation \corpus{not even \dots} 
is only possible for a clause in negative voice.
A final remark is that finding the boundary of the nucleus 
requires testing the transformational properties of each clausal dependents.
The constituency tree obtained by immediate constituent analysis 
is without labels,
and it is impossible to decide, say, whether a constituent is a topic (a prenucleus construction)
or a subject (a dependent within the nucleus).
This fact leads some Chinese grammarians to abandon the distinction between topic and subject. % TODO: ref
Here is not the best space to have in-depth discussion for the definition of the nucleus.
A working definition is given in \prettyref{sec:nucleus-dependents},
but it involves terms like \term{complement} and \term{adjunct},
which can't be defined by the label-free constituency tree obtained by clause partition.

\subsubsection{Clausal dependents in the nucleus}\label{sec:nucleus-dependents}

Now it is time to define the nucleus clause:
\begin{exe}
    \ex\label{ex:nucleus-def} A \concept{nucleus clause} is 
    \begin{itemize}
        \item a minimal nucleus clause, or 
        \item a nucleus clause with adjunction.
    \end{itemize}
    \ex\label{ex:minimal-nucleus-def} A \concept{minimal nucleus clause} is a complex of 
    \begin{itemize}
        \item the \concept{predicator}, 
        prototypically a verb but with possible alternatives,
        possibly marked for grammatical categories involved in the clause structure, and 
        \item one or more visible or invisible \concept{complements}, and 
        \item possible function words marking clausal grammatical categories
    \end{itemize}
    or it is a serial verb construction (\prettyref{sec:serial-verb-construction}).
\end{exe} 
Here \concept{adjunction} means adding \concept{adjuncts} into the tree structure, 
in the manner in \ac{tag}.
This is the surface-oriented counterpart of optional projections in Cinque hierarchies.
Adjuncts are contrasted with complements,
the latter being somehow closer to the predicator, 
but not necessarily obligatory.
There are several tests to find whether something is a complement or an adjunct
(see \ac{cgel} \citesec{4.1.2}, for example),
but the distinction is usually quite blurred and language-specific 
(\citesec{\ref{chinese-sec:sub-cat}} in \chinesenote).
The complement-adjunct distinction is usually hard to test simply by pure constituent analysis,
because an adjunct is not necessarily higher than all complements.
The distinction is not what can be studied here -- 
it has to be delayed to \prettyref{sec:complement-type-necessary}.

The term \term{adjunct} used in this note means clausal modifier.
\term{Adjunct}, in generative syntax, 
means optional non-head components of any projection,
though nowadays, especially in the Syntactic Cartography program, 
it is often assume that there is no adjoin operation beside the usual Merge,
and so-called adjuncts are specifiers of certain optional functional heads,
and hence the term \term{adjunct} loses its structural significance.
Many descriptive grammars, like \citep{quirk2010comprehensive}, 
use the term \term{adverbial} for the term \term{adjunct}.
A third name used for adjuncts are \term{peripheral argument} in \ac{blt}.

The term \term{complement} may sometimes be used to denote specifically \term{complement clauses}. % TODO: ref
In \ac{blt}, the term \term{complement} is usually replaced by \term{core arguments}.
\ac{cgel} insists on a strict form-function distinction 
and hence the term \term{argument} is reserved for semantics.
\ac{blt}, on the other hand, emphasizes on the semantic basis of syntax, 
and so the term \term{argument} is used.
But here comes a subtle difference between \ac{blt}'s standard of clausal dependents and \ac{cgel}'s:
certain constituents, like the direction complement in Mandarin Chinese 
(see \citesec{\ref{chinese-sec:direction-complement}} in \chinesenote),
are definitely complements under the standard of \ac{cgel},
but are definitely not arguments, 
and hence they aren't recognized as clausal complements in \ac{blt}
-- they are thus recognized as a part of the \ac{blt} predicate (see \prettyref{infobox:term-predicate}).

\subsubsection{Pre- and post-nucleus constructions not well defined for free-order languages}\label{sec:free-order-blt}

It should be noted that for languages with a relatively free constituent order,
it is almost impossible to find a neutral order, 
and hence pre-nucleus and post-nucleus constructions 
can't be well-defined,
let along the fact that some linguists posit so-called in-VP scrambling
and the pronominal argument construction for radical non-configurational languages
where argument NPs are actually adjuncts,
which are by no means pre- or post-nucleus constructions 
but nonetheless induces changes in the constituent order.
In this case, \eqref{ex:clause-def-2} and \eqref{ex:nucleus-def} should be merged together,
and notions like pre- and post-nucleus constructions are to be replaced by 
discussions on the relation between constituent order and semantic and pragmatic information.

\subsubsection{Classification of clausal dependents}\label{sec:complement-type-necessary}

In the above discussion I intentionally avoid mentioning subject and object
or more generally, any complement and adjunct types,
because as is discussed in the end of \prettyref{sec:nucleus-to-clause}
and in \prettyref{sec:nucleus-dependents},
by staring at the surface form,
it is impossible to define these terms.
The constituent analysis of the surface syntax 
gives us a constituency tree without labels.
In this section, I discuss how to add function labels to the nodes of the tree.
The above discussion -- why the surface-oriented constituency tree is not enough -- 
has a strong flavor of \ac{cgel}.
For \ac{blt}, the importance of function labels is much clearer:
the complexity of the constituency tree is highly limited,
so most of the information has to be conveyed in terms of 
dependency relations, i.e. function labels.

Trivially, the label of a node can be assigned as the category of the node.
Thus we have 
{[$_{\text{S}}$ [\corpus{I}]$_{\text{Pron}}$ [$_{\text{VP}}$ [\corpus{like}]$_{\text{V}}$ [\corpus{fruits}]$_{\text{NP}}$]]}.
But of course \corpus{I} and \corpus{fruits} occupy different syntactic positions:
from folk grammar we know the first is the subject while the second is the object.
Can this distinction be seen from their structural positions?
It is true that in generative syntax,
complement positions can be distinguished purely in structural terms 
(SpecTP, Spec\vP, etc.),
but in the surface-oriented analysis the relevant functional heads are all invisible,
and inevitably there are occasions 
when structural terms are occasionally insufficient to decide the role of a clausal dependent.
A grammarian may want to define the \term{subject} as 
``the most external clausal dependent in immediate constituent analysis'',
and this leads to the ridiculous conclusion that 
in \corpus{quietly, he entered the room}, 
the adverb \corpus{quietly} is the subject.

This example is a vivid example of the importance of \emph{functional} labels in the constituency tree.%
\footnote{
    Here \term{functional} means syntactic function and not pragmatic function.
}
Another illustration can be found in \citefig{\ref{cgel-fig:coarse-grained}} in \cgel:
the information contained in the functional projections in the left tree 
is displayed in the functional labels like \term{predicator}, \term{agent}, \term{patient}, etc.
in the right one.
In the above example of the definition of the subject,
``the most external clausal dependent in the nucleus'' seems to work well.
But \emph{what} is the nucleus?
This question is raised at the end of \prettyref{sec:ica-clause} 
but has never been answered definitely.
One may want to define it in semantic terms:
``a nucleus clause contains all constituents that are necessary to complete the meaning of the verb''.
But isn't \corpus{quietly he entered the room} such a clause?

The conclusion is that in order to find 
complement positions, adjunct positions, the definition of \term{nucleus} 
-- in a word, functional labels -- in a language, 
syntactic tests based on transformational behaviors 
(or to be precise, baed on comparison between regularly related constructions) is needed.
The grammarian should expect lots of cross-linguistic variation of available functional labels,
because they are reflections of the underlying feature structure,
which allows plenty of variations.
In a purported language without any EPP feature, SpecTP may not be not of much significance,
and hence the label \term{subject} 
-- the combination of the syntactic agent i.e. the topmost argument in the argument structure, 
and the obligatory ``topic'' i.e. the topmost argument after TP is finished --
is of little use.
An example of how to find complement types is \citefig{\ref{cgel-fig:english-object}} in \cgel.
By six syntactic standards 
-- constituent order, passivization, preposing, postposing, gap controlling, and predicative adjunct --
five object-like complement types are identified,
and four of them are recognized as objects with one being kicked out of the object family.

\subsubsection{Alignment}\label{sec:alignment}

Recognizing complement types is mostly about syntax.
Semantics is also involved, of course, 
because otherwise it is impossible to decide whether one clause is transformed from another.
But this is not what people already with knowledge on traditional grammar expect.
What do semantic roles of complements and adjuncts 
do in complement type recognizing?
They are important factors in determining complement and adjunct types,
but they are by no means the decisive ones.
Passivization is a well-known counterexample of the uniform matching between semantic roles and complement types.
A locative role may be realized as a complement or an adjunct.
The defective -- yet still largely regular -- mapping between semantic roles and complement or adjunct types 
is therefore worth separate treatment.
This is what known as \concept{alignment}.

\begin{infobox}{Nucluse clause and clausal dependents}{clausal-dependent}
    Clauses can be constructed from nucleus clauses via pre- and post-nucleus constructions 
    and/or coordination and supplementation.
    The nucleus clause contains clausal dependents and the verb complex.
    In the \ac{cgel} approach, the clausal dependents include complements and adjuncts.
    In the \ac{blt} approach, the clausal dependents include core arguments and peripheral arguments.

    This note keeps all the four terms: \term{complement}, \term{adjunct}, 
    \term{core argument}, \term{peripheral argument}.
    Though roughly complements correspond to core arguments,
    and adjuncts correspond to peripheral arguments,
    certain complements -- like the non-argument complements in Chinese -- 
    are certainly not arguments, and may be included as parts of the \ac{blt} predicate 
    (see \prettyref{infobox:term-predicate}).
    Adjuncts are also referred to as \term{adverbials} in some grammars.

    The complement-adjunct distinction and subclassification of the two types of clause dependents 
    rely on syntactic analysis and argumentation 
    based on extraction properties like preposing,
    valency changing properties like passivization,
    and marking of constituent order and morphology.
    Without syntactic argumentation, 
    it is impossible to distinguish adjunct, complement, and topic (a pre-nucleus construction) 
    or to find subtypes within clausal dependents,
    for immediate constituent analysis is unable to provide relevant information.
    
    There is typically uniform but nontrivial mapping from semantic roles to complement and adjunct types,
    called alignment.
    Alignment can also be described as a mapping from the argument structure of a verb 
    to the complement/adjunct types of arguments. 
\end{infobox}

\subsection{So-called ``topics''}

% TODO: topic and subject

\subsection{The inner structure of the verb-complement construction}

\subsubsection{The subject-predicate analysis}

For languages in which \term{subject} can be well-defined (\prettyref{sec:subject-alignment}),
it is useful to divide the nucleus clause into 
the subject and the predicate:
\begin{exe}
    \ex\label{ex:subject-predicate} A nucleus clause is made up by an \concept{external complement}, often named the \concept{subject},
    and a predicate.
    \ex\label{ex:predicate-as-vp} A \concept{predicate} is either a predicate without adjunction, which may be 
    \begin{itemize}
        \item a predicator-complement construction, or 
        \item a serial verb construction, 
    \end{itemize} 
    or a predicate after adjunction,
    or a predicate after syntactic processes marking clausal grammatical categories like 
    negation, modality, etc.
    \ex\label{ex:predicator-complement} A predicator-complement construction consists of 
    a \concept{predicator} (which is the head of the predicate and the clause) 
    and its \concept{internal complements}.
\end{exe}

The above rules replace \eqref{ex:nucleus-def} and \eqref{ex:minimal-nucleus-def}.
These are the rules used in \ac{cgel}, and actually also the Chinese school grammar
(see \citesec{\ref{chinese-sec:clause-constituent-order-overview}} in \chinesenote).
\ac{cgel} doesn't acknowledge the role of verb complex,
which is in principle correct, 
because English is already highly analytic and rigid-order,
and most information about dependency relations can be reconstructed 
from the surface-oriented constituency analysis.
Therefore, the term \term{predicate} is used as in traditional grammar:
it means the nucleus minus the subject, 
and its status as a constituent summarizes \prettyref{sec:subject-alignment}.

\subsubsection{The flat-tree analysis of the verb-complement construction}

It should be noted the above concept of \term{predicate} doesn't always correspond to 
an uncontroversially constituent in the surface structure:
in a VSO language, for example, the predicate is discontinuous.
This urges some to accept a flat-tree approach to describe the nucleus predicate.

To see another motivation of the flat-tree approach in surface-oriented description, consider the following facts.
There are some controversies arising from the ``what's the head'' parameter in \prettyref{sec:what-is-head}.
Some verbs are auxiliary verbs.
In the clause \corpus{I should do this},
what is the predicator?
Here we are in the same delimma as the one concerning ``preposition phrase''.
An analysis in which the predicator is \corpus{should} will face the criticism 
that function words are never heads in a surface-oriented analysis,
or otherwise, in order to be self-consistent,
its bound morpheme counterparts should also be regarded as heads,
which falls back to the generative functional head analysis.

The above two motivations urge us to take 
the analysis in which the main verb \corpus{do} is the predicator.
This approach usually occurs with the flat-tree approach,
or otherwise \corpus{should} is analyzed as a clausal dependent similar to the determiner in an \ac{np},
which is acceptable in the structuralist analysis of Chinese non-argument complements 
(see \citechap{\ref{chinese-chap:non-argument-complement}} in \chinesenote)
but is not prevalent outside the Chinese grammar community.
The nucleus minus arguments (core and peripheral) is named \term{verb phrase} in \ac{blt},
while in \ac{cgel} the term \term{verb phrase} means the verb plus its internal complements,
which is the form of the predicate.
The \ac{blt} \term{verb phrase} is the hierarchy \emph{span} 
(as in \term{span spellout}, or the head slot in the \ac{blt} part in \prettyref{fig:three-formalism}) 
of the \ac{cgel} VP shell.
To avoid conflict, 
the term \term{verb complex} may be used to denote the verb phrase in the \ac{blt} sense
\citep{hockett1948potawatomi,Friesen2017,Wilbur2014}.
Now since there is no need to do fine grained partition of the nucleus,
the term \term{predicate} can be assigned to something else 
and the commonly accepted practice is to use it to denote the verb complex.
In this way, \eqref{ex:subject-predicate}, \eqref{ex:predicate-as-vp}, and \eqref{ex:predicator-complement}
are replaced by the following:
\begin{exe}
    \ex A nucleus clause is made by a predicate
    (with a different meaning with the \term{predicate} in \eqref{ex:predicate-as-vp}) 
    and core and peripheral arguments.
    Rearrangement of constituent order may be necessary.
    \ex A \concept{predicate} may be a simple one or a serial verb construction 
    (\prettyref{sec:serial-verb-construction}).
    \ex A simple predicate consists of a head (prototypically a lexical verb) and  
    possible grammatical category markers (including auxiliary verbs). 
\end{exe}

This approach is not without doubt: the analysis that the main verb \corpus{do} is the main verb 
also faces a problem of non-consistency:
the boundary between auxiliary verbs and lexical catenative verbs % TODO: ref
is somehow vague in some languages, 
and in this case, in somewhere in the grammaticalization process the head status 
suddenly flip from verb to another.
Also, the problem of discontinuous constituent (the verb complex)
still exists, which can also be seen from \prettyref{fig:three-formalism}.
But this is the practice accepted in \ac{blt} and most of grammars sticks to this paradigm,
so I introduce it here.

\subsubsection{Switching between the two}

\begin{figure}
    \centering
    \input{line-box/simple-clause-cgel-blt.tex}
    \caption{Comparison between a \ac{blt} analysis and a \ac{cgel} analysis}
    \label{fig:simple-clause-compare}
\end{figure}

Finally I discuss how to translate between the two approaches.
There are only two key points:
in \ac{blt}, most heavy lifting jobs are done by dependency relations within a large, non-branchable constituent, 
not constituency relations,
and what is a constituent depends heavily on semantics.
Consider \prettyref{fig:simple-clause-compare},
which illustrates the divergences between \ac{cgel} and \ac{blt} 
over a simple predicator-complement construction.
Here is a comparison between the two analyses:
\begin{itemize}
    \item The argument positions in both approaches are labeled by explicit functional labels.
    The necessity has been argued in \prettyref{sec:complement-type-necessary}:
    in Minimalism, the argument positions can be decided purely in terms of the structure:
    the subject is SpecTP, and object is SpecTransP, etc.
    But in the surface-oriented formalisms 
    information contained in the constituent structure
    is not enough to tell us the role of arguments,
    and hence arguments are explicitly labeled via notions like 
    ``subject:'', ``object:'', ``subject slot'' and ``object slot''.
    \item The binding relation between the subject and the object 
    is realized in \ac{blt} as ``the subject tends to somehow control the object cross-linguistically''.
    This reflects the dependency relation-based nature of \ac{blt}:
    the fact that the subject is somehow ``higher'' than the object 
    is not reflected in the constituent structure, 
    but rather by a dependency relation placed in the clause template.
    \item The fact that \corpus{have completed} is a span in the TP projection 
    is reflected in the \ac{cgel} analysis also by constituency structure,
    but \corpus{have completed} is recognized as a constituent in the \ac{blt} approach.
    This reflects the semantic-informed constituent standard in \ac{blt}:
    as can be found in the \ac{cgel} part of \prettyref{fig:simple-clause-compare},
    in the clause structure we have a \ac{vp} \corpus{have completed the task},
    but it is neither a clause nor an \ac{np},
    so it is not a constituent in \ac{blt},
    but once the \ac{np} \corpus{the task} is stripped away,
    the purely verbal skeleton \corpus{have completed} -- which is a span and colored as blue --
    is of semantic significance: 
    it describes the event going on, 
    and the internal gap for the object as well as the external complement (i.e. subject) gap 
    are interpreted as valency (here the dependency-based nature of \ac{blt} is also reflected).
    So \corpus{have completed}, a span in \ac{cgel}, gets recognized a constituent in \ac{blt} 
    and it fills the predicate slot.
    This is illustrated in \prettyref{fig:simple-clause-compare}.
\end{itemize}

\begin{figure}
    \centering
    \input{line-box/predicate-blt.tex}
    \caption{The structure of a simplest \ac{blt} predicate}
    \label{fig:blt-predicate-simple}
\end{figure}

There are adjustable parameters in both the \ac{cgel} approach and the \ac{blt} approach.
The aforementioned ``what is the head'' question 
and ``whether discontinuous constituents are recognized as constituents''
are obviously parameters.
For \ac{blt}, since auxiliary verbs aren't considered as heads in clauses containing them,
the question what are lexical verbs arises,
since in many languages, 
there are catenative verbs that seem to already undergone some degree of grammaticalization,
but aren't typically auxiliary verbs nonetheless.
This question decides what is the head in the \ac{blt} analysis.
Similarly, certain adverbs are actually lexical markers of \ac{tame} categories,
and hence whether they should be considered as a part of the \ac{blt} predicate like auxiliary verbs
is a question without a universal answer.
The \ac{cgel} approach is unable to analyze VSO languages 
with the subject-predicate division without movements,
but similarly, the \ac{blt} approach suffers the problem of discontinuous predicate
in cases like V2 languages.

The fine structure in a nucleus which is a verb-complement construction can therefore be summarized as follows:
\begin{infobox}{The fine structure of the nucleus and the term \term{predicate}}{term-predicate}
    There are fine structures within the nucleus,
    but they are caught with radically different approaches in \ac{blt} and \ac{cgel}.
    The comparison between the two is given in \prettyref{fig:simple-clause-compare}.

    In \ac{cgel}, if \term{subject} can be defined for a nucleus clause,  
    the nucleus clause is divided into the subject and the \term{predicate},
    and the predicate can be then be further divided 
    by successively stripping off the most external dependent.
    In a predicator-complement construction,
    after all adjuncts and complements are separated, 
    the remaining of the predicate is the \term{predicator},
    the head of the predicate, the nucleus clause, and the whole clause,
    which is prototypically filled by a verb.
    The phrasal category of the predicate is typically called the \term{verb phrase}.
    It is also possible to have a serial verb construction,
    where there is more than one predicator-like syntactic item.
    The \ac{cgel} constituent analysis is the immediate constituent analysis 
    plus dependent types annotated by argumentation mentioned in \prettyref{infobox:clausal-dependent}.
    The constituency tree is the coarse-grained result 
    of the constituency relations in the corresponding generative syntax,
    which plays a role in deciding binding government and binding between arguments, 
    which verb to move in subject-auxiliary inversion, etc.

    In flat-tree approach grammars, i.e. \ac{blt}, 
    the only uncontroversial constituent types are \ac{np}s and clauses,
    and all constituents within a larger constituent are placed on the same plane.
    What aren't arguments in a nucleus clause 
    are all placed into the \term{predicate},
    which corresponds to a span in the verb phrase in \ac{cgel}. 
    The predicate in \ac{blt} is also said to be a \term{verb phrase},
    or \term{verb complex} to avoid confusion with the \ac{cgel} meaning of the term.
    What is conveyed by constituency relations in the \ac{cgel} approach
    is alternatively articulated by dependency relations.

    Certain constituents acceptable as complements in \ac{cgel}, 
    like the direction complement in Chinese, 
    tend to be placed in the predicate position in \ac{blt} (see \prettyref{infobox:clausal-dependent}),
    because these non-argument complements are somehow comparable with auxiliary verbs.

    It can be seen that for surface-oriented sketching of previously unknown languages,
    the \ac{blt} approach is more flexible,
    since there is no need to do surface-oriented binary-branching in-depth constituent analysis,
    there is no subject-predicate division which is dubious in ergative languages, % TODO: whether this is true
    and surface constituent order is not taken to be directly reflecting the dependency structure.
    However, since the predicate is the surface correspondence of the verbal skeleton span,
    it can be expected that the predicate may be more frequently realized discontinuously
    compared with arguments which themselves are also constituents in generative syntax,
    and some may want to reduce the content of the predicate to ensure continuousness,
    while others may not.
    When there are auxiliary verbs,
    the \ac{cgel} approach naturally recognizes 
    the most external auxiliary verb as the head of the whole clause,
    the then the second most external auxiliary verb is deemed
    the head of the VP dependent of the top auxiliary verb, etc., 
    while in \ac{blt}, the general tendency is to deem 
    the head of the predicate as the main verb, and not the most external auxiliary verb.
    Both \ac{cgel} and \ac{blt} have the problem of noncontinuous constituents,
    and whether to split them is an adjustable parameter.
    These parameters are left to the grammarian to decide.
\end{infobox}


\subsection{So-called serial verb constructions}\label{sec:serial-verb-construction}

There is no well-recognized definition of serial verb constructions.
Under this name several heterogeneous constructions are put together,
some of which are similar to verb compounding,
while others may be like complement clause constructions but the complement clause is a \vP.

Similar to \prettyref{fig:blt-predicate-simple}, 
the generative constituency relation-based illustration 
of serial verb construction in e.g. \citet{chen2016mandarin}
can be replaced by the dependency relation-based syntactic process to insert several verbs into a single predicate 
and alternate the dependency relation between the verbs and the arguments in \ac{blt}.
\prettyref{fig:serial-verb-construction-1} is an illustration of 
the correspondence between the analysis in \citet{chen2016mandarin} and a \ac{blt} analysis.
The phenomenon discussed in the diagram is the direction complement construction
or direction compounding construction 
(see \citesec{\ref{chinese-sec:direction-complement}} in \chinesenote),
in which a word indicating spacial movement (picked from a closed category) 
-- which is the direction complement or direction compound -- 
is merged together with a TransP,
and the object -- in this case \corpus{t\={a}ng} `soup' -- in the TransP
also becomes a complement of the direction complement,
and by head movement, the direction complement is attached to the main verb 
-- \corpus{s\`{o}ng} `send' here -- 
and hence a compounding verb \corpus{s\`{o}ng la\'{i}} is formed,
which precedes the object.
Now we try to translate the above derivational process to a \ac{blt} account:
\begin{itemize}
    \item The fact that \corpus{t\={a}ng} is both 
    the complement of the main verb \corpus{s\`{o}ng}
    and the direction complement \corpus{la\'{i}} 
    (by appearing in two specifier positions)
    is reflected by the two dependency relations 
    on the right side.
    \item The fact that \corpus{s\`{o}ng la\'{i}} is the span spellout of the verbal projections minus the object 
    is reflected by putting the two words into the predicate slot.
    The constituent order information is reflected in a flat-tree way.
\end{itemize}
Here I intentionally skip the \ac{cgel} approach 
because the complexity of serial verb construction
is already comparable with the functional projection 
that gives rise to the \corpus{have been being done} hierarchy
\citep{ramchand2014deriving},
and the surface-oriented \ac{cgel} analysis will not be substantially different 
from the \ac{blt} analysis shown in \prettyref{fig:serial-verb-construction-1}.

\begin{figure}
    \centering
    \input{line-box/serial-verb-example-1.tex}
    \caption{An example of serial verb construction: 
    \corpus{send come soup} is the literal translation of the Chinese éæ¥æ±¤ \corpus{s\`{o}ng la\'{i} t\={a}ng},
    which means `send soup here (= send soup and make it come here)'.
    Based on (74) in \citet{chen2016mandarin},
    which adjustment to make it compatible with Distributed Morphology.}
    \label{fig:serial-verb-construction-1}
\end{figure}

Here is a problem with \prettyref{fig:serial-verb-construction-1}:
% TODO: è¿ä¸ªææ¾ä¸æ¯SVCåï¼å ä¸ºâæ¥âå·²ç»éåæè¯ç¼äº
and the verbs appearing in a serial verb construction 
may be regarded as morphological components of a complex predicator.

\section{Morphology}

\subsection{The notion of \term{word} and the coverage of morphology}\label{sec:what-is-word}

\subsubsection{Words as black boxes in syntax}\label{sec:word-black-box}

Morphology, roughly speaking, 
is the study of the inner structure of words.
But what are words, anyway?
It should first be noted that the definition of \concept{phonological words} 
doesn't necessarily agree with the definition of \concept{grammatical words}.%
\footnote{
    Here the term \term{grammatical word} isn't limited to function words.
}
\ac{blt} spends a whole chapter (\citechap{10}) to discuss 
criteria to decide the two.
And what is a grammatical word is also not without controversy.
Morphosyntactic phenomena used to decide 
whether a syntactic unit is or is not a grammatical word 
can be summarized as certain versions of the Lexicalist Integrity Hypothesis,
i.e. when it comes to the syntax,
the speaker doesn't really feel the inner makeup of the unit.%
\footnote{
    A stronger claim is the speaker doesn't feel the inner makeup of words at all.
    It is true that native speakers' intuitive partition of words 
    is often problematic when examined closer.
    The representation of Japanese verbal conjugation in the Kana system
    -- in which stem alternation is invoked --
    is accepted by most native speakers,
    but a closer look reveals that concatenative morphology 
    is enough to account for the observed conjugation paradigm,
    though the stem may end up with a consonant and  
    thus native Japanese speakers do not find the concatenative analysis.
    But ignorance of minimal analyzable units happens equally in syntax: 
    native speakers often forget to mention important function words 
    when they are invited to introduce a construction of their language.
    When a native Chinese speaker is invited to talk about the disposal construction,
    the optional function word \corpus{geÇ} in clauses like 
    \corpus{t\={a} b\v{a} w\v{o} geÇ ch\={u}ma\`{i} le} %ä»ææç»åºåäº
    is often left untouched. % TODO: ref
}
Today, we know the hypothesis is not really necessary.
Extraction constraints exist on all levels of morphosyntax,
which can be explained without mentioning \term{word}.
The distinction between word and phrase is likely to be 
a secondary concept,
derived from more fundamental laws of morphosyntax
\citep{bruening2018lexicalist}.
And the hypothesis is also not one hundred percent correct.
What is uncontroversially syntax, like coordination, 
can definitely see how a word is formed,
as in \corpus{pre- and post-revolutionary France}.

One way to redeem this definition is to loosen it.
We may define the word as a constituent that is somehow small enough.
A single noun or verb without any inner structure (like \corpus{apple} or \corpus{hit}) is a word,
and if a construction is unable to take complements larger than a word,
then the construction is a word.
And if we are sure a construction only take word complements,
then whatever complement it takes is a word.
The rules can be used recursively to decide the range of words.
It is possible to arrive at a self-consistent definition of \term{word} now.
But this is still not without problems.

One problem is the tests shown above are unable to decide whether a function morpheme is a word:
the tests to decide whether \corpus{American history teacher} is a word work on lexemes 
(whether the position before \corpus{teacher} is filled by an \ac{np} or just a nominal word, etc.)
and not on function morphemes, especially the most prototypical ones
(\prettyref{sec:lexical-function-distinction}):
A slot filled by things like \corpus{the} doesn't contain more complex structures,
and therefore it's impossible to use substitution tests.
One way to fix the bug is to 
claim that phrases are made up solely by words,
so if a function item always appears as an immediate constituent of phrases,
then it is a word.
Thus, Chinese direction words are nominal words, not suffixes,
since they may head \ac{np}s.
This, however, is problematic for the Latin \corpus{-que}:
it always appears as a marker of coordination,
but it always appears after the initial word of an \ac{np} or the head of the \ac{np},
while on the other hand usually doesn't participate phonological processes in Latin that targets words.
So is it an immediate constituent of the relevant nouns or adjectives,
or is it an immediate constituent of the \ac{np}?
Deciding wordhood according to surface-oriented analysis will hit the wall at a particular time,
because surface-oriented constituency relations don't contain all the information.
But if we then work with frameworks 
that are more flexible with the relation between the phonetic form and the underlying structure, 
like Distributed Morphology,
then the notion of words loses utility:
if, for example, we recognize \corpus{-que} as a grammatical word, 
i.e. a constituent of the \ac{np} and not the nouns or adjectives,
then immediately we have to question the status of the perfect auxiliary \corpus{have} in English,
as one may argue its full form is \corpus{have -en},
with \corpus{-en} being attached to the verb following it in the deep structure,
and ultimately, since the Latin case endings 
-- which anyone with sanity will recognize as affixes -- 
is subject to agreement rules in \ac{np}s,
one may argue that the case is marked on \ac{np}s and then spreads into all nominals within.
And since the selectional relation between \corpus{have} and \corpus{-ing} particles 
can be described by positing an underlying form of \corpus{have -en},
then why not doing the same for subcategorization?
Once transformational post-processing rules are allowed,
we end up in a theory about features and not concrete words.

A rule of thumb is the more a unit is functional,
the more meaningless wordhood is in pure morphosyntax.
(Wordhood of function items is still definable using standards 
in \prettyref{sec:word-minimal-utterance} and \prettyref{sec:phonological-word},
but this is not so useful for people only curious about the structure of the grammar.)

Another problem is the resulting definition is often too broad and hence
goes against with native speakers' ``common sense''.
For example, \corpus{American history teacher} may be argued to be a word.
Its inner components are hard to be extracted out in syntax.
Attributives never go into the constituent.
The form \corpus{*late American history teacher} is not grammatical,
and hence in the construction which starts with the field of learning and ends with \corpus{teacher},
the field of learning part must be a word,
so \corpus{American history} is a word,
and \corpus{American history teacher} is also likely to be a word.
But this of course conflicts with the orthographical standard of wordhood.
In some languages like Chinese, 
wordhood defined by the above morphosyntactic tests 
differs so much from everyday intuitions,
making people wondering whether it is a good idea to just throw the term \term{word} into the trash bin.

\subsubsection{Words as minimal utterance}\label{sec:word-minimal-utterance}

Another way to distinguish words is not based on morphosyntactic tests done by linguists,
but the native speakers' intuition in discourses:
a word may be defined a smallest unit that appears in metalinguistic discourses,
or maybe it is defined as a smallest unit of ordinary utterance.
Here the definition of \term{word} relies more on the condition of being somehow minimal,
rather than on being a unit that is felt by the speaker in actual discourses,
because many -- actually most -- morphosyntactic constituencies that are clearly there 
(which can be tested using standard constituency tests)
are never cited in metalinguistic discourse,
and they are of all sizes.
What are universally realized by the speaker in intuitive language use 
seem to be limited to \ac{np}s and clauses in syntax,
the two being the only constituency types recognized in \ac{blt}
or in other words, two maximal functional domains in generative syntax,
namely the DP domain and the CP domain (see \prettyref{sec:theory}).
In this perspective, the relation between morphemes and words 
is similar to the relation between clausal dependents and heads and clauses.
Consider the theory on categorizers in Distributed Morphology, for example.
A categorizer phrase,
presumably a phase, 
adds category labels to the stem,
without definite categories, which resides in the lowest position of the whole functional projection,
in the same way CP and TP grammatical relations are added to a \vP.
What singles words out is they are the \emph{smallest} units that can be cited.

This view, unfortunately, also suffers from several issues.
The most important problem is speakers sometimes do cite affixes.
Consider the following dialogue:
\begin{exe}
    \ex\label{ex:affix-real} -- You mean `pre-revolutionary', or `post-revolutionary'? \\
    -- `Pre-'.
\end{exe}
The second problem is ``appearing independently in discourse''
is not self-consistent
once several standards are considered together.
In English, single-verb utterances are rare,
but verbs can be cited metalinguistically.
So the two standards -- minimal everyday utterance and minimal metalinguistic utterance -- 
run against each other.
The third problem is the status of function words as words is challenged in this view:
the article \corpus{the}, 
for example, never appears on its own as an utterance,
but since it appears as a dependent in the \ac{np},
it has to be a word, not a morpheme.
Dixon seems to be content with this,
since in \ac{blt} \citesec{1.11} he says there is no need to treat 
``function words'' in the same way as lexical ones:
the former are grammatical markers that can be enumerated in the grammar
and it is even appreciated if they aren't listed in dictionary.

\subsubsection{Words as units with conventionalized meanings}\label{sec:word-meaning}

The inner structure of words are subject to fossilization.
This gives rise to the so-called criterion of wordhood 
that a word has a single conventionalized meaning,
which can't be seen by looking at morphemes inside it.
Thus, the structure of syntactic units directly implies its meaning,
while the structure of words,
though analyzable,
is of primarily historical and etymological interest.

But fossilization exists, of course, in syntax,
examples including idiomatic verb-preposition constructions 
and verb-particle constructions,
and periphrastic conjugations as well.
The distinction between fossilization of word structure and fossilization of syntax 
is better regarded as quantitative, rather than qualitative.
Fossilization means storing a whole structure in the lexicon,
which of course works better for smaller constructions i.e. so-called morphological trees,
rather than large syntactic ones.

As is often said, morphological rules are usually less productive than syntactic ones,
and among morphological rules,
derivation is less productive than inflection in general.
The reason of the first observation is already said above:
smaller structures are easier to routinize 
and hence relevant rules are easier to be eroded.
This also explains why derivation is usually less productive than inflection:
functional projections involved in inflection are somehow more external,
and thus the root together with derivational heads and specifiers
constitute a smaller constituent than 
the root together with derivation heads and inflectional heads,
so the former is easier to fossilize.
This doesn't mean there is no fossilization in inflection.
Fossilization in inflection is the same as collapsing of the functional hierarchy involved,
which increases the fusion index of the language.
So fossilization of inflection makes the inflection rules more obscure 
-- but there are inflectional rules, after all,
because the spellout of the inflectional functional projection span 
still needs to be concatenated to the stem.
On the other hand, 
fossilized derivation results in new synchronic morphemes 
(though with historically analyzable inner structures),
and the corresponding rules are just gone.

\subsubsection{Phonological words}\label{sec:phonological-word}

\subsubsection{What morphology is about}\label{sec:morphology-coverage}

Several conclusions can be drawn from the above discussion.
First, what does the heavy lifting in any definition of wordhood 
is always minimality:
most of the criteria raised apply to what is uncontroversially recognized as syntax as well,
and it is minimality that tells words from phrases.
Second, different criteria usually give conflicting definition of wordhood.
The true meaning of the term \term{word}
will definitely contain some idiosyncrasies 
of the language in question as well as the personal preference of the author of the grammar
(or the cultural tradition, especially the orthography).
The term \term{word} can still be useful as a language-specific descriptive concept,
but the grammar writer has to be explicit about what he or she means by the term.

Considering the chaos in defining \term{word},
expectedly, what is involved in morphology is highly heterogeneous,
much more heterogeneous than the case in syntax.
The following mechanisms involved are the same as ones in syntax:
\begin{itemize}
    \item Constituency tree within the word: 
    functional projections (and thus function labels or grammatical relations in surface-oriented terms) 
    that may appear in categorizer phrases.
    For example, we accept \term{faithful} as a word,
    then the function label of \term{-ful} never appears outside the adjectival categorizer phrase:
    in English phrases it is never possible to simply add a word 
    and term a noun into an adjective meaning ``full of \dots''.
    Function labels within the word may also be the same with 
    the labels in syntax.
    In Chinese, the predicator-object relation is both found in the clause structure and in verb morphology. % TODO: ref
    \item Span spellout without much nontrivial post-syntactic operations: 
    a span of functional projections are spelt out into morphemes
    in a transparent manner.
    Japanese conjugation is a good example of this:
    the conjugation endings may be seen as auxiliary verbs,
    which is reflected in the terminology in the School Grammar tradition.
    This is also seen in syntax:
    the \ac{blt} definition of \term{predicate} (\prettyref{fig:blt-predicate-simple}) is a typical example 
    of how a span is analyzed as a constituent 
    in a surface-oriented theory.
\end{itemize}
As is said above, 
the syntactic (in the sense of Distributed Morphology) part 
of morphology (in the sense of surface-oriented analysis)
has nothing different with the syntax (in the sense of surface-oriented analysis)
in basic structure building mechanisms.
But beside these, languages also have highly localized processes.
The following mechanisms are highly localized and therefore are usually only observed 
in what we call morphology:
\begin{itemize}
    \item Nontrivial spellout: post-syntactic operations like fusion,%
    \footnote{
        Sometimes they are called morphological operations,
        though the latter term is kind of misleading,
        since post-syntactic operations work on features and not morphemes.
        The term \term{morphological operation} suggests 
        a surface-oriented analysis,
        which is dual to Distributed Morphology
        but is not identical to it superficially.
    }
    and spellout based on underspecification.
    Portmanteau is a good example.
    \item Phonological realization may combine two grammatical words into one 
    or split a grammatical word into several phonological words.
    It is even possible for a morpheme of a grammatical word to be 
    attracted by and attached to another grammatical word
    in phonological realization (\ac{blt} \citesec{10.6}). 
    These subtleties are usually not introduced in detail in the name of ``phonology''
    and should be accounted for in chapters about morphology.
\end{itemize}
Of course, nontrivial spellout and phonological rules also appear in syntax (in the surface-oriented meaning):
English auxiliary hierarchy, like \corpus{have been being done},
involves nontrivial span spellout,
while the auxiliary inversion involves head movement,
which may be analyzed as an example of post-syntactic operation.
Prosody drives lots of syntactic phenomena in many languages,
and may even break a word into its parts. % TODO: ref:ä½äºä¸å æ

All processes mentioned above are subject to fossilization.
Fossilization is another source of so-called Lexical Integrity phenomena 
(one source being phasehood of categorizer phrases):
once a word with complex internal structure is fossilized,
its internal structure is of no synchronic significance,
and thus ``morphological rules'' -- to be exact, fossilized diachronic morphological rules --
are irrelevant to ``syntactic rules''.

\subsection{Three models of morphology}

\subsubsection{The Item-and-Arrangement model}\label{sec:item-and-arrange}

For people not interested in recent developments of morphological theories,
the Item-and-Arrangement model may be the default model.
It is the standard model used in American Structuralism
and dates back to the work of PÄá¹ini.
It breaks a word into a sequence (with fixed order by default) of morphemes,
each morpheme carries a piece of semantic information.

The major problems of the Item-and-Arrangement morphology,
despite its clearness and conciseness,
include that in this model 
the difference between functional and lexical morphemes aren't recognized,
that certain morphological devices, 
like duplication or deletion,
are impossible to account for using purely concatenative morphology,
and that sometimes morphemes have no direct meanings.

Distributed Morphology can be regarded as the modern incarnation of the approach,
which solves the aforementioned problems.
Nonconcatenative morphology is accounted for by ``abstract morphemes''
which serve as tags for following phonological readjustments.
An internal change of vowels, for example, may be modeled as a \term{simulfix},
which guides the morphology-phonology interface to change the relevant vowels. 
As \citet{anderson2017words} points out,
Distributed Morphology is essentially a theory of features,
where alleged ``morphemes'' undergo so many operations 
that their correlation with
actual morphemes observed in structuralist analysis of languages
is undermined.

A more straightforward surface-oriented equivalence of Distributed Morphology
is the Item-and-Process approach.
Recall that in Distributed Morphology, a functional projection hierarchy
is mostly a root surrounded by a series functional heads (which allow span spellout) and their specifiers,
and there are phonological rules following the vocabulary insertion 
to produce things like Semitic template morphology \citet{tucker2011morphosyntax}
or stem alternation.
The surface-oriented counterpart of the process above 
therefore starts from a stem, features it carries and its ``arguments''
(which, in the generative theory, are the specifiers of functional heads),
and then the stem is modified according to the features,
and the ``arguments'' are glued to the modified stem at certain stages,
the result undergoing modification as well.

\subsubsection{The Item-and-Process model}

The Item-and-Process morphology is a variant of the Item-and-Arrangement approach.
As is said at the end of \prettyref{sec:item-and-arrange},
it works on \emph{lexemes} instead of morphemes,
which means functional morphemes are kicked out of the lexicon.
What exist are lexemes or ``stems''%
\footnote{
    Here is a subtlety concerning the definition of the terms 
    \term{stem}, \term{root}, and \term{lexeme}.
    See \prettyref{sec:history-or-current-morphology}.
}
and a set of morphological processes 
that can be used to alter the lexemes,
like adding an ending or changing the internal vowels.
The input to the rules are lexemes and 
features like person or number that the speaker wants the result to carry.
This solves the first and the second problems of the Item-and-Arrangement model,
and the third problem about meaningless morphemes 
is also eliminated because 
the rules are free to add morphemes that bear no specific meanings 
to meet, say, phonological constraints.

The Item-and-Process model is not without problems.
It doesn't explain why most of the time,
the Item-and-Arrangement model does a good job.
In other words,
it seems the Item-and-Process model often overproduces.
Maybe it is a good idea to add some of the affixes to the lexicon.
Nor does the Item-and-Process model 
explain why sometimes affixes -- i.e. functional morphemes -- seem to be somehow ``real'',
in marginal cases like \eqref{ex:affix-real}.

\subsubsection{The Word-and-Paradigm model}\label{sec:word-and-paradigm}

The Word-and-Paradigm model is the modern revival of the framework of traditional Latin (and Greek) grammar.
It regards words 
-- and sometimes periphrastic inflection forms like \corpus{have been working},
so to be precise, a better term for the Word-and-Paradigm model is Item-and-Pattern
-- as the smallest units in grammar,
carrying the information of the stem 
and (morphosyntactic, not inherent) features like case and number,
and all of the information is packaged into \emph{one} form,
without real compositional inner structures.
Words differ only in features they carry 
form a concrete paradigm,
and generalizations can be made to extract 
abstract paradigms (like Latin conjugations) or schemas or analogical rules
from concrete ones.
The morphology of a language is represented by a set of prototypical words 
and analogical rules generalized from them.
When a new word is acquired, 
it is inclined tentatively according to the analogical rules (or in other words, schemas).
In the Item-and-Arrangement and the Item-and-Process models,
the paradigm is epiphenomenal,
while in the Word-and-Paradigm approach, 
it is in the central stage.

The basic idea of Item-and-Pattern morphology resembles the overall picture of construction grammar,
though the former has nothing to do with the innateness debate,
because the possible ways people make generalization about word paradigms
may be -- and are likely to be -- limited by domain-specific factors.
(People never encode grammatical categories into things like the Morse code, for example.)
And we can always replace schema with transformation rules.
What really makes the Word-and-Paradigm approach special 
is psychological reality.
If we are just talking about producing all and only grammatical utterances,
then the Item-and-Process model (and the Item-and-Arrangement model plus abstract morphemes)
is equivalent to the Word-and-Paradigm model.%
\footnote{
    There may be mismatches about specific models,
    like for fusional languages,
    Word-and-Paradigm models work well, 
    while a Item-and-Process model in which a process corresponds to only one grammatical category 
    breaks down.
}
But there are some evidences suggesting paradigms are psychologically real.
Back-formation can't be easily explained, for example,
without assuming speakers do not have paradigms in their brains.

Still, the Item-and-Pattern model is not universally recognized,
since it is unable to describe a large part of derivational morphology, like compounding,
and we know there is no clear boundary between inflection and derivation 
(\prettyref{sec:inflection-derivation})
and between functional items and lexemes 
(\prettyref{sec:lexical-function-distinction}). 
And the old problem faced by the Item-and-Process model --
which the morpheme-based theory works well in many cases -- 
is not answered, either.

\subsubsection{What to do}\label{sec:practical-morphology}

The Item-and-Arrangement model catches the overall picture of morphology in many languages
but is unable to account for nonconcatenative morphology easily.
The Word-and-Paradigm or Item-and-Pattern model 
is strikingly powerful to describe nonconcatenative morphology
and implies the psychological reality of paradigms
but is unable to catch the psychological reality of morphemes
or explain why morpheme-based analysis works quite frequently.
The Item-and-Process model is somehow in the middle,
which (under some adjustment) emphasizes on the significance of morphemes
and has the same productivity with the Item-and-Pattern morphology,
but can't explain things like back-formation that implies the existence of paradigms.

What is actually used in grammar writing is a mixture of the three models.
Dixon argues strongly for the Item-and-Process model in \ac{blt} \citesec{3.13}.
But it is quite often that terms like \term{simulfix} (``affix of vowel change''),
which implies abstract morphemes that guide nonconcatenative processes,
are widely used in grammar writing.
The Word-and-Paradigm approach is largely ignored in descriptive linguistics
(\ac{blt} doesn't mention it),
though it is said that in a small corpus situation,
the Word-and-Paradigm approach is more efficient 
for at least knowing something about a language \citep{copot2022word}.
This is possibly because of the classical structuralist idea that
psychological reality doesn't really matter 
in a descriptive grammar.
The term \term{paradigm}, though, frequently appears in grammars.

\subsection{Questions to ask when describing morphology}

\subsubsection{Inflection v.s. derivation}\label{sec:inflection-derivation}

It's sometimes useful to distinguish between derivation and inflection. 
Distinction between derivation and inflection is useful in some languages, but not others. 
In the former case, we can define the \term{stem}: it's something that may or may not undergo some derivation, 
but not inflection, and after proper inflection it becomes a full word. 
For some other languages the distinction between derivation and inflection doesn't make sense. 

Some people claim that inflection is always subject to agreement and derivation is not. 
This is not the case in Dyirbal. 
In Dyirbal, the comitative case can stack with another case marking the argument role, 
so it may be seen as derivation suffix. 
However, it shows agreement between the adjective and the head noun.

Zero inflection marker is a convenient term when other configurations of the same category are marked.

\subsubsection{Historical v.s. synchronic morphology}\label{sec:history-or-current-morphology}

As is said in \prettyref{sec:word-meaning},
erosion of inflection usually means fusion of several inflectional endings,
and the paradigm is still there after all,
while erosion of derivation means creation of new synchronic morphemes.
For languages with historically rich morphology
that has been largely simplified, 
like English and even Latin, 
the linguist may notice that most of the derivation devices observed in the lexicon 
are no longer productive.
Historical morphology is no longer relevant for today's native speakers:
if one item is derived from another using a historical morphological device,
then in purely synchronic considerations,
no regular relation can be established between the two:
we know \corpus{in-} is a negative prefix, 
but \corpus{indifferent} doesn't identically mean \translate{not different}:
there is a shift in its meaning.
In this case, the relevant affixes are not even a part of the grammar,
and hence not a part of the synchronic language
(they are never supposed to be a part of the lexicon, strictly speaking,
and now they are also not a part of the grammar):
they are a part of the history.
It's also possible that a mechanism is still in use today,
but there are already some shifting in the meaning (or even form) 
of a particular result of that mechanism.
This is just the counterpart of idioms in syntax.

In syntax there are also counterparts of totally eroded mechanisms:
\corpus{till death us do part} is an archaic form,
invoking the SOV constituent order,
but since it's in the Book of Common Prayer,
the phrase is still in use today as a whole.
Having an abundant number of historically analyzable but no longer synchronically analyzable processes 
is a feature of morphology as compared with syntax,
but it's more a quantitative problem:
again, a clear border between morphology and syntax can't be drawn.

The fact that derivation creates new synchronic morphemes -- synchronic roots, to be exact
-- with analyzable historical inner structures 
is invoked by some as a definition of the term \term{derivation}:
whatever creates new lexemes is derivation.
But here comes the question about what is a \emph{new} lexeme.
Does valency changing devices create new verbs?
And if English were a language 
in which adding \corpus{-ly} to an adjective
strictly created adverbs with meaning of \translate{in the manner of \dots},
then would this process create new lexemes?
The claim that derivation creates new lexemes 
has nothing essentially different from 
``derivation corresponds to inner functional projections'',
and the latter inevitably involves endless disputations like whether the passive is derivational.
If we understand ``creating new lexemes'' 
as ``creating new synchronic roots (though still with analyzable inner structures)'',
then the claim that derivation creates new lexemes
has no essential differences with ``derivation is kind of historical''.
Under this definition -- 
that derivation is historical while inflection is not --
the tendency that derivation happens before inflection becomes truism.

\subsubsection{What is a word form?}

TODO: how to decide whether to split one cell in the paradigm or not;

\subsection{Morphological devices}

See \ac{blt} \citesec{3.13}, \citesec{3.14}, \citesec{5.2}, \citesec{5.3}, \citesec{5.4}.
Here we introduce different morphological operations and how they are labeled in the Lepzig Glossing Rules. 
The terminology adopted here is slightly different from Dixon's, 
as the latter insists only morphological units in concatenative morphology can be said as morphemes.

\subsubsection{Concatenative morphology: affixation and compounding}

In concatenative morphology, we can distinguish ``free'' and ``bound'' morphemes. 
This doesn't work for nonconcatenative morphology for obvious reasons.
There are people defining free morphemes as morphemes that are independently utterances,
but under this criterion,
most function words (type 3 and type 4 in \prettyref{sec:lexical-function-distinction}) are bounded,
and certain morphemes that are of course not themselves words 
may appear as single utterances in certain occasions (as in \eqref{ex:affix-real}).
Concatenative morphology includes \concept{affixation} and \concept{compounding}.
An \concept{interfix} is something that must appear to link other morphemes 
but doesn't have any semantic meaning.
Concatenative morphology is almost universal, 
with exceptions of perhaps several totally isolating languages.
Affixes (as well as compounding) are labeled as prefix-compound-compound-suffix.

A \concept{clitic} is grammatical item with certain degree of grammatical wordhood 
yet doesn't have independent phonological wordhood.
It's possible (and not rare) 
that certain in-word phonological rules like assimilation doesn't apply on a clitic,
like the case of Latin \corpus{-que}, 
but \corpus{-que} still has to be appended to
the head of an NP as well as the first word.
Clitics are labeled in the format of attached.word=clitic. 

The fact that grammatical wordhood and phonological wordhood allow certain degree of discrepancy
means whether something is an affix or a clitic or a grammatical word should be decided according to 
a usually highly nontrivial procedure.
First, if a morphosyntactic item surely belongs to grammatical word structures only
and not phrasal structures,
then it's an affix,
and otherwise it's a grammatical word.
A grammatical word is then classified according to phonological wordhood, not syntax
(and note that for function words,
sometimes discussing whether something is a word is meaning less (\prettyref{sec:word-black-box})).%
\footnote{
    This also means a clitic doesn't necessarily ``wander among words in the phrase'':
    this is the case for latin \corpus{que},
    but the English article \corpus{the}, 
    in daily speech, is just a clitic 
    and it definitely doesn't go around in the \ac{np}.
}

And now here is the problem of the influence of 
the definition of wordhood on distinguishing affixes from grammatical words.
We have seen that tests used to distinguish grammatical words are not without controversies,
and it's possible that under a particular definition of grammatical words,
there are morphosyntactic units that clearly only appear in grammatical words,
but they happen to have independent phonological wordhood.
Actually, the suffix \corpus{-like} may just be an example:
it's not quite rare to see things like \corpus{Distributed Morphology-like} 
(for example in the wording of \citet{baker2010two}),
and despite the grammatical wordhood of the whole expression
(it can't be added after an arbitrary \ac{np}, etc.),
there seems to be a pause between \corpus{Morphology} and \corpus{-like},
and hence the latter may be regarded as a phonological word,
though grammatically it's a suffix.
There seems to be no good terminology to tell it from affixes like \corpus{-ize}.
Still, it's important to realize things similar to the word-clitic distinction happen inside words,
and phonological observation doesn't necessarily reveal the grammatical relations.

An example similar to the above example but involves a phonologically dependent affix 
is the word \corpus{Feynman diagrammatic}:
from the morphosyntactic perspective, 
it definitely is \corpus{[Feynman diagram] -tic},
instead of \corpus{Feynman [diagramma-tic]},
but the latter is the accepted analysis in printing.%
\footnote{
    By the way, this is also a demonstration that \corpus{Feynman diagram}
    is a grammatical word,
    because it's subject to grammatical processes that only words are able to undergo.
}
which roughly means the latter is the accepted phonological analysis.
So there is a mismatch between phonological and morphosyntactic standards of wordhood 
(\prettyref{sec:morphology-coverage}).
If \corpus{Feynman diagram} were analyzed as a phrase,
then \corpus{-tic} would be analyzed as a clitic.

A grammatical system (like tense) may be realized as affix, clitic or separate grammatical word. 
Here is a generalization that is not merely definition:
the latter two are unlikely to be associated with every word of a phrase, 
but an affix may be (for example, Latin case). 
A case marker at the end of an NP is often a clitic. 
Note, however, that there are no necessary restrictions. 
It's possible that a dialect of a language place the case marker to the end of the NP, 
while another dialect place the case marker after the head of the NP. 
The phonological-morphological process involved is the same between the two dialects, 
but if we claim that a case marker after an NP \emph{must} be a clitic, 
then we have to make the strange claim that 
the \emph{same} morphological process results in a clitic in one dialect and an affix in another.
The problem here is exactly the problem in \prettyref{sec:word-black-box}:
morphosyntactic tests are unable to completely settle down 
what absolutely is an affix and what absolutely is a grammatical word,
especially when the functional item in question has narrow distribution.
So deciding whether something is an affix or a clitic sometimes is impossible.

\begin{figure}
    \centering
    \input{table/word-affix-clitics.tex}
    \caption{Classification of function items}
\end{figure}

Here is a list of frequent functions of compounding:
\begin{itemize}
    \item Noun compounding as a mean of derivation
    \item Verb compounding, 
    \item Noun incorporation
\end{itemize} 

\subsubsection{Non-concatenative morphology with easy-to-recognize morphemes: infix, circumfix, transfix}

An \concept{infix} is annotated as morpheme<infix>morpheme,
which is inserted into an otherwise complete morpheme.
A \concept{circumfix} is annotated as cirfumfix.meaning1-morpheme-circumfix.meaning1.


\subsubsection{Internal change: suprafix, simulfix}

A \concept{suprafix} is a suprasegmental change, including tone, prosody, stress, or nasalization.
a \concept{simulfix} changes one sound in an existing morpheme,
like \corpus{foot} $\to$ \corpus{feet}.
Simufixes are mostly vowel changes,
but consonant changes are also attested.
A \concept{transfix} is an abstract morpheme used to describe 
template morphology as seen in Semitic languages, 
where the root is a template and the affix is a discontinuous one inserted into the root. 
In this wiki, it is glossed as morpheme 
A transfix can be seen as a generalized simulfix.
Reduplication is modeled by abstract morphemes named \concept{duplifix},
annotated as lexeme~the.function.of.duplication.

Things repeated may be initial syllable, first two syllables, last syllable, and the whole word; each of them may have a distinct function
For nouns: plural, diminutive, selection (\translate{long} -> \translate{things that are long})
For verbs: repeating, intensive, continuous
But generally this operation can mean anything
When annotated, reduplication is treated similar to affixation, but with a tilde: yerak-rak in Hebrew means "greenish" and is tagged as green\~ATT

Subtraction is modeled by \concept{disfix}.
For example in Somoan we have \corpus{silaf} $\to$ \corpus{sila}.

\subsubsection{Suppletion and the notion of inflectional stem}

In many languages, like Latin,
we have a ``regular irregularity'':
some forms of a verb can be derived from one ``fundamental form'',
while other forms are to be derived from another ``fundamental form''.
These ``fundamental forms'' are called \term{(conjugation) stems},
and the fact that different stems of the same verb exist 
is called suppletion.
Note that the so-called stems here has no deterministic relation with the stem 
that is the product of derivational processes and the input of inflectional processes
(\prettyref{sec:inflection-derivation}),
regardless of what the two terms mean,
though there is obviously a strong correlation between the two.
Just consider the case of Latin:
the result of verb derivation is subject to conjugation,
and therefore the result of verb derivation is of course the stem 
in the meaning of derivation-inflection distinction
and the stem in the meaning of conjugation paradigm as well.

In principle, a Distributed Morphology analysis doesn't really require the notion of \term{stem}
\citep{embick2005status}.
The paradigms of some verbs, like the Latin \corpus{sum}, are highly irregular and may be analyzed 
as the mixture of several conjugations on several (possibly historical) stems,
but this almost always happens to verbs with important grammatical functions:
in this case, the verbs may even be better glossed as spellout of functional heads.
It's therefore both theoretically and practically wise to list them in the grammar.

Speaking of clearly lexical verbs, the notion of stem is easily replaced by 
readjustment rules:
it's well known that some Latin perfect stems are just present stems with reduplication.
Thus there may be a readjustment rule dictating 
``for verbs like this, this and this, 
reduplicate the root when in the perfect aspect''.

The above Distributed Morphology analysis may be accused of 
conflating diachronic and synchronic rules:
opponents may argue that the reduplication rule is no longer in effect 
in Classical Latin,
and hence a synchronic analysis based on the rule is invalid.
Note, however, that in this context the diachronic-synchronic distinction is somehow vague:
as is said in rule-based generative phonology,
historical sound changes sometimes have one-to-one relation with synchronic deep phonological rules.
The prediction of a generative grammar of a language is
``the language fits in the complexity class of our generative theoretical framework,
which reflects the constraints human language faculty imposes to actual languages''.
The prediction needs not to be (though it can be, as some studies in psycholinguists show)
``that's how human brain actually deal with the language''.
When we say the Latin verb stem alternation can be analyzed in terms of readjustment rules,
we are not claiming ancient Romans really went over the reduplication rule in their minds
when they said \corpus{pepigÄ«} instead of \corpus{pangÅ}.
What we say is the linguist output of ancient Romans shouldn't be very different 
from those who indeed go over the reduplication rules in their minds:
outside canonical verb conjugation, most verbal morphology should be based on the present stem;
one dialect may have productive rules between the stems.
That being said, the notion of stems as the first step in morphology 
is still useful in a surface-oriented analysis. 

\section{The lexicon}\label{sec:lexicon}

\subsection{Overview}

\subsubsection{What is in the lexicon}\label{sec:lexicon-content}

For grammars, the lexicon is the end of top-down partition
and the start of bottom-up description.
Contents in this section are mainly about what are likely to appear in the lexicon,
and how argumentation can be made to assign category labels to them.

\term{Lexicon} here means the surface-oriented one,
the one that people without much knowledge on the generative tradition can also use.
That is, we zip the formative list and the exponent list into 
the library of lexemes and processes on it,
and the Encyclopedia is represented by idiom entries.
In our surface-oriented analysis,
the three lists in Distributed Morphology are mixed into one single lexicon.
This is of some benefits for diachronic research,
since now fossilization, collapse of the inner structure of words, etc. 
are totally within the lexicon,
without problems like whether 
\corpus{kick out} is created by $\sqrt{\text{KICK}}$ plus \corpus{out}
and is interpreted by the Encyclopedia,
or is simply $\sqrt{\text{KICK-OUT}}$ plus a word splitting operation.
The cost is there is now (morpho)syntax within the lexicon
and a lot of lexicon entries 
(actually most of, since even a word like \corpus{get} is a categorizer phrase)
are no longer atomic.

Taking a look at \prettyref{sec:what-is-word},
some ideas can be grasped about what is to be stored in the lexicon
-- and where are the relevant controversies.


It is often said that the lexicon includes form-meaning pairs.
This is a oversimplified statement
for our surface-oriented single lexicon.
If we say \corpus{developed} contains two morphemes -- 
\corpus{develop-} and \corpus{-ed} --
and therefore is the composition of two meanings 
(an action and the past tense),
then what about \corpus{went}?
It contains two meanings compared to \corpus{developed},
but it of course contains only one morpheme.
It is better to understand the lexicon as a traditional dictionary,
in which the meaning of fossilized units that are larger 
override the meaning of smaller units that make up the former,
and regular forms are replaced by irregular forms
(or in other words,
a single form overrides a composite).
Under the general entry about \corpus{kick}
we may see a more specified entry about \corpus{kick out},
which overrides the definition of \corpus{kick} plus \corpus{out},
and in the entry about \corpus{go} we will see \corpus{went} replace \corpus{go-ed}.
Still, the idea of form-meaning pair is important when building an actual lexicon,
since it is the main source of minimal pairs,
especially in the initial stage of investigation.

A morpheme may have several alternative forms or \concept{allomorphs}. 
Again, for languages in which non-affixation processes are frequent and/or fusion is strong 
(as is the case in Latin: a noun inflection suffix should be analyzed as one or three morphemes?), 
we can't use these terms and should work with a more general Item-and-Process approach.

\subsubsection{Notes on the lexical-function distinction}\label{sec:lexical-function-distinction}

In \zhudexi{} \citesec{\ref{zhu-sec:lexical-function-def}},
I note the ambiguity of the lexical-function dichotomy.
The parameter determining the dichotomy may be  
\begin{itemize}
    \item whether a class (noun, verb, determinatives, etc.)%
    \footnote{
        Note that here the term \term{class} is intentionally used instead of \term{category}, 
        because I'm going to talk about category labels later, 
        which are not seen in some classes like determinatives,
        and I also avoided the term \term{word class} because there are also classes of morphemes.
    }
    in the lexicon is open, or 
    \item whether a class has a real category label
    (nouns, verbs, etc. do -- and in Distributed Morphology they are called categorizer phrases -- 
    while words like \corpus{the} do not, and
    whether prepositions have categorizers are not clear
    and may have crosslinguistic variations \citep{deacon2014adpositions}), or 
    \item whether a class is a part of the grammar.
\end{itemize}
There are correlations between the three parameters.
By definition, if a class is a part of the grammar,
then it has to be closed,
and crosslinguistic generalization tells us 
a class without any category label is a part of the grammar 
and hence is closed.
The latter generalization may have exceptions,
because the term \term{class} isn't yet clearly defined,
and if you recognize things like ideophones as a uniform class in the lexicon,
then it is open to a certain extent 
and may be argued to be outside the grammar and has no definite category labels:
they are categorized according to need.
But of course ideophones are grammaticalized,
and if they are like typical nouns or verbs when they fill argument slots or predicate slots,
the ideophone class is not on the same plane with classes like noun or verb,
and can be split into subclasses that belong to nouns, verbs, etc.
In this case, the term \term{ideophone class} 
is actually not about category labels but about etymology.
On the other hand,
if the ideophone class truly has largely uniform morphosyntactic appearances
(like prototypically appearing in the topic position, etc.),
then there are of course a separate category label for the class 
and thus they are just another ordinary class in the lexicon.

So now concerning the lexical-function distinction,
the lexicon can be divided into the following parts:
\begin{enumerate}
    \item Open, with real category label, not a part of the grammar: 
    these classes are prototypically lexical.
    Ordinary nouns, verbs and adjectives 
    are just of this kind.

    \item Closed, with category label, not a part of the grammar:
    these classes are so-called ``closed lexical'' ones,
    examples of which include Japanese verbal adjectives.

    It's subtle to decide how closed a closed class should be.
    Japanese verbs still rarely take new members, 
    like \corpus{gugu-ru} \translate{to google},
    and this may lead some to claim there are new real closed lexical classes.

    \item Closed, with category label, a part of the grammar:
    the most typical class of this kind is the pronoun class.
    Things like verbalizer or nominalizer are also in this type.

    There are some subtleties between the last kind and this kind:
    should things like Chinese direction words be considered as a part of the grammar?
    How about prepositions?
    English prepositions are not just syntactic case markers:
    some, like \corpus{at} or \corpus{during}, may be so,
    but others, like \corpus{up} or \corpus{down}, 
    have meanings still analyzable in verb-particle constructions 
    as well as predicative distributions (\corpus{the network is down}).
    These facts can be accounted for by assuming a uniform preposition class,
    but allowing prepositions to be transitive or intransitive,
    and in this perspective, they are of course not simply case markers.

    \item Closed, without category label, a part of the grammar:
    the prototypical function classes.
    They are about the structure of the language,
    not concrete contents.
    Still, there are marginal examples lying on the boundary between the third kind and this kind:
    Chinese direction words, again,
    are sometimes said to be postpositions,
    and this wording makes people to feel 
    whether it's a good idea to just assume 
    Chinese direction words are category-less grammatical items.
\end{enumerate}
It's also possible for classes of different lexical degrees to overlap.
A verb may have another role as an auxiliary verb,
which may be considered as belonging to a type 3 or type 4 word class.

A final remark on the lexical-functional distinction: 
recall that so-called lexical items 
are just fossilized Distributed Morphology trees with a core root 
and a smallish functional hierarchy 
(possibly containing some ``arguments'', realized as compounding lexemes),
and functional items are span spellout of a functional hierarchy.
This helps us to understand things like closed verb or adjective categories.
Items in a category in type 1 basically have one-to-one relations with 
roots that can be categorized by a certain categorizer.
If a language decides to realize all roots first as nouns,
we will have a closed \citep{cinque2009cartography} TODO

\subsubsection{Lexemes and affixes}

As we say in the above discussion as well as \prettyref{sec:practical-morphology},
the building blocks of a language contain lexemes and morphosyntactic processes,
the letter including affixes.
Affixes can be classified according to \prettyref{sec:lexical-function-distinction} as well.
They are always grammatical 
so they are always sorted into closed classes.
So the parameter left is whether an affix has a real category label.
The English \corpus{-tion} is a suffix with a noun category label,
while \corpus{-ize} is with a verb category label.
Some affixes are without category labels,
On the other hand, \corpus{dis-} seems to lack any category:
we have \corpus{disadvantage}, which is a noun,
but \corpus{dislike} is a verb.

Since affixes are purely grammatical
(or even historical -- see \prettyref{sec:history-or-current-morphology}),
one may insist them being excluded from the lexicon.
But this is faced with the problem mentioned 
in \citesec{\ref{latin-sec:lexical-function-distinction}} in \latin.
The blurred line between lexical and function items means 
any ambitious dictionary editor doesn't dare to skip the latter.
Also, there are often so many of grammatical items 
(whoever doubtful about this can take a lot at the Latin correlative table)
that most grammars simply ignore most of them for brevity.
and the burden of documenting them is shifted to the dictionary editor.
And for practical reasons,
searching for something in a grammar is exhausting,
and to keep everything in the dictionary is a good idea.

\subsubsection{Lexicon entries}

Recall what is covered in a dictionary entry
-- you will need something like ``advanced learners' dictionary'',
because sometimes a dictionary hides important information like subcategorization of a word 
into examples containing the word.
For a lexeme, at least the following things are required:
\begin{itemize}
    \item Enough information to find all (at least frequent) morphological forms.
    Note that in dictionaries for well-known languages,
    it's often the case that 
    the stem is not given.
    What is given instead is the so-called principal forms,
    as in standard Latin or Japanese dictionaries.
    \item Subcategorization: how the word takes complements.
    The term \term{subcategorization} is usually seen together with verbs,
    but of course nouns also have subcategorization: 
    we have \corpus{the fact that \dots} but never 
    \corpus{*the fact to \dots}.
    Sometimes the subcategorization pattern can be found by semantic hints.
    Sometimes it can't: 
    what's the difference between \corpus{accuse of} and \corpus{charge with}?

    It's possible for a lexeme to have several subcategorization patterns.
    Then there are transformational relations between these patterns.
    A good grammar should include frequent subcategorization patterns
    and to recognize function labels in each of them:
    which one is better named \term{direct object},
    what does \corpus{oblique} means, 
    and so on.

    It's possible that the subcategorization pattern of a lexeme 
    varies with respect to the environment?? % TODO

    \item 
    \item Agreement information: 
\end{itemize}
Sometimes we need more subtle information:
\begin{itemize}
    \item Transformational properties of constructions headed by the lexeme.
    For example, we have 
    \corpus{I'm given a gift ward},
    but not \corpus{?he was bought a gift}.
    This shows the difference between \corpus{give} and \corpus{buy} 
    in passivization.
    \item 
\end{itemize}

Everything concerning the properties of constructions headed by verbs -- mostly clauses --
should be in principle recorded in the lexicon,
as long as the properties are not results of general morphosyntactic rules 
or smaller constructions
(for example, it's possible that in a language,
a verb taking a preposition phrase complement,
and because of the properties of the preposition,
the \ac{np} following it can't move out of the nucleus clause).

\subsection{The noun category}

Despite some morphological and syntactic varieties, 
all languages have nouns. 
A noun is typically a head of a phrase filling an argument slot (see \prettyref{infobox:clausal-dependent})
-- this distribution feature is the \emph{prototype} role of nouns, 
and also an important criterion to recognize a noun class.
This is not the only criterion:
for languages with rich morphology, 
the paradigm of prototypical nouns is a useful tool to probe other nouns.

\subsubsection{Possible distribution}



\subsubsection{Semantic classification}

\subsection{The verb category}

\subsubsection{What is recorded in the dictionary entry of a verb}

Verbs may just take \ac{np} arguments

Here is a list of what needs to be described in the dictionary entry of a verb
if the dictionary is expected to provide full information instructing 
how to build a sentence from words:
\begin{itemize}
    \item Grammatical categories marked on the verb and 
\end{itemize}

\subsubsection{Semantic classification}

\subsection{Distinguish verbs and nouns}

In \ac{blt}, how to tell verbs from nouns is discussed in \citechap{11}. 
In many languages, nouns can take the prototypical role of verbs, 
being the predicate in the \ac{blt} sense, 
and/or verbs can take the prototypical role of nouns and head an NP.  
These languages are claimed to lack the distinction between nouns and verbs. 
This is confusion between form and function.
Such cases are discussed in \prettyref{sec:verb-as-np-head} and % TODO:ref



Above are all morphosyntactic criteria. Note that semantics should not be the key point when classifying words.
Thorough analysis of semantics should be done \emph{after} establishing word classes.

\subsubsection{Verb as \ac{np} heads}\label{sec:verb-as-np-head}

Sometimes verbs are able to head \ac{np}s.
Note this is different from complement clauses and free relative clauses:
there is usually no place for \ac{tame} categories or prototypical clause adjuncts in \ac{np}s,
and the marking of arguments may also be quite different.
The comparison between verb nominalization and non-finite clause is a good demonstration of this point
(\prettyref{sec:non-finite}).
In all languages with clause embedding in argument slots,
something headed by a verb appearing as an argument is nothing strange,
but if a real \ac{np} is headed by a verb without any explicit derivation,
this will be an intriguing case.

\subsubsection{List of useful tests}

Argumentation is required to see whether the prototypical noun class and verb class
really have largely overlapping coverage,
and if so, 
what tells verbs and nouns apart.
It is often the case that one or more of the following
happens, and in this case,
only a noun-verb distinction can explain the relevant phenomena: 
\begin{itemize}
    \item Verb and noun may undergo different morphological process: word-class-changing derivations, reduplication, etc.
    \item Verbs in argument slots show meaning different from their meaning when being the predicate,
    hinting a zero derivation.
    Since the derivation is irregular, 
    it may be actually be historical rather than synchronic (\prettyref{sec:history-or-current-morphology}).
    This is exactly the case in English. 
    The verb version of \corpus{father} means "to be sb's father", 
    while \corpus{baby} has no meaning of being sb's baby.
    \item There are clearly noun-to-verb and verb-to-noun derivation mechanisms
    (often irregular and are historical), 
    and they show similar semantic behavior of so-called "noun used as verb" or "verb used as noun" mechanisms.
    If we do not realize the example of \corpus{father} in English,
    the fact that English has a clear noun-verb distinction 
    can also be demonstrated via the following argumentation.
    In English, we have
    \corpus{witness} v. $\to$ \corpus{witness} n. \translate{someone who witness},
    and once this is compared with 
    \corpus{observe} v. $\to$ \corpus{observer} n. \translate{someone who observe},
    it can be seen the noun version of \corpus{witness}
    is from derivation instead of the verb-as-noun mechanism.
    \item Nouns acting as predicates are almost always intransitive, 
    especially when there are many, many normal transitive verbs. 
    This can be easily explained by 
    assuming the nouns undergo a zero derivation and the derivation affix is intransitive. 
    \item A \ac{np} headed by a verb often has more restricted structures 
    (this is also one criterion that defined pronouns).
    \ac{np}s headed by a verb are possible to reject possessive affixes.
    \ac{np}s headed by a verb may obligatorily take an "article" - a nominalizer, actually.
    \item So-called \ac{np}s -- verbs in argument slots -- 
    may actually be clauses,
    which can be found by signs like no adjective modifiers are permitted.
    \item Typical nouns may have a 
    noun class or gender system (which can be reflected by a classifier), 
    number feature, definiteness, and/or case. 
    So-called \ac{np}s headed by a verb often lack one or more systems. 
    \item Nominal predicates may 
    be unable to have \ac{tame} categories
    or be modified by certain adverbs
    or have uncanonical behaviors in serial verb constructions.
    They may also undergo fewer valence changing devices.
\end{itemize}

\begin{infobox}{Distinguishing nouns and verbs}{noun-vs-verb}
    Here is how to distinguish nouns and verbs:
    first accumulate a set of prototypical nouns and verbs,
    and then make a sketch of the prototypical \ac{np}s and clauses,
    and then use word distribution in them as testing standards to 
    divide nouns and verbs.
    It should be remembered that surface-oriented analysis categories 
    is just a projection of the underlying composition of features:
    ``noun features'' and ``verb features'' are universal,
    but what it means to be a noun is not.
\end{infobox}

\subsection{The adjective category (or categories)}

See \ac{blt} \citechap{12}. 

The prototypical function of adjectives is to modify an \ac{np}. 
But whether it is really necessary to introduce a single adjective class is often disputed. 
To describe a single language, any terms may be used, 
and it is alright to merge adjectival classes into the verb class or the noun class. 
However, for unified description and role in typology 
(to ``provide feedback into the basic linguistic theory''), 
we need to recognize a separate adjective class, 
because what can be said as adjectives have robust cross-linguistic functional properties and semantic content. 
If we do not set an adjective class, 
then typical adjectives (those in English) 
have to be compared with a subclass of a language lacking prototypical adjectives, 
creating unnecessary obstacles.

Adjectives can't be distinguished using a single criterion. 
Latin grammarians use the existence of a gender feature to distinguish adjectives, 
which fails for the genderless language Finnish. 
However, in Finnish, some words take possessive suffixes but not comparative and superlative suffixes, 
but another class of words is the opposite, so we can name them as nouns and adjectives.

When analyzing languages where adjectives are like verbs, 
linguists often use the term ``stative verb'', 
but if so, in order to be consistent, they have to admit that in Spanish, 
adjectives are ``stative nouns''. 
Here we see the effort to remind people that something in a language is different from European languages
unfortunately makes them to ignore important language universals.
Avoiding Eurocentrism unfortunately creates more Eurocentrism.

Adjective is a highly heterogeneous category. 
Any word class with the following properties can be named as an adjective class: 
\begin{itemize}
    \item It may (but not necessarily) make a statement that something has a certain property, 
    either by a copula clause,
    or by a clause headed by the adjective, 
    just like an intransitive clause.
    \item It may (but not necessarily) modify an \ac{np} 
    (and sometimes a verb, with explicit derivation or with zero derivation).
    \item It may be in a comparative construction (which may not exist for certain languages).
\end{itemize}

According to its behavior in these constructions, we can classify adjectives into
\begin{itemize}
    \item Type 1: verb-like. It can be head a predicate
    which may be slightly different from typical verbs in
    \begin{itemize}
        \item how it is modified when acting as a predicate 
        (less adverbial constituents are available, for example);
        \item that it can be an attributive modifier 
        (but usually only possible via a relative clause for adjectives like verbs; 
        if this can be achieved without a relative construction, i.e. like a typical attributive construction,
        then the adjective is not likely to undergo similar morphological processes with the noun);
        \item that it can appear in comparative constructions;
        \item that it can form adverbs.
    \end{itemize}
    \item Type 2: noun-like. Here is a list of noun-like properties that frequently appear on adjectives:
    \begin{itemize}
        \item that it may be an NP modifier;
        \item that it may head an NP;
        \item that it has similar morphological properties like a noun;
        \item that it may fill the copula complement position.
    \end{itemize}
    Noun-like adjectives are often slightly different from typical nouns in
    \begin{itemize}
        \item that fewer modification possibilities exist for it than for nouns
        \item that it can appear in comparative constructions
        \item that it can form adverbs
    \end{itemize}
    \item Type 3: sharing properties with both verbs and nouns
    in that it can both appear in the head of a predicate and
    inflect like a noun when in an \ac{np}, 
    and sometimes also act as a copula complement.
    \item Type 4: different from nouns and verbs.
    It's different from nouns in that
    it doesn't undergo similar morphological process with the noun, or maybe 
    its behavior in the copula complement position differs with a typical noun 
    (for example a noun may require an article but an adjective doesn't).
    It's different from verbs in that
    it can't appear in the head of a predicate.
\end{itemize}

A language may have two adjective classes, 
and the adjective class in a language can be closed and highly restricted.
The size of the adjective category is often related to the semantics of adjectives. 
When the number of adjectives is just a dozen or so, 
there are likely to be adjectives about
\begin{itemize}
    \item dimension: big, small, long, short;
    \item age: old, young;
    \item color: black, white, red;
    \item value: good, bad.
\end{itemize}
When there are more members, 
physical properties (raw, hard, heavy) are likely to be described by adjectives.
Larger adjective categories also code human propensity (clever, rude).

\subsection{So-called adverbs, prepositions and other particles}

\section{Verb and arguments}

\subsection{Introduction}

\subsubsection{The argument structures}

This section is mainly about argument slots of ``prototypical'' verbs,
that is to say, about simplest clauses without,
say, complement clause constructions.
That's not to say complement clauses 
can't be analyzed in the framework of verb and arguments,
but since the complement clause construction(s) has much more subtleties,
they're to be discussed in \prettyref{sec:complement-clause}.
And that's not to say this section completely excludes 
complement clauses.
In clauses like \corpus{that he becomes ill strikes us},
\corpus{that he becomes ill} is definitely a complement clause,
but its role -- both syntactic and semantic -- has almost no difference with \ac{np}s
(except the dummy subject construction like \corpus{it strikes us that he becomes ill}).

Listing all argument slots of a verb,
we get its \concept{argument structure}.
It should be noted that argument positions in the argument structure 
are not the same as clausal complement or adjunct types.
The latter are so-called A-positions,
and may be understood as ``surface'' argument positions (\prettyref{sec:valency-changing-theory}).
The argument structure of a verb 
is useful in cases beside clausal structures:
it also largely decides the structure of the \ac{np} headed by the nominalized version of the verb.
And even when valency changing is not considered, 
the clausal complement type of an argument is not purely decided by its position in the argument structure:
\corpus{John's playing the national anthem} and \corpus{that John plays the national anthem}
have the same head verb (I mean the lexeme, not the inflection ending),
but the agent argument is marked differently.
From the generative perspective,
this is because the relations between the verb and its arguments 
are not created locally:
arguments are introduced (probably assigned semantic roles)
in the lower projections of the \vP{} layer,
the accusative case (in accusative languages) is assigned when the \vP{} layer is almost finished,
and the nominative case is assigned in TP.
And this means beside largely semantic discussion of argument structures,
alignment is needed (\prettyref{sec:alignment}).

One prediction -- though a quite soft one -- 
made by the light verb theory in generative syntax 
is that argument positions in the argument structure are not on the same plane.
In the argument structure of \corpus{paint} as in \corpus{I paint the box green},
there are three arguments:
the agent, the patient, and the copular complement.
We may say the patient \corpus{the box} and the copular complement \corpus{green} 
are somehow ``closer'',
and they may be argued to form a ``small clause'' (\prettyref{infobox:clause-def}),
and the agent \corpus{I} is somehow ``higher'',
and reflexive pronouns usually do not appear in this argument slot.
Of course, movement within the \vP{} is still possible,
and sometimes there is no consistent hierarchy of the relative height arguments:
argument A may be closer to argument B under one criterion,
while closer to another argument C under another.
And we have control and ECM constructions,
in which the ``object'' is both an argument in a complement clause and 
an argument of the main verb.
In practice, many descriptive linguists just ignore this 
and just enumerate possible schemes of argument structures,
often in the name of so-called ``semantic-based analysis'' (\prettyref{sec:caveats}).
The scheme of argument structure as is found in \corpus{I paint the box green}, for example,
contains an A argument, an O argument, and a copular complement,
and instead of making a generalization like [A \term{become} [O CC]$_{\text{small clause}}$],
a descriptive linguist will just name this as 
the argument structure of state-changing verb. % TODO:correct name
The fact that the A argument is somehow higher in the position may be implicitly accounted for 
by the generalization that A argument binds other arguments.
The fact that O and CC form a small clause may be accounted for 
by emphasizing on the correspondence between 
\corpus{I point the box green} and \corpus{the box is now green}. 

\subsubsection{Alignment}

There are usually much fewer complement types than semantic roles.
The detailed semantic classification of verbs -- and their complements -- is quite messy. 
\citetable{3.1} in \ac{blt} lists the semantic classification of the most frequent verbs in English.
There are 16 semantic roles mentioned, 
and if the linguist just lists his/her testing results of the syntactic behavior of each of them,
the reader will be driven mad!
A well-organized description of the alignment of a language therefore is based on 
the mapping relation between complement and adjunct roles and coarse-grained semantic roles.
A priori no one knows how to coarse-grain semantic roles 
so that the resulting ``macro-roles'' make sense both syntactically and semantically.
After periods of investigation, several macro-roles have already been identified.
The most important three are S, A, and O\footnote{
    Many denote it as P, to be consistent with the A argument (agentive/patientive). 
    In this note I use O to be consistent with Dixon.
}.
S is the only argument in the intransitive clause.
A and O are the more agentive argument and the more patientive argument in the transitive clause, 
respectively.
Other argument labels include E 
(whatever that occurs together in addition to a SV or AVO construction
-- see \citesec{\ref{cgel-sec:blt-e-argument}} in \cgel),
G (the goal-like argument in a ditransitive construction with a meaning of ``transferring''
-- see \citesec{\ref{cgel-sec:direct-indirect}} in \cgel),
and T (the theme-like argument).

If somehow S and A are realized by the same complement type 
-- which is commonly named the \term{subject} --
then the language is \concept{nominative-accusative} or simply \concept{accusative}.
If, however, S and O are realized by the same complement type,
then the language is \concept{ergative-absolutive} or simply \concept{ergative}.
As is shown by \citefig{\ref{cgel-fig:english-object}} in \cgel,
to say two complement types are essentially the one 
has a vague meaning:
the monotransitive object and the T argument in the English V-G-T construction 
(\term{sth.} in \corpus{give sb. sth.})
are both analyzed by \ac{cgel} as instances of the direct object,
but they differ in passivization.
The same is for identification between S and A or O.
A language can be accusative under certain standards while ergative under others.
For details, see \citesec{\ref{alignment-sec:sao-marking}} in \alignment.

\subsubsection{Additional notes on macrorole labels}

The labels S, O, and A (and similar ones) 
used here are relevant to to the semantic details of argument structure,
but they are syntactic concepts anyway,
i.e. concepts that captures a set of grammatical features that occur together.
Since they are introduced in a derivational stage earlier than the alignment,
in a generative perspective,
the S, O, A labels are just aliases of relative positions of specifiers of light verbs, 
with A being the most ``external'' argument (SpecDoP, for example) 
when there are two core argument being present and O being the internal one, 
and S marking the argument when there is only one. 
These labels are defined before alignment is considered%
\footnote{
    That's not to say alignment has nothing to do with \vP:
    the accusative case is often argued to be assigned when \vP{} is finished, for example. 
}
and are about dependency relations in the \vP{} layer, 
which are connected to properties like binding,
but not necessarily about grammatical properties concerning higher layers, 
like topicalization.

There is some drifting in the meaning of these argument labels.
Essentially being functional labels just like \term{subject} and \term{nucleus},
these labels are syntactic concepts in some contexts,
including ``passivization means turning the O argument into S and turn the A argument into E''
and ``the A argument always binds%
\footnote{
    The term in \ac{blt} \citechap{13}, Appendix 1 is \term{control},
    but \term{control} has its specific meaning in a class of verbs 
    that take an object and an infinitive complement clauses.
}
the O argument, which means O can be filled by a reflexive pronoun bound by A,
while reflexive pronouns do not occur in the A position''.
The notion of S argument is also an inherently syntactic one,
because S arguments in so-called unaccusative verbs are closer to O, not A.
When the S, A, O, etc. concepts are syntactic,
we may say ``the agent argument is marked as A'',
or ``the goal argument is marked as E'',
and complement or adjunct labels are just coarse-grained S, A, O, etc. labels
(e.g. the \term{subject} label is the clustering of A and S), 
because the S, A, O etc. labels denotes complement types with semantic specification.

On the other hand, there are contexts in which the S, A, O, etc. notations 
are defined on an almost solely semantic footing:
the English G argument shows rather split behaviors in the V-G-T and V-T-pG constructions,
as is shown in \citefig{\ref{cgel-fig:english-object}} in \cgel.
Now we do not say ``the agent argument is marked as A'',
but ``the G argument is marked as the indirect object in the V-G-T construction''.
Note that in this case there is no simple relation between complement types and the argument labels:
for example, in \citefig{\ref{cgel-fig:ditransitive-gt}} in \cgel,
The O argument and some instances of the T argument are marked as the monotransitive object,
while the rest of the instances of the T argument are marked as the ditransitive direct object,
so there is no surjection relation from argument labels to complement types,
but nor is there surjection relation from complement types to argument labels.

The requirements of uniform syntactic behaviors, 
of uniform semantic behaviors and of conciseness
when coarse-graining semantic roles 
often can't be satisfied together,
giving rise to the above phenomenon.

\subsubsection{What this section is about}\label{sec:about-argument-structure-sec}

So in principle, a two-step description is required for 
describing the relation between arguments and the verb:
the first step is formation of the argument structure,
in which labels like (syntactic, corresponding to rough positions in the \vP) S, A, and O 
are assigned to arguments,
and the second step is alignment,
in which more ``concrete'' function labels 
like ``subject'', ``direct object'', ``subject-possessor''
are given to S, A, and/or O arguments,
which involves constituent order, case, etc.
Alignment is a process that eats an argument structure 
and produces a semi-finished clause,
which, after marking of \ac{tame} categories and so on,
becomes a complete clause.

The problem with this approach is labels like S, A, and O are not visible in surface-oriented analysis:
clausal dependent types can be found using argumentation like 
\citesec{\ref{cgel-sec:gt-typology}} in \cgel.
Descriptive grammars, therefore,
often mix the discussion on frequent argument structures 
and the discussion on their alignments together, 
and we will see the familiar notion of ``a verb that has a subject and an object'',
instead of the more appropriate ``a verb that has an A argument and an O argument, % TODO: semantic-based SAO labels
which are realized as a subject and an object 
in an active finite clause''.
Still, the macrorole notations S, A, O, etc. are better used when discussing valency changing.
And we can see a mixture of typical argument labels and clausal complement labels
in descriptive grammars:
in \citet[\citesec{8.1}]{jacques2021grammar},
we find \term{intransitive subject}, \term{transitive subject}, \term{object},
\term{theme}, and \term{location} are listed together. 
Things like argument indexation,
though are highly dependent on the TP layer,
are also introduced in the chapter about argument structure 
\citet[\citechap{14}]{jacques2021grammar}.

The output of the morphosyntactic machine described in this section 
is a semi-finished clause with no adjuncts (or ``peripheral arguments'' and adverbs),
no \ac{tame} categories, and no other categories like negation or valency changing.
\ac{tame} categories are necessary for a clause to stand alone
-- actually the case marking of external arguments 
(like the subject in accusative languages) 
is done after T enters derivation.
Thus, once \ac{tame} categories are trivially added,
the output of this section is a nucleus clause (\prettyref{sec:nucleus-to-clause}),
which is subject to information packaging operations,
relativization, etc.
Often, the term \concept{canonical clause} is used to denote the output of this section,
which is the most transparent realization of the argument structure.

The above discussion on argument structure can be summarized as the follows:
\begin{infobox}{Argument structure and argument labels}{argument-structure}
    Coarse-grained semantic roles based on similar syntactic and semantic properties 
    include S, A, O, G, T, etc.,
    and the alignment of a language decides 
    the relation between these argument labels
    and complement and adjunct types.
    The exact meaning of the argument labels varies.
    Sometimes these argument labels are just clausal complements subclassified 
    according to the semantics,
    and sometimes they are mostly semantic.
\end{infobox}

\subsection{Primary and secondary verbs}

Primary-A verbs don't allow complement clauses in their argument slots:
they are the final building blocks of clauses.
Primary-B verbs allow at least one of its argument slots to be filled by complement clauses,
but they still carry meaning that are complicated enough 
and thus never appear as a part of the grammar in any language.

Secondary verbs are simple in there meaning:
they are about meanings like \translate{not}, \translate{must}, \translate{begin}, \translate{want}, etc.
that can be -- but not necessarily are -- expressed by 
functional heads in the \vP{}-TP-CP projections.
In some languages secondary \concept{concepts} in semantics 
correspond to secondary verbs in syntax;
sometimes the former correspond to auxiliary verbs;
sometimes the former correspond to affixation.

Auxiliary verbs may seem to take complement clauses,
but this is more about the superficial realization 
(``morphology'', though people will not call it this way)
instead of the underlying syntactically structure.
Secondary verbs come from a root at the core of a \vP-TP-CP sequence, 
just in the same way as primary verbs,
and the complement clause a secondary verb takes 
is introduced by a higher functional head (like the Trans head in a transitive clause);
auxiliary verbs are more affix-like:
an auxiliary verb is directly attached to the so-called complement clause it takes,
and it doesn't have a $\theta$-grid,
and there are no Trans head introducing the complement clause.
Thus, an auxiliary verb construction is a single-clause construction.

\subsection{The S, A, O typology}

Discussion on prototypical transitive and intransitive verbs is usually the first step 
to understand verb subcategorization and alignment in a language.

\subsubsection{The notion of \term{subject}}\label{sec:subject-alignment}

\eqref{ex:minimal-nucleus-def} is a flat-tree analysis, 
but there are several evidences suggesting 
a fine-grained hierarchy is useful even for surface-oriented analysis.
For accusative languages, 
the S and A arguments are and hence are identified as the \term{subject}, 
and we have the following facts:
\begin{itemize}
    \item The subject is much easier to be extracted out of the nucleus,
    which can be explained by the theory that 
    it is somehow higher and movement operations are localized.
    \item The quantifiers of the subject and internal complements, 
    explicit or implicit, 
    demonstrate a stable scope hierarchy:
    the scope of the subject quantifier is always larger.
    When talking about a charity organization,
    one may say \corpus{every woman helps three boys}.
    Here, the subject is bounded by $\forall$ and the object is bounded by `there exists three \dots',
    and $\forall$ $>$ \corpus{three} and *\corpus{three} $>$ $\forall$:
    the meaning of the sentence aforementioned is 
    `for each woman, there are three boys that she helps,
    but I do not know who they are,
    and possibly the boys Sarah helps aren't the boys Lily helps'.
    After a seemingly trivial passivization, 
    we get \corpus{three boys are helped by every woman},
    which means 
    `there are three boys -- I don't know who, but anyway there are three -- 
    who are helped by every woman in our organization',
    and we have \corpus{three} $>$ $\forall$ and *$\forall$ $>$ \corpus{three}.
    If we assume the semantics is related to the syntactic structure at least partially,
    then this is a piece of evidence that the subject is higher in the syntactic tree,
    no matter what its semantic role is. 
    \item If the subject is indefinite, then it is by default bounded by $\forall$, TODO: really???
    
    Some notes about \ac{blt} \citechap{13}, Appendix 1: 
    TODO: S argument and A argument are by default bounded by $\forall$,
    while O is bounded by $\exists$ -- is this cross-linguistically correct?
    This also explains why verb-object incorporation is frequent:
    \corpus{a cat kills some animals} = \corpus{a cat kills}.
    It seems the only argument -- be it peripheral or core -- 
    that is by default bounded by $\forall$ is S in intransitive clauses and A in transitive clauses
    (which may be seen as a double check ).
    What's the counterpart in syntactic ergative languages?
    \item Verb-argument incorporation, nominalization, etc. 
    (for example compare \corpus{solve problem} and \corpus{problem solving}) 
    usually happens between the verb and the internal complement(s),
    not between the verb and the subject.%
    \footnote{
        Grammaticalization of a span is also in principle possible,
        so incorporation between the verb and the subject 
        may still rarely occur.
        But note that operation on span is usually seen for functional hierarchies,
        in which what are spellout as a single word 
        are highly lightweight functional heads, 
        not more substantial lexical categories.
    }
    \item If there is something looking like reflexive pronouns, 
    then it usually follows the Government and Binding scheme,
    and using this as a test,
    the subject is always predicted to occupy a higher position.
\end{itemize}
The list can go on and on.

\subsection{The G-T typology}

\subsection{Semantic types of verbs and argument structures}

\subsubsection{\classify{motion} and \classify{rest}}

The list can be found in \citet[\citechap{4}]{dixon2005semantic}.

\subsubsection{\classify{affect}}

\subsubsection{\classify{giving}}


\section{Valency changing}

\subsection{Descriptive framework}\label{sec:valency-changing-theory}

Definition of S, A, O, etc. of course doesn't work without modification
for passive sentences and other valence changing constructions. 
Here we need to talk about \term{surface} and \term{deep} labels.
For example, in passive sentences, 
it is convenient to talk about the ``surface S'' which is the underlying O. 
No one would think of the surface S as agentive in the argument structure in any sense. 
So here the question is what \term{surface} and \term{deep} mean.

\subsubsection{Comparison with canonical clauses}

The most direct way to define deep S, A, and O may be transformation relations: 
the deep S, O and A arguments of a clause is the S, O and A arguments in the canonical form of the clause.
Thus, in English passivization construction \corpus{he was arrested}, 
the deep A is not specified and the deep O is he. 
The canonical form of this clause is \corpus{someone arrested him}, 
where \corpus{him} 
(we are not talking about alignment so \corpus{he} and \corpus{him} are considered as identical) is O, 
so in the passive clause, 
the deep argument position of \corpus{he} should be O as well.
Contrary to the deep definition, 
the surface S, A and O are defined by the similarity in \emph{alignment} 
to S, A and O in an active canonical clause. 
We should remark that though the deep definitions of S, A and O are largely independent 
to the details of alignment, 
this is not the case for the surface labels.

Note that we don't specify what grammatical relations are involved 
in the definition of surface argument slots. 
What we just say is ``what marking strategies that define the surface S, A and O 
in a canonical construction also define the surface argument labels in a non-canonical construction''. 
What \term{alignment} means is to be decided by the internal structure of the language in question.
In both syntactic and morphological ergativity,
we say ``S and O are marked similarly'',
but what makes them similar in alignment need further specification.
A single notion of ``surface A'' contains no information,
while a single notion of ``deep A'' does carry some information
(like binding properties).
Essentially, the surface argument labels are A-positions 
(SpecTP, SpecTransP, etc.),
and the alignment details used to distinguish them come from 
peculiarities of the functional projection in clause structure in the language being discussed.

\subsubsection{What does valency changing devices do}

By definition, surface and deep S, A and O coincide in canonical clauses. 
Valence changing of course changes the surface ones,
and by definition, the deep ones are not changed. 
What is changed is also \emph{not} alignment:
alignment is decided by the structure of the language in question
and in some cases \ac{tame} categories and clause types.
Speaking of alignments, external arguments may be extracted out of the \vP{} to higher positions,
for example:
the most famous example is the subject,
and if certain locational arguments are generated inside the \vP{}
but are understood as adjuncts in the surface-oriented analysis,
then they are also moved to functional projections in TP or even CP.
The basic components of these mechanisms are highly stable cross-linguistically:
there are variations on how they combine 
(and hence various types of ergativity, for example),
but they are not altered by the valency changing device.
What's changed is the argument structure,
and the new argument structure is fed into the largely invariable alignment procedure 
to create the clause we can actually see.

Still, (slightly) new A-bar positions -- or the surface argument roles in other words -- are created
in valency changing devices. 
The English \corpus{by} phrase in passive clauses
is unable for most types of extractions, for example,
which is quite different from ordinary peripheral arguments.
Claims like ``in passive clauses, A becomes E'' is thus just approximately correct.

There are several ways to change the argument structure.
It's possible that a layer of functional projection in the canonical clause is ``peeled away'',
or maybe added or replaced.
The argument structure in a canonical clause 
may not strictly appear in a clause with valency changing,
though with transformational analysis,
relations between arguments in the two clauses can still be established.
The subject in an English passive clause is uncontroversially the deep O:
it's introduced in SpecTransP.
However, DoP projection as in the canonical clause doesn't exist in the English passive clause:
it's replaced by another version of DoP with selects a \corpus{by} phrase indicating the agent
(we say the A argument is somehow suppressed),%
\footnote{
    The \ac{np} in the \corpus{by} phrase already gets its case,
    and therefore is invisible for any following probe-goal mechanism:
    we may say it has received an inherent case.
    This may be seen as a demonstration that it's indeed a peripheral argument,
    because nominative and accusative cases are all structural,
    assigned in a much more non-local manner.
}
and it only appears together with the participle affixation.
Therefore, the deep A argument -- which is introduced in the SpecDoP position -- 
strictly doesn't exist in the English passive clause.
Still, there is a strong one-to-one mapping between the two DoPs,
so we still say there is a deep A argument in the passive clause.

It's also possible that something like an auxiliary verb 
(or its affixation equivalence, or serial verb construction) occurs,
and in this case, 
a new argument is introduced.
This operation doesn't really change the existing argument structure:%
\footnote{
    The term \term{existing} here (and henceforth) hints the new argument structure is obtained by
    applying a transformation rule on the canonical argument structure.
    This is, in modern perspectives, not the case,
    but is still a useful tool.
}
it just expands the latter.
But in the following procedures concerning alignment,
there will definitely be some change
in the mapping between deep argument labels and the A-positions.
The auxiliary verb may be the spellout of features in the \vP{} layer
and \ac{tame} categories in the TP layer.

Valency decreasing devices have to involve at least one peeling-or-replacing-argument operation.
Valency increasing devices can be (but is not necessarily) realized by the argument adding operation.
It's possible for a valency decreasing device to have auxiliary verb(s).
English passivization seems like an example,
though the inserted \corpus{be} is more about the TP layer
and not about the argument structure.
The English \corpus{get}-passivization involves 
a semi-auxiliary verb \corpus{get},
and the semantic role of ``undergoer'' or ``experiencer'' of the subject 
is definitely assigned by it,
and therefore we may say \corpus{get} introduces a new argument slot,
which is uncontroversially marked in the TP layer as the subject.
Still, \corpus{get}-passivization involves 
disruption of the canonical argument structure,
since it still takes a past participle expression,
which is formed by replacing the original Do head to its participle version,
as is described above.
The so-called passivization construction in Chinese,
the è¢«-construction,
seems to lean more to the ``adding auxiliary verb'' side,
instead of ``alternating existing structure'' side:
when the agent is present,
as in æè¢«ä»æäºä¸é¡¿, 
ä»æäºä¸é¡¿ -- a \vP{} -- is taken by a light verb è¢« as the complement;
when the agent is absent,
as in æè¢«æäºä¸é¡¿,
the structure is just like ``I underwent beating-up''.
Still, there is arguably change of the existing canonical argument structure.
Suppression of the agent argument is achieved by not introducing the Do projection at the first place,
but this can also be described as 
``first peel off the DoP projection and then insert the resulting æäºä¸é¡¿æ into the è¢«-phrase,
and then move the patient æ to the experiencer position''.
Certain degree of argument structure alternating is still present.
But the derivational æè¢«ä»æäºä¸é¡¿ involves almost no change of the canonical argument structure.

Whenever an auxiliary verb appears,
there is a problem about whether the construction is actually a serial verb construction
or a complement clause construction 
(and the never-ending puzzle what does serial verb construction mean). % TODO:

\subsubsection{Applying multiple valency changing devices}\label{sec:multiple-valency-changing}

The difference on whether a new light verb is introduced
means there is something else that is different between English and Chinese passivization:
where the A-to-E change \emph{first} takes place.
In Chinese it's the \vP{} layer, i.e. the argument structure,
while in English it's the TP layer, i.e. the alignment:
after the agent argument is suppressed,
only the O argument is subject to the EPP probing
and get the structural nominative case.
In a more surface-oriented account,
we say the deep A argument in English becomes peripheral and somehow like ``preposition phrase'',
being indifferent about what's going on in the rest of the clause,
so the deep O argument \emph{has to} be the surface S argument,
though it's never assigned any A-like semantic role in the argument structure,
while in Chinese,
the deep O argument does get some A-like semantic role in the argument structure
(being the ``experiencer'').

This shows a large deficiency of the deep v.s. surface argument notation,
because since we can only guarantee that the surface argument labels are about A-positions, 
it's in principle not possible to fully describe what's going on 
when several valency changing devices are applied one by one
(as in \citet[\citefig{11.1}]{jacques2021grammar}),
because the so-called surface argument labels may be simply not about argument structure,
while the valency changing devices at least partially work on the argument structure.
Fortunately, for the same reason, if a language relies heavily on the alignment process 
to complete valency changing,
then it tends to avoid too much operations on the argument structure,
just as is the case in English,
where semantically valency changing devices are nothing more than 
complement clause constructions,
like the so-called ``causative'' \corpus{let it be done},
where a full passive clause -- instead of a bare argument structure -- is embedded 
into an argument slot of a ``causative'' verb.
Valency changing in these languages are therefore rightfully called \concept{voice}.
For languages in which valency changing is mainly inside the argument structure,
successive application of real valency changing devices which work on the argument structure
instead of the full clause is permitted.
Some authors still use the term \term{voice} \citep{jacques2021grammar},
but this of course created confusion on the typological information of the language in question.

\subsubsection{Working procedure}

Surface S, A and O 
are to be investigated based on the same set of grammatical relations that define 
the clause subject, clause constituent order, 
the interpretation scope, distribution in non-finite clauses, etc. 
The deep S, A and O are defined
by binding, serial verb construction, etc., 
and also largely by semantic intuition -- 
though without knowing the cultural background, mistakes can be done here.
Consider the example of English \corpus{Jack marries Marry and Marry marries Jack} 
and the Chinese å¥¹ç¸å¦æå¥¹å«ç»äºä», 
where an asymmetry arises between å¨¶ and å« and for å« the family of the bride is the agent.
So what we are actually doing here is to first investigate deep S, A and O, 
and then clause-level constructions, which are used to define surface S, A and O, 
and finally to determine whether a seemingly non-canonical construction 
is really a valence changing construction or something else.

When making crosslinguistic comparison,
note that the naming of surface argument labels is assigned 
according to their correspondences with deep arguments in canonical clauses.
The surface A argument in one language can be quite different from the surface A argument in another language.
A word order typology with parameters defined 
according to the clause constituent order 
and the S, A, O, etc. labels (deep of surface) 
is likely to fail when the alignment changes.
Comparing the constituent position of ``topic-like argument'' may be a better idea.

\begin{infobox}{Overview of valency changing}{valency-changing}
    Valency changing is the mismatch between 
    the prototypical argument structure of the head verb (deep argument labels)
    and the morphosyntactic markings of arguments in the actual clause
    (surface argument labels)
    which are classified according to their correlations with the canonical argument structure.
    There are basically two strategies for valency changing.
    In one strategy, valency decreasing is just to push an argument to a peripheral position 
    and let the alignment mechanism to decide how to match surface argument positions 
    with the rest of arguments
    (for example, in passivization, the deep O argument 
    has to fill the surface subject position),
    and the semantic need of valency increasing (like causative) 
    is primarily done by using complement clauses.
    In another strategy, the alignment process is largely trivial:
    passivization is done by create an ``experiencer'' position (which is A-like in the argument structure)
    and move the O argument into it,
    and then the trivial deep-A-to-surface-A alignment is applied. 
    The first strategy is largely clause-based,
    and the second strategy is largely argument structure-based. 
    The first strategy is usually limited in number of valency changing phenomena and compositionality,
    while this is not the case for the second strategy.
    Languages can mix the two strategies in one construction or in different constructions.
\end{infobox}

\subsection{Passivization}

\section{\ac{tame} categories}

\ac{tame} categories, or \term{non-spatial settings} (the name used by Dixon),
are prototypical TP categories.

\subsection{Categories discussed in Reichenbach's theory}

What's commonly called \term{tense} in traditional Latin grammar actually has complex inner structures.
Reichenbach proposes the earliest theory on this topic.
He recognizes three points in time to define Latin (and English) ``tenses''.
The first is the speech time (S), 
the time when a clause is uttered.
The second is the event time (E),
the time when the event corresponding to the clause happens.
Note that an event may last for a certain period of time,
and in this case, 
when is E requires a further parameter,
i.e. which stage of the event is emphasized.
This topic isn't discussed in detail in Reichenbach's theory,
but I still place it in this section (\prettyref{sec:phase-of-activity}), anyway.
The third point in time is the \concept{point of reference (R)}.
It's a point in time that is somehow highly relevant to the event in question,
for example, a consequence of it.
While S and E are about actual events in realis clauses (\prettyref{sec:irrealis}),
R is always conceptual.

\subsubsection{Tense: the relation between S and R}

The category of \concept{tense} codes the relation between S and R.
If S happens after R (we use R-S to denote this circumstance),
we say it's \concept{past tense}.
If S and R happen together (we use R,S to denote this circumstance),
we say it's \concept{present tense}.
S-R is the \concept{future tense}.

\subsubsection{The relation between E and R}\label{sec:completion}

Now we consider the relation between R and E.
E-R is \concept{anterior}.
E,R is \concept{simple},
and R-E is \concept{posterior}.
Completion is sometimes also called \term{aspect},
though this is in conflict with \prettyref{sec:composition}.

In many languages, the event time is the moment when the event ends.
In this way, the relation between R and E
is uncontroversially the category of \concept{completion}:
E,R is the \concept{imperfect},
while E-R is the \concept{perfect}.
A perfect clause is about a completed (and ended) event occurring before the reference time,
while an imperfect clause is about an event still going on at the reference time.
But this isn't always true,
and a category traditionally analyzed as the perfect/imperfect distinction 
may not be so under a closer look.

Consider the English \corpus{has -en} construction, for example.
It's traditionally analyzed as a perfect aspect,
but is this always true?
The clause \corpus{the construction has been labeled as the perfect}
surely doesn't mean 
\translate{the construction's being called as the perfect is already a previous thing}.
It's still about an E-R event,
but here the event time is the time of the \emph{starting point} of 
the construction's being called as the perfect,
not the \emph{end point}.
This kind of usage of the \corpus{has -en} construction is prevalent:
\corpus{he has lived in New York for two years}
means he \emph{started} living in New York two years before the reference time 
(which is the same as the speech time here),
not he \emph{ended} living in New York two years before the reference time.
Thus, the English \corpus{has -en} construction -- and the absence of it --
is not about the category of \emph{completion}:
it's just about the relation between E and R,
without any specification of what E is.
\citet[7.4.2]{dixon2005semantic} uses the 
\term{previous/actual} distinction to name the E-R and E,R cases, respectively.

But the term \term{perfect} isn't that wrong, actually:
if E is the starting point of an event,
we may semantically reinterpret it as ``the ending point of the starting part of the event'',
so \corpus{he has lived in New York for ten years} 
semantically is equivalent to 
\corpus{he has moved (= start to live) in New York for ten years},
and in the latter clause,
the \corpus{has -en} construction is in the perfect aspect.
Thus the English previous or ``perfect'' aspect 
is always semantically equivalent to a real perfect clause.

\subsubsection{Phase of activity: the definition of E}\label{sec:phase-of-activity}

Now we discuss the problem about what's the event time,
especially for a durative event (\prettyref{sec:inner-make-up-event}).
The \concept{phase of activity} of a clause 
may be beginning, continuing or finishing (\ac{blt} \citesec{19.6}).
This can be marked by the grammar (affixation, auxiliary verbs, etc.) 
or by secondary verbs,
like \corpus{start} or \corpus{continue}.

\subsection{The inner makeup of an event}\label{sec:inner-make-up-event}

\subsubsection{Composition: how the inner makeup of an action is perceived}\label{sec:composition}

If the speaker wants to emphasize the inner makeup of an event,
then the clause is \concept{imperfective} in its \concept{composition};
otherwise it's \concept{perfective}.
In English grammar, 
there is something that looks like the perfective-imperfective distinction:
the distinction between the plain aspect and the \concept{progressive} aspect
(\ac{cgel} \citesec{3.1} [3]).
Note, however, that for static events, 
the progressive aspect is not available,
while an imperfective meaning is still possible \citep[\citepage{124}]{cgel}.
So the English progressive is a partial implementation of the imperfective aspect.
In \cite[\citesec{2.2.3}]{dixon2005semantic},
the terms \term{perfective} and \term{imperfective} are used instead.
Note that the composition of a clause can be relatively independently 
with the actual extension of the event it describes:
opening a door is almost a transient event,
but we will say \corpus{I'm opening the door} when asked about what we are doing.
One way to elicit a progressive clause is like the follows:
\corpus{I was writing an article when he knocked the door}.
Whether he knocked the door for a while isn't important here,
so the \corpus{when} clause is non-progressive,
while the main clause is about what was going on in a particular time step
and the inner makeup of my writing the article is important here,
so the progressive is used.

The composition is the most accepted meaning of the term \term{aspect}.
But the term is also used to denote 
\prettyref{sec:completion}, 

\subsubsection{Static or dynamic}

An event can be static or dynamic.
Verb-like adjectives usually head static clauses,
but the inverse is not true:
A clause headed by a verb can still be static, especially when the verb is copular.

Note that the static-dynamic distinction is not purely semantic.
Whether an actual event is described by a static or a dynamic clause 
is not purely determined by semantics.
The event conveyed by \corpus{the flag is red} 
can also be conveyed by \corpus{the flag is colored red},
but the former is static while the latter is dynamic.
And we know the static-dynamic distinction has morphosyntactic consequences.
For example, in English, a static clause is never in the progressive aspect:
We just can't say \corpus{*the flag is being red}.

\subsubsection{Temporal extent: the real makeup of an event}

The temporal extent -- the really makeup of an event, not how it's perceived -- 
may also be marked in languages.
It may be \concept{punctual} or \concept{durative}.
This is not the category of the English progressive v.s. non-progressive distinction:
\corpus{I slept} is ``non-progressive'' in the English grammar tradition,
but it may be punctual (at that particular time, I just went to sleep) 
or durative (I slept during a certain time period).

\subsubsection{Boundedness}

Some events are vague in its boundary:
\corpus{walk from my home to the school} has a clearly defined boundary,
while \corpus{walk around} doesn't.
If a schoolgirl tells you she walked from her home to the school this morning,
you know she said goodbye to her mother at a particular time point 
and entered the classroom at another precise point.
The exact times may not be known,
but there are time points of the start and the end of the event.
But what if she tells you she walked around for a while after school?
The event, ``walking around'', doesn't have a definite boundary
\emph{even in principle}.
She may spend some time in some stores with her friends,
and this is definitely a part of the event;
but is the action of ``heading in the direction of her home after having fun''
a part of the event?
Is it the end of the event, or after the end of the event?

\corpus{Walk from my home to the school} is \concept{telic},
while \corpus{walk around} is \concept{atelic}.
The \concept{time-span/time-frame test} can be used to find the boundedness of a clause,
if it doesn't have any explicit marking.
In English, the time-span adverbial \corpus{for one month} is for atelic clauses,
while \corpus{in one month} is for telic clauses.

\subsubsection{Combination of the categories}

The diagram in \citet[\citepage{128}]{cgel} demonstrates combination possibilities of the categories listed above 
except the composition category.
First we have the division between static clauses and dynamic clauses,
and we have two subclasses under the dynamic class,
which are durative clauses and punctual clauses,
and the punctual class can be further divided into the telic class and the atelic class.
This kind of division seems to be motivated mainly by semantic reasons.

The categories of actual makeup of an event have nontrivial interaction with the composition category,
as is shown in the example of English progressive -- 
static clauses are never progressive.

\subsection{Whether the event really happens}

When we ask whether something happens,
we may be asking about whether it happens in our world 
or maybe just in an imaginary world (in other words, having the tendency to happen),
or we may be asking about how certain \emph{we} are about the event's happening.

\subsubsection{Realis and irrealis}\label{sec:irrealis}

\subsubsection{Modality}

\subsection{Modification of the event}

\subsubsection{Frequency and degree}

\subsubsection{Speed}

\subsection{Reporting an event}

\subsubsection{Degree of certainty}

In \corpus{he may go to New York, but I'm not quite sure},
the auxiliary \corpus{may} doesn't code a modality category.
A modality category is about a fact in the outside world:
\corpus{you may not park your car here} in a notice 
means if you park your car, 
you are in the risk of being fined, etc.
But the \corpus{may} in \corpus{he may go to New York} 
isn't about what's going on in the outside world:
it's about the speaker's certainty on an event.
\corpus{He may go to New York}, 
in this context, means 
``I think he just went to New York, 
but I'm not confident about that''.

\subsubsection{Evidentiality}

Evidentiality is about how the speaker knows about an event.
The speaker may just eyewitness the event.
Or maybe it's just heard from somewhere else.
Note that it's different from the degree of certainty:
the speaker may be quite sure about an event,
but he or she is sure simply because 
a very trustworthy guy says so,
so in this case, 
the degree of certainty is high,
while the evidentiality may just be \classify{reported}.

\subsection{\ac{tame} categories in the real world}

The above framework of \ac{tame} categories is an idealized one.
No attested language compositionally realizes these categories.
It's quite possible for a language to ignore a category,
or to somehow ``fuse'' two categories (\prettyref{sec:fusion-of-tame})
to form a single category,
as is the case of English or Latin ``tense''.

How \ac{tame} categories are marked is also not uniform.
Prototypical marking of \ac{tame} categories include 
verb affixation and/or auxiliary verbs.
But it's of course possible to have \emph{periphrastic} conjugation:
some configurations of \ac{tame} categories are marked morphologically,
while others are marked partly by auxiliary verbs (\prettyref{sec:word-and-paradigm}).
And though adverbs, \acs{pp}s, and serial verb constructions,
which can be used to describe the temporal information of an event 
with or without \ac{tame} categories in detail,
are usually optional 
and hence may be recognized as modifiers instead of markers of grammatical categories,
adverbs like \corpus{now} never appear with past tense,
and now if the past tense suffix in English gets eroded one day,
there may still be arguably a tense system
hidden in the distribution of temporal adverbs.
This is exactly how things work in Cinque hierarchy:
the habitual adverb \corpus{usually}, for example,
is introduced as the specifier of $\text{Adv}_{\text{habitual}}$.


\subsubsection{The status of future}

\subsubsection{Fusion of categories}\label{sec:fusion-of-tame}

In Indo-European languages, the categories of tense and aspect (no matter what this term means)
are often fused together.
It's common to see the habitual tense (or aspect? or even modality? TODO) 
and the simple present tense together.

One way to avoid the confusion of the ideal \ac{tame} categories 
and the actual constructions coding them 
is to use an initial capital to refer to the latter:
thus we may say ``the \term{Perfect} construction in English. i.e. the \corpus{have -en} construction,
may have a \term{perfect} sense, but that's not always the case''.
This is demonstrated well in \citet[\citepage{1091}]{jacques2021grammar}.

\subsubsection{Compatability with other categories}

\ac{tame} categories are limited in clauses 
that are non-finite (\prettyref{sec:non-finite}).
Limitations are also demonstrated when the verb itself has some constraints.
If 

\section{Non-finite clauses and nominalization}\label{sec:non-finite}

In generative syntax, finiteness is in a low position in the CP domain.
It determines what \ac{tame} categories are possible in the TP domain:
non-finite clauses are usually highly limited in \ac{tame} marking,
(which may be understood as ``feature inheritance'' in some generative theories; 
I'll skip theoretical controversies around this).
This even makes some authors to give up the term \term{non-finite} completely 
and use terms like \term{non-tense} instead \citep[\citesec{2.2.1}]{dixon2005semantic}.
Non-finiteness is sometimes confused with \concept{nominalization}:
\corpus{playing the national anthem} is sometimes said to be a nominal clause,
indicating that it has already been nominalized.
But this is not quite true:
it's \corpus{playing of the national anthem} that is truly nominalization,
while \corpus{playing the national anthem} marks the object like all clauses else,
can be modified by adverbs, etc.,
so everything beside the \corpus{-ing} form of the head hints it's a clause, not an \ac{np}.
Still, sometimes nominalization and non-finite clauses are similar enough:
in Classical Latin, for example,
so-called gerunds almost never take objects,
but they can be modified by adverbs,
and this raises the question whether they are non-finite clauses or nominalizations.

\section{Speech force in grammar}

\subsection{Clause types}

\section{Simple clauses}

\subsection{About the notion of \term{clause}}

The term \term{clause} has no mutually accepted definition.
Of course it must contain the verb (or the verbal adjective)
and its arguments,
but is that enough?
What's a unit with the verb and complements but without any \ac{tame} marking
(if it's possible)?
And what's a unit with \ac{tame} marking but without any definite speech force? % TODO: speech force, clause type
The \vP{} constituent, the TP constituent and the CP constituent 
all possibly appear in complement positions,
making it hard to define clausehood by ``independence''.
And don't forget the split projection theory.

One terminology system, 
adopted in \citep[\citepage{140}]{deng2010formal},
names full CPs with fully marked speech force as \term{sentences} 
and CPs with only clause types marked as \term{clauses},
This terminology is peculiar to Chinese,
because Chinese has a rich sentence final particle system 
coding speech acts much more than traditionally recognized clause types like declarative or imperative
(compare \citesec{8.3} and \citesec{8.4} in that book 
-- and that's why the author only assumes split CP but not split TP).
In English, grammatically coded speech force is a synonym of clause type,
leading some linguists to use the label Force to denote the functional head responsible for clause type.
This is not the notation used in \citet[\citepage{142}]{deng2010formal}.
The term \term{small clause} is used in \citet{deng2010formal} 
to denote verbless clauses (\citepage{173}),
but it's used in other places more generally as an informal name of \vP s
(Chinese verbless clauses can be see as a \vP{} headed by a somehow ``predicative'' light verb),
or a bare argument structure in more surface-oriented terms.

Coincidentally (or maybe not so coincidentally),
Dixon assumes a clear distinction between clauses and sentences,
although he also says what is only marked in sentences 
is just coded speech force 
(which he names \term{mood}, 
and argues against the tradition notion of \term{mood} as the distinction between, 
say, indicative or subjunctive;
the latter he names \term{modallity};
this is also the terminology used in some Cartography papers \citep{rizzi2016functional}),
like ``imperative''.
But here is a subtlety:
a clause like \corpus{what you want} is definitely not interrogative in its speech force,
but it does have the clause type of \term{interrogative},
because of the existence of \corpus{what}.
Thus, we may split Dixon's \term{mood} into a largely syntactic version,
according to which \corpus{what you want} is interrogative,
and a version with significance at the syntax-pragmatics interface,
for which \corpus{what you want} doesn't have a definite value.
We may understood the notion of \term{sentence} as a clause 
with the latter version of \term{mood} marked,
and this brings the definition in \citet{deng2010formal} and \ac{blt} together.

Sentences are less frequently embedded than clauses.
Embedded sentences are usually quoted speech,
and in some writing systems may be surrounded by quotation marks.

Now the problem becomes whether a single term \term{clause} is enough 
for all syntactic objects in the surface-oriented analysis.
Even in surface-oriented analyses,
it's well recognized that clauses are of different sizes,
which, in generative terms, 
means some clauses have more functional projection layers than others.
The idea of Cinque hierarchy means if a grammatical category (or grammatical relation)
is present in a clause,
then grammatical categories and relations below it in the functional hierarchy 
are also highly likely to appear.
If \ac{tame} categories are marked,
then the argument structure should be complete.
If the clause has topicalization,
then it is likely to have \ac{tame} categories.
Something 

Clarifying the meaning of the terms is of course important for fully understanding various subordination devices,
like complement clause constructions (\prettyref{sec:complement-clause}) 
or relative constructions.
Indeed, the study of the latter feeds back to the study of clausehood.

\begin{infobox}{Definition of \term{sentence}, \term{clause}, and \term{small clsue}}{clause-def}
    In this note, a \concept{sentence} means a clause plus \emph{full} marking of speech force,
    while the exact meaning of a \term{clause} is indefinite.
    A full clause is an argument structure plus \ac{tame} marking and clause type marking,
    the last indicating certain aspects of speech force.
    But it's also possible to have reduced clauses,
    in which certain functional projections are peeled off.
    A bare argument structure is named a \concept{small clause},
    and the name also applies to small clauses with only little further modification.
\end{infobox}

\subsection{The verb complex}

periphrastic conjugation may also be seen as a complement clause construction. % TODO

\subsection{Constituent order}


\section{Information packaging}

Information packaging, i.e. syntactic marking of the information structure
(what are new, what are old, what are focused, etc.),
involves several strategies.
Certain non-canonical constructions within the nucleus clause, 
like valency changing, 
despite having independent functions,
have marked information structure.
For example, passivization naturally emphasizes what is done instead of who does it.
Pre- or post-nucleus constructions,
like topicalization, 
are frequently about information packaging.


\subsection{Topicalization}

For morphologically rich languages,
the topic may have special morphological marking,
which may even override the morphological marking of structural case received in the TP layer 
(like the nominative or accusative case).
In this case,
the marking of the topic is called the topic case.

\section{Clause combining}

The term \term{subordination} is slightly narrower than the caption of this section.
To be precise,
coordination (\prettyref{sec:clause-coord}) and supplementation (\prettyref{sec:clause-supp}) 
are also means of clause combining,
but since they are not restricted to clauses,
I'll discuss about them later.

\subsection{Serial verb construction}

\subsection{Complement clause}\label{sec:complement-clause}

Complement clauses can be used to replace several grammatical devices:
together with verbs with secondary meanings, 
they replace certain \ac{tame} markings (\prettyref{sec:phase-of-activity}),
certain valency changing devices like the causative (\prettyref{sec:multiple-valency-changing}),


\section{Coordination}\label{sec:coord}

\subsection{Clausal coordination}\label{sec:clause-coord}

\subsection{Coordination in \ac{np}s}

\section{Supplementation}\label{sec:clause-supp}

\bibliographystyle{plainnat}
\bibliography{typology,famous-grammars,controversy,cartography}

\end{document}