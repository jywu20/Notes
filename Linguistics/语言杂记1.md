# 2019.5.5
汉语口语：
- x一x 想一想，吃一吃，看一看
- x不x 好不好，好玩不好玩，好不好玩（“玩”被删除了一个）
- 大保健不大保健，大不大保健，*大保不大保健
- <del>看起来，被删除的应该是一个constituent而不是随便的一个音节</del>
- For him, it is hard to conduct this. 按道理for him不应该是一个constituent的。

# 2019.5.7

- \* Although he is mad.
- ? Though he is mad.
- He is mad, though.
- He is mad, although.
- \* Although he is mad, though.

# 2019.5.9
> ？摩不摩登
> 二不二逼

看起来这和2019.5.5中“被删除的应该是一个constituent”矛盾了

> 爱x不x

> 什么x不x的

# 2019.5.11

否则 的词性

# 2019.5.12

> be factor analyzed

factor analyze $\rightarrow$ factor analyze \[+verb]?

Again, phrasal verbs should be considered via a generative approach.

# 2019.5.13

with/by/... which to do sth.

# 2019.5.15
把字句和被字句真的是非常诡异的东西。

> - 张三抢了李四一顶帽子。
> - 李四被张三抢了一顶帽子。
> - *张三把李四抢了一顶帽子。

# 2019.5.16

> - It is hard for him to do this.
> - For him it is hard to do this.
> - For him to do this is hard.

这三个句子的深层结构是相同的吗？

关键点在于`for him`这个序列是不是一个constituent。

语码混合在生成语言学的框架下应该怎样解释？

同时学习两种不同的语言呢？又该怎样解释？

不是很喜欢现在认知语言学的一些人好大喜功的架势。搞深度学习的一些学者，如LeCun，在这方面当然是专家，但是在语言学的问题上大放厥词，实在是容易陷入外行指导内行的误区。争议的焦点主要在几个方面：

- 带位移的PSG，以及其后继者Minimalist Grammar等等，是不是语言足够合适的模型。（有些人认为Construction Grammar是更好的选择，虽然两者之间似乎并没有足够清晰的界限）
- 语言是否是先天的，也就是，婴儿的神经网络是不是可以不需要任何预设的东西就能够习得复杂的语法结构
- 各种语言是不是具有普遍的结构

我们发现这些点之间并没有特别明确的捆绑关系。我们完全可以想象一个语言学派，他们使用Minimalist Grammar作为**描写**而**不是解释**语言的工具，同时采纳认知语言学的其他论点，又或者一个基本上是正统生成语言学的学派，然而采纳了认知语言学的一些理论作为语用学或者心理语言学的基石（比如说，认为语法是自洽的系统，而语义和语用是usage-based的）然而现在的情况是，争论的双方似乎把这些观点打包成了两个门派，接受一个门派就意味着接受了全部的观点。这不是接近真相的方式！

此外，真心不理解为什么语言先天伦这样不受欢迎。既然能够接受比较底层的认知能力是先天的（它必须是，因为刺激的贫乏在这里是适用的——深度学习需要的经验远远比人类要多，很有可能就是因为人脑是针对某些问题有过优化的，或者干脆就是真·parameter setting），那么为什么不能够接受语言能力是先天的？

也许可以设计一个学习模型，使用相对较少的数据就学习出了一些传统上认为需要使用普遍语法假设的语法规则，然而，像这样的学习模型往往已经预设了“一些东西是值得注意的，另一些不是”（*需要进一步查证*），那这难道不是某种先天知识吗？（**知识**一词或许不是非常确切，因为这里的知识指的是认知能力。真正的知识可以被当成一条语句说出来，而在这里，**先天知识**正是用来说出其他知识的语言能力）

# 2019.5.17

> 北京自行车很多
> 
> 这个过程传输了Q的热量

几个问题：
- “北京”、“这个过程”应该是话题，而不是真正的主语
- “Q的热量”这个结构怎么解释？

Topic-Comment 结构怎么画树？

要是能够找到神经层面的普遍语法证据，就能够有力地支持生成学派；~反之，假如证明了可以找到一种**通用型**的学习机制（以避免手动引入关于语言的特殊优化）来学习到传统上认为只可能来自先天知识的语法结构（Principle A B C之类），那么就能够有力地支持认知学派。（是否能够有力地支持构式语法？）~这个是扯淡，你会用CPU去做显卡做的事情吗？

未来的语言学可能会是认知语言学和生成语言学的某种混合。生成语言学当中的一些东西，如PSG、移位，和认知语言学当中的一些东西，如隐喻、构式，也许可以有机地结合成一种新的学说。也许，现在的生成语言学将主要作为一种描写语言的唯象模型被使用，而偏认知的理论则可以解释一些与非语言认知能力有关的机制。正如量子化学的出现既没有完全否定将配合物看成共价键连接产生的、可以画出结构式的分子的假设，又没有完全否定将配合物看成主要由离子键形成的晶体结构的假设，生成语言学和认知语言学应该都解释了语言能力的一部分，并将被统一到一个更大的框架当中。

需要注意的是，语言学上的语言模型和机器学习上的语言模型以及外语教学当中的语言模型可以完全不一样。前者应该尽可能反映关于语言的人的认知结构，而后两者只需要能够比较好地描写语言就可以了。外语教学中的那种语言学习和儿童的语言习得是完全不一样的东西，前者是纯粹的、自觉的技能学习（当然我本人认为也应该能使用一些残余的普遍语法，也就是说，可能包含一定的Parameter resetting），后者并不是自觉的，而且按照生成语言学的观点，并不是真正的技能学习。机器学习则更不同。如果说，外语教学还需要注意“实际上的人的心理机制”的话，机器学习根本没有必要关心语言产出是如何形成的——建立起来的语言模型只需要能够反映自然语言的一些属性就可以，没有必要一定要和“人说话的机制”一样。（这也就是NLP的大发展不一定对语言认知科学有什么帮助的原因）

某种意义上，机器学习面临的问题和传统的科学是完全不同的。就物理学而言，主要的问题在于写出自然界物质发生变化的普遍规律（量子场论、标准模型，等等），我们当然可以怀疑这普遍规律是不是真的就是自然界运行的方式（“是不是真的是世界的本质”），但这样的追问毫无意义，因为这些规律实在只是在描述“我们眼睛里面的世界”（所以，数学在物理当中异乎寻常的有效性也许只是因为数学能够反映我们使用语言清晰地描述一些东西的时候必然会有的结构，所以数学当然也能够很好地反映物理当中必然会有的一些结构）；涉及较复杂系统的科学——比如说神经科学以及我们这里讨论的语言学——又不一样，因为这里真正重要的是小的元件如何能够拼装出大的系统。在这个层面上，我们可以追问“我们总结出的规律是不是真的本质”。（例如，如果人的认知能力当中语言能力真的是相对独立的，那么生成语言学的解释就更加本质，反之亦然）虽然此时“本质”仍然在哲学上是虚幻的概念，但它已经成为了一个很好的讲故事的方式，因为此时承认还原论并没有什么太大的不良影响。但是，机器学习不一样，在这里既没有物理式的“讲一个能够和实验事实融贯的故事”的要求（以直觉上可以接受的方式与实验事实融贯的物理理论是好的），又没有研究比较复杂的系统时“理论要能够解释为什么小的元件可以构造出我们研究的系统”（所谓“本质性的理论”）的要求。机器学习只需要输入数据有某种规律，是某种计算系统生成的——具体是什么并不重要，关键在于存在性——就可以使用某种通用的模型，比如神经网络，去拟合它。具体用于拟合的模型与实际产出这些数据的系统有什么关系，并不重要。

# 2019.5.18

以后写东西要画句法树了可以使用这样的格式：
> [NP^ Alice]
> 
> [NP [N Alice] and [N Bob]]
> 
> [S[NP[N Alice]][VP[V is][NP[N'[N a student][PP^ of physics]]]]]
> 
> [S [X_a Movement] [Y example \<a>]]

然后使用Tree generator生成树。

# 2019.5.19

## 汉语的被动结构

两种被动句：
> 张三被李四打了。（长被动）
> 
> 张三被打了。 （短被动）

此外尚有中动结构：
> 衣服洗干净了。

- 假设一： 汉语的被动句和英语关系类似，底层宾语移动到表层主语。
- 假设二：“被”字是主句动词，“被”后面的成分实际上是一个从句，其宾语因为和主句主语重复被删除。
## 强调句
> - It was yesterday that he met her.
> - Was it yesterday that he met her?
> - When and where was it that you were born?
> - It was not until his wife came back that he went to bed.
> - It was he who broke the window.
> - *It was him who broke the window.
> - It is they that are mad.
> - It was three books that he read yesterday.
> - It was he that ...
> - It was he who ...
> - It was the book that ...
> - *It was the book who ...

## 主句和PP中代词的分布

> - In his paper, Sam argued that...
> - ?In Sam's paper, he argued that...

或许这是因为这一类句子的深层结构应该是

[TP [VP Sam [V' [V' argued [CP^ that ...]] [PP^ in his paper]]]]

这样一来，表层结构就成为

[TP [PP^_j in his paper] [TP [NP_i Sam] [VP t \<i> [V' [V' argued [CP^ that ...]] t \<j>]]]]

现在尝试构造一个深层结构来产出In Sam's paper, he argued that...

[TP [VP He [V' [V' argued [CP^ that ...]] [PP^ in Sam's paper]]]]

或者

[TP [VP He [V' [PP^ in Sam's paper] [V' argued [CP^ that ...]]]]]

显然这两个结构都违反了约束理论的规定，因为Sam's这个名词性短语始终被放在VP下面，所以肯定受到A-bind。

*这里的一个问题是：约束理论是要对所有层的投射都成立，还是只需要对表层结构成立？*

[TP [PP^_j in Sam's paper] [TP [NP_i he] [VP t \<i> [V' [V' argued [CP^ that ...]] t \<j>]]]]

*我们发现经历Movement之后，原本存在的对约束理论的违背消失了，因为Sam's被放置在了PP下面，因此不能够m-command He。如果约束理论只需要对表层结构成立，就不能够使用这个理由来解释第二个句子不如第一个句子读着自然。*

*感觉这其实可以用作一个测试管辖约束原则在什么意义上成立的测试。一旦弄清楚了在什么意义上管辖约束原则成立，就可以借此作为证据来推断不同的短语是在什么位置生成的。*

# 2019.5.20
为什么therefore后面要加逗号？

> - 北京自行车很多。
> - 她眼睛很大。

乍一看起来，这种一大一小两个主语的结构中，“外面的主语”是“里面的主语”的specifier（她的眼睛很大），但是下面的句子体现出这是不一定的：
> - 田间工作他有经验。
> - ？无线电他是专家，我不是。
> - 无线电方面他是专家，我不是。
> - 无线电，他是专家，我不是。（口语？）

那么是不是“外面的主语”要是“里面的主语”或者“里面的宾语”呢？
> - 汶川地震，他捐了不少钱。
> - 这一期彩票，你买几号？

看起来，所谓的“外面的主语”应该是topic-comment结构的一种体现。

*顺带提一个问题：如果topic和comment没有任何关系，这样的句子是会被判定为非法的，这似乎体现出了一些语言的“意合性”而否定了“形合性”；但是另一方面，“没有颜色的绿色思想愤怒地睡着觉”这一类的论调全然没有意义，完全凭凑不到一起，但是我们还是会承认这是合法的句子。看起来，topic和comment必须有关系应该是有语法基础的，只是这个语法基础和语义关系比较大罢了。*

*我能想到的大概是theta role的分配，也就是topic应该承担一个theta role，但是有了theta role也就应该有格，而topic位置没有什么能够管辖到，所以也就没有格；另一方面，如“王冕死了父亲”这种句子，如果将“王冕”看成topic，那么它就不带有theta role。*

# 2019.5.21

“在x里”应该怎么分析？

“x里”是不是一个名词？

> - 脑子是个好东西。
> - *脑子里是个好东西。

> - 脑子里有问题。
> - 脑子有问题。
> - 在脑子里
> - 放在脑子里

其实可以把“在”当成介词，“脑子里”看成一个方位名词：“脑子的里面”。“脑子里有问题”不合法是因为搭配不对，因为一个单纯的空间区域不可能“有问题”。相反，一个地方可以有问题，

> - 保温杯里装着液氮。
> - 液氮装在保温杯里。
> - 液氮装在保温杯里。

> - 保温杯装着液氮。
> - 保温杯里装着液氮。
> - *保温杯液氮装着。
> - *液氮装着保温杯里。
> - 液氮

# 2019.5.23

The first one is red, the second one (is) blue.

> - 我本来打算这样的。
> - 我们平时不这样说的。

# 2019.5.28

## correlative conjunction
> as . . . as
> just as . . . so
> the more . . . the less
> the more . . . the more
> no sooner . . . than
> so . . . as
> whether . . . or
> both . . . and
> either . . . or
> neither . . . nor
> not . . . but
> not only . . . but also

correlative conjunction

## 2019.5.31
？这篇文章7页。
一篇文章7页，一篇文章13页。

把+NP1+作为+NP2+VP（例：“把文献回顾作为一独立的一部分分离出来”）
把+NP1+状语+VP（例：“把文献回顾清晰地分离出来”）

## 2019.6.1

*not Europeanized language

non-Europeanized language

---

中文和英文当中都有一些词，放在句子里如果没有语境就会很不协调。
- He still comes here (, even ...).
- （如果没有足够的算力，）这办法就不行。

这种东西怎么分析呢？

---

有[\[如何如何]的]功能。这里的“的”字结构很有意思。

# 2019.6.8
An intriguing issue named extraposition: we have the following judgement test:

(1) a. 

tell me it

tell me it seriously

*tell me seriously it

b.

tell me the news

tell me the news seriously

?tell me seriously the news

c.
tell me that his company is undergoing a reconstruction

*tell me that his company is undergoing a reconstruction seriously

tell me seriously that his company is undergoing a  reconstruction
___
it is none the less true
___
be artistically inclined

be inclined to arts
___

Should corpus-based methods be used in generative research? Some linguistics, especially those holding an orthodox opinion of generative theory, may reject introduction of corpus study, contending that corpus data can only reflect language performance, but what is really of importance is **competence**. Therefore, they may claim that generative study should be based on - and solely - on the basis of judgement test.

However, I suspect this claim is doubtable. Although I totally agree that the core of linguistics should focus on competence, and performance is to be investigated by some subjects on the interface - say, psycholinguistics - I don't think corpus study is irrelevant to the research of competence.

The point is in how infancies acquire language. Young children tend to ignore corrective sentences from their parents when they make a grammatical mistake. On the contrary, it seems that they correct these mistakes automatically by exposure to more input. It can be concluded that if there **is** a universal grammar, it should be constructed in a way that can be easily implemented by statistical learning, especially neural network, and in such a system, rare frequency of a certain kind of input results in a parameter configuration that does not produce such kind of input. For example, if Subject-Auxiliary-Inversion is rare in language input during first language acquisition of a child, the parameter of SAI is "closed" in his command of the language.

If universal grammar, in the biological sense, works in this way, each well-elaborated generative grammar should take the same property. Therefore we attain the conclusion that corpus study can well reflect internal grammar construction of a language, because if one structure is rare in the corpus, probably it is just because people do not produce sentences with this structure, and any native speaker of this language is inclined to acquire a language in which parameters are set to block generation of such structure.

Hence it can be observed that corpus data is a candidate method for acceptability test.

# 2019.6.9
各种谜之错误使用标点符号：

    如果是这样。你会怎么做？

    如有打扰！还请抱歉。

    若有！是什么？

真搞不懂某些人的脑回路。

# 2019.6.15

在一些版本的管约论当中，句法树可以有三分支，例如考虑这样的情况：
$$
\text{NP} \rightarrow \text{NP} \; \text{Conj} \; \text{NP}.
$$
在最简方案当中这样的分析是不被允许的，因为Merge操作只能够产生二叉树。不过，如果我们将之前那种允许三分支或者更多的分支的句法树看成是最简方案二叉树使用方括号表示法表示之后略去某些层级，然后再画成树的结果，就可以发现，三分支仍然能够以某种方式停留在最简方案当中。

例如，This plan is proposed by Stephen.这个句子中的VP一开始应该是这样的：

[<sub>VP</sub> *e* [<sub>V'</sub> [<sub>V'</sub> [<sub>V</sub> proposed] [<sub>NP</sub> this plan ] [<sub>PP</sub> by Stephen ]]]]

这里我们使用的方括号记号要求每个方括号内或是一个完整的、不再被分解的成分，或是对树形结构的真实反映。也即，下面的记号

$$
[_\text{X*} \; [_{\alpha_1} \dots] \; [_{\alpha_2} \ldots] \; \ldots]
$$
意味着$\alpha_1, \alpha_2, \ldots$是$X^*$的直接子节点，并且$X^*$**只有这些直接子节点**；在这样的记号下，显然，一个方括号内或是只有一连串单词，代表将这些文本视为一个整体而不再加以分析，或是只有两个方括号，代表这个节点之下的两个子节点。

然而，我们还可以使用一种宽式记号，在这里，记$\alpha$代表某个单词序列或者方括号记号，
$$
[_\text{X*} \; [_{\alpha_1} \dots] \; [_{\alpha_2} \ldots] \; \ldots]
$$
表示：$\alpha_1, \alpha_2, \ldots$是$X^*$的**直接或间接**子节点，并且，它们的顺序按照1，2，3这样的顺序排列。实际上这样的宽式记号更加常用，虽然对于一个没有任何限制的句法树，只有它的一个宽式记号并不足以复原出它来。如果我们把一个宽式记号当成一个严式记号来看待，并根据这个宽式记号绘制句法树，那么宽式记号对应的句法树中“直接子节点”的关系就和严式记号对应的句法树中“直接或者间接子节点”的关系一一对应。

因此即使是在最简方案的框架下，$[_{\text{NP}_1} \text{NP}_2 \; \text{conj} \; \text{NP}_3]$这样的记号仍然是有意义的，只不过它对应的是宽式记号，代表“NP1之下有直接或间接子节点NP2, conj, NP3，并且它们的排列顺序就是这样”。

于是我们建立起最简方案句法树和多分枝句法树的对应关系：最简方案句法树可以看成多分枝句法树按照特定的算法做二叉化的结果，多分枝句法树可以看成最简方案句法树写成宽式记号之后画树的结果，也就是，宽式记号中直接子节点的关系对应着最简方案句法树中直接或间接子节点的关系。

---

另一个问题是X-Bar理论和最简方案中Bare structure的关系。这个其实容易理解，最后一个Merge上去的argument就是specifier。

---

最简方案的很多东西都需要词库配合才能够完成。

---

当……时
在……的时候
……时

“当”字看起来是可以省略的。

---

看起来，英语口语当中也会有topic-comment结构：
The notion that you hear fusion is another 20 years away, 30 years away, 50 years away—it's not true

---

另一个值得探讨的问题是，在什么样的情况下两个Minimalist Grammar可以看成是等价的。我举一个例子：在分析英语的时候，可以将句子结构分析为 [CP [TP [T' [VP ]]]]，但是在很多其他的语言当中我们需要把T拆分成几个更细致的层级。但是，在英语的情况下，这更为精细的分析和粗糙的将T看成一个整体是等价的。那么在什么样的情况下，可以把[AP [BP [CP ...]]]的层级结构看成单一的一个最大投射？

我们还可以注意到，把名词短语分析成NP和DP在很多情况下并没有多大差别，也就是说只要树的形状正确，中心语是什么可能并不重要。怎样在数学上分析它呢？

# 2019.6.18
看成……

看成是……

正如我们可以预期的那样，“是”在汉语当中绝对不是简单的系动词。

# 2019.6.19
You find him, Stan!

这个句子的结构怎么处理？

---

我有话要和你说。

我有要和你说的话。

存在一个态不可能经过绝热过程达到。

存在一个不可能经过绝热过程达到的态。

---

关于……，请参见……。

？然而，关于……， 请参见……。

有趣……句子前面的那个位置不能容纳太多的成分。

# 2019.6.20
节操碎了一地。

花瓶碎了一地。

水撒了一地。

*节操碎一地。

？碎了一地的节操。

do thou nothing there against

他喝酒喝醉了。

他吃饭吃饱了。

SUB V OBJ V ADV 

# 2019.6.22
the number of times it is selected

it is selected 4 times

---

同位语怎么解释呢？

# 2019.6.23
“这件事是过于复杂了”

注意到句子当中并没有出现“的”。这里“是”的作用很有趣。

# 2019.7.8

(i) a. Who/Whom is he talking to?

&emsp;&nbsp;b. To whom(?who) is he talking?

从这两句话当中可以看出一些挺有趣的东西。不谈书面语的问题，在口语当中我们发现(ia)当中不管用who还是whom都是很自然的，而在(ib)中使用whom明显读起来要比who来的流畅。如果我们认为who和whom唯一的区别就是who可以带上主格而不能带宾格，而whom可以带上宾格而不能带主格，那么显然没办法解释(ia)who版本的合法性。

这应该怎样解释呢？首先我们要想办法做出来一种概率化的MG，这点毫无疑问。

---

NLP和语言学并不是非常相关本来是一件可以预计到的事情。根据某个系统的功能仿制出一个对应的系统是一回事，弄清楚原来的系统是如何运作的是另外一回事。我们可以使用下面的比喻：
| 物理     | 语言                   |
| :------- | :--------------------- |
| 日常体验 | 语言产出               |
| 物理定律 | 语言机制               |
| 实验     | 句法实验、Minimal Pair |
| 直观理解 | 内省                   |
| 物理引擎 | NLP                    |
无论是物理还是语言学，我们面对的都是一个“不肯开口的证人”：我们不能直接从语言产出或是日常体验当中推断出其背后的机制，因此，需要通过各种各样人为安排的手段（比如实验——在这里最重要的并**不是**定量，而是**控制变量**，也就是保持其它因素不变，改变某个参量看看能够的出什么样的结论——显然Minimal Pair的思路就是这样。因此，使用Minimal Pair做可接受性测试本质上和正式的实验是一样的，如果它不可靠，那是因为样本数过少（只有作者自己）而导致无法对结果的质量做出评定而不是因为它不合科学原理），来“开黑箱”。

可是，对NLP和物理引擎的课题而言，“背后的机制”是无关紧要的——在这些议题中，最重要的是**做出一个能够产出差不多内容**的东西。物理引擎要求生成看起来像是现实中场景的画面，并且允许“合理的”物理交互，NLP要求能够解析和生成自然语言。具体使用的机制可以和自然界中实际存在的机制有非常大的差异，但这**不重要**。

由于这个差异，在具体研究中语言学和NLP的关注点、工具、进路会产生不同是很正常的。关于工具与进路的不同，以句法为例，乔姆斯基那一套句法理论基本上是为了解决“不同自然语言背后为什么有非常相似的结构”以及“人如何能够通过相对贫乏的刺激习得语言”。可是，在NLP中，这两个点根本不存在。对前者，自然语言的相似和不同是一个**已经给定的**、只需要**简单地重现**的特性，而不是一个**需要解释**的特性；对后者，NLP完全不需要担心输入的样本数，自然不存在刺激的贫乏。纯端对端的神经网络、与一个自动机关联的神经网络、某些传统统计学习方式都是可能的NLP工具，它们在生物学上是**不可行**的，因为幼儿并没有那么多的输入语料，但是它们在NLP中却是很好的想法。关于关注点的不同，我们会注意到在语言学上困难点在于句法习得，语义可以成为附着在句法上的一个组件（特别是在最简方案当中），可是NLP中句法学习根本就不是问题（那么大的数据量要有问题就有鬼了），而语义在这种情况下反而成了问题——我们并不知道神经网络学习到了什么样的句法，又怎么能够直接用句法得出对应的语义呢？

“解释”和“复现”的差异，还体现在对“过度生成”的不同态度之上。如果一个语言学理论允许一些在人类语言中不太可能出现的特征（比如说加括号，或是使用一个奇怪的逻辑体系，等等），或是能够允许一些实际上不合句法的句子，我们会说这个理论不够格，因为它预言了一切，等于什么也没有预言。（预言“特征F可以出现也可以不出现”只能说明这个理论具有很强的韧性，预言说“特征F必定出现”或者“特征F必定不出现”才使得理论能够产生与经验有关的的推论）相反，在NLP中，小小的过度生成却能够成为提升系统结构稳定性的手段——既然人们有时候可能生成不合法的句子，为什么不允许系统接受一部分不合法的句子呢？

# 2019.7.14
且比于我老彭
很有意思的一个句子

# 2019.7.15
Earliness Principle怎样形式化？

或者，什么样的句法操作适用Earliness Principle?

我们会注意到，这条规则的直接结果是“被移
位的成分只移动最小的距离”、“格只赋予给最近的元素”等等。

能够把Head Movement当成Phrase Movement来处理吗？

# 2019.7.18
a different setting from = a setting different from

# 2019.7.19
就理论框架而言，虽然最简方案下的研究不再要求手写产生式规则了（因此不再是传统的、也是常被统计学派批判的rule-based方法），但是很多研究又转向了另一种“手写规则”：写下大量来源不甚了然的原则、条件（比如说疑问句当中某某、某某语类可以带上某某特征、触发某某移动，等等），然后使用这一大堆原则来解释语言现象。如果这一类研究的目的是详尽地描写语法，那么传统语法也能做到（而且使用一种耗费更小的方法）；如果这一类研究的目的是为了探究语言背后的心理过程或者说“算法”，那么它们往往会被心理语言学的实验证伪；如果它们的目的是使用一种特定的形式化框架在representation的层面上描述并解释语言现象，那么RNNG之类的方法显然比手写原则更方便（但也更加难以给读者一种图像式的直观理解——所谓可解释性的问题）。

我认为有必要设计一种足够简单的（不要涉及太多句法运算）、计算友好的（方便概率化、可以用于构建一个基于转移的parser）最简方案的形式化，一方面可以将语言学中的一些想法代入到NLP当中，另一方面对语言学研究也有意义，比如说可以自动使用语料库以获得某些规律。

才发现一个很有意思的事实：在最简方案当中，绝对不会出现目的地为complement的移位。如果移位是head movement，那么移位的目的地一定是另一个head；如果移位是constituent movement，那么移位的目的地一定是specifier。

# 2019.7.21
我们知道在最简方案的框架下，一个XP并不一定要画出X-Bar的三级投射。例如，我们有

[<sub>VP</sub> `passengers` [<sub>V</sub> `complain`]]

但是CED原则区分specifier和complement……所以这里其实有问题，因为在上面的例子中你是看不出`passengers`是specifier还是complement的。

# 2019.7.26
如果要尽可能减少句法操作，就不应该引入probe-goal机制。此时，agreement可以使用简单的selection property来解释（主语被T选择，T又要选择和主语一致的VP，VP中的phi-feature与宾语一致）。

但是，这样的理论不是crash-proof的，因为如果错误的主语被插入了，derivation就没办法进行下去。

为了让理论crash-proof，我们需要使用feature valuation来代替一部分feature checking。这就需要引入probe-goal机制了。

# 2019.8.16
关于Agree运算的一些注记：

Merge操作的selectional性质（一个成分选择另一些成分）和Agree有一定的功能重叠。例如，我们考虑宾格的授予，有两种方法表述这个过程，第一种是动词选择的名词性表达式必须具有\[acc-case]特征，第二种是认为动词与宾语被Merge之后，动词与宾语发生Agree运算，宾语原本的\[u-case]特征被核查和求值，被计算为\[acc-case]然后删除，从而产生形态学上的格。很显然，在这里，Merge运算的功能和Agree发生了重叠。

这个功能重叠在一些情况下会使理论**表面上**看起来不一致，但这实际上是错觉，因为我们完全可以将标准的Merge运算改写成两个运算的叠加：
- 不具有选择能力的Merge，它任意地将两个成分结合在一起；
- Agree运算，它事实上起到了特征核查的作用（在selectional properties不一致的时候derivation就崩溃了）

以上我们构造了一个在其中Agree起到决定结构的主导作用的句法系统，现在我们反道而行之，考虑Agree是不是可以从句法系统中被消去。这正是Stabler的Minimalist Grammar的基本思路。

第一个问题是，Agree在什么样的情况下是必不可少的？我们知道如果在Merge运算中$\alpha$选择了$\beta$，那么$\alpha$和$\beta$之间的任何一种语法一致都可以被认为是selectional properties的结果（如果$\alpha$和$\beta$没有达成语法一致，比如数、性不一致，那么Merge操作就不能完成）因此，Agree运算的作用在于解释**非局域**的句法一致。

广义上说，格授予也可以看成一种句法一致，因为如果一个名词表达式带有某个特定的格，那么必定有某个有关的句法成分让它能够带有这个格。现在我们考虑以下某个分句当中主语的格：

[<sub>CP1</sub> [<sub>PRN1 [nom-case]</sub> I] want to do this].

He wants [<sub>CP2</sub> [<sub>PRN2 [acc-case]</sub> me] to do this].

在两个分句CP1和CP2当中的第一人称代词都在分句中承担主语角色，接受Agent的θ-role，两个分句的动词是一样的，而且宾语也是一样的。唯一不同的是Tense，而不同的Tense带来了PRN1和PRN2不同的格，因此看起来，主语的格是Tense授予的。

但是现在的问题是，Tense不可能和分句中的主语发生直接的selectional关系，因为主语在生成时是VP的specifier，而被T选择的是整个VP而不是单个的主语。因此，主格的授予虽然和T有关，但这个关系**并不是局域的**。

这时就可以使用Agree运算来完成格授予。我们可以认为T派出了一个特征探头来寻找一个被它c-command的成分，在这里就是主语，然后格授予发生，因为Agree运算可以直接做特征核查从而把[u-case]改写为[nom-case]。

不过，Agree运算也有可能带来一些理论上的混乱，因为我们如何确定哪个元素被特征探头选中呢？例如通常我们有

T [u-number, u-person]

那么T就需要和一个被它c-command的成分发生Agree运算。通常我们预期这么一个元素是名词，但是确实动词也可以带有这样的特征。那么就需要一些特定的机制来避免动词被拿来做Agree运算。类似的，Agree运算还有可能发生在T和宾语之间，这导致的结果就是T可能会带上错误的性或者数特征。还有一个例子如下：

[<sub>TP</sub> [<sub>T</sub> Φ] [<sub>VP</sub> [<sub>DP1</sub> [<sub>NP</sub> [<sub>DP2</sub> Jack's] employer]] [<sub>V'</sub> do sth.]]]

你怎么知道Agree运算应该发生在T和DP1之间而不是T和DP2之间呢？

一种可能的思路：要求Agree运算的两个对象（probe和goal）之间的距离要尽可能小。这可以看成是Earliness Principle的一个具体例子，因为一旦发现了一个可能的goal（也就是，带有未求值的、然而在probe上面却已经求值的某些特征的一个成分）立刻开始Agree操作也算是Earliness Principle的体现。

这样的思路解决了刚才提到的第二个和第三个问题。就第二个问题而言，主语在句法树上的位置比宾语高，所以主语首先被确定为可能的goal。既然主语已经提供了T需要的所有特征，Agree运算到这里就停止了。就第三个问题而言，DP1首先被当成可能的goal，它提供了T需要的所有特征，所以Agree运算不会深入DP1内部去寻找DP2，而是核查完DP1就停止了。

第一个问题的解决略微复杂。一种可能的方案是认为动词变位实际上并没有给动词数或者性特征。相反，变位之后的动词具有selectional property，它**选择了**具有相对应数或者性特征的名词。

这种思路的不完善之处首先在于句法距离难以良定义，其次在于有可能会出现句法距离相同的情况。此外，这个算法在计算上非常不经济，难以想象大脑会采用这样的方案。（实际上，演化上“一种涉及某个距离量度的基本机制”是非常少见的）最后，引入一个独立的、用于评定句法距离的运算引入了**句法树之外的因素**，因此恐怕违反了最简的原则。

还有一个技术细节：当我们需要核查一个**短语**的某个特征（比如说格）的时候，这个短语的head的相应特征也需要对应的核查！这样一个机制实际上要求一个**特征链**，也就是说，如果X具有某个未核查特征uF，那么围绕它形成的短语XP也应该具有uF（这很自然，因为Merge运算是这么规定的），并且**XP和X的F特征在被核查之后必须保持一致**。因此，当XP的F特征被核查为aF的时候，X的F特征被**连锁地**核查为了aF。我们又一次发现一个**句法树之外的**约束：它引入了额外的复杂度。（*但是，引入额外的复杂度真的有百害而无一利吗？如果做一下内省，以一个英语第二语言者的感觉，特征链或许真的是存在的。一种可能的思路：不显式地引入特征链，但是允许在derivation当中使用未核查特征，不过这样一来就需要在derivation结束的时候手工引入一个过滤器，筛选掉所有不满足一致性原则的句法树。这个过滤器恐怕并不十分自然，因为其它过滤器都有音系或是语义上的动机，但是语法一致是一个纯粹语法的东西而并不能很大地影响语义。*）

如果我们打算完全消去Agree这个运算（或者说把Agree的作用范围保持在局域，从而使它能够合并进Merge当中），应该怎么做呢？

There are a few kids crying.

There is a kid crying.

---

虽然我向来比较赞同形式学派，或者准确地说，正统乔派的意见，但我还是觉得使用非常复杂的形式系统来尝试描写语言的**每一个特征**会是一个足够好的想法。

比较理想的情况是这样：我们能够提出一个比较简单的句法框架，它具有一系列（不很多的）句法运算，然后能够以一种比较清晰的方式生成非常像自然语言的句子，然后它能够很好的和概率模型结合在一起，这样的框架就能够同时具有乔派语法的清晰和深度学习nlp的强大能力。

例如，neural parser中使用一个神经网络操纵一个bottom-up 的transition-based CFG parser就是很好的想法：后者就是一个非常简单的、但是表现力很好的模型。

我认为最简方案的形式化也应该能够起到相同——如果不说更好——的作用，但可惜看起来CFG已经够用了，所以似乎没几个人在找新语法……而且很明显，往Minimalist Grammar里面塞入越来越多的运算肯定不是正确的方法。

应该如何挑选基本的句法运算呢？Merge肯定少不了。在CFG的模型中这就是唯一的运算了，但是由于最简方案当中还会讨论复制、删除、移位，我们肯定还需要一些运算。

还有一种思路就是将先前提到的所有运算打包成一个“巨型”Merge运算。

---

关于adjunct。其实这个问题我也有疑惑很久了。主要原因在于，在最简方案当中所有的树都是严格二叉而且向心的，并且树的生成严格地由selectional properties确定，但是在adjunction当中并不是这样的。adjunct是可选的，所以adjunct被加到树上的过程不能简单地视为selection properties控制的结果；可是另一方面，我们又知道adjunction不是完全任意的。此外，adjunct似乎并没有严格的顺序，这也从侧面表明了不能使用selection property来描述adjunction。没有严格的顺序还意味着，含有adjunct的短语没有必要写成严格的二叉树，这和最简方案的基本假设相冲突，因此可能威胁到整个理论框架的完整性。

# 2019.8.24
Something about head movement:

In "classical" phrasal movement, the movement trigger or licensor α is first merged into the tree, and then attracts the licensee or the mover to move to a higher position of itself. The derivation can be illustrated as follows:

[...β...] → [<sub>α</sub> α [...β...]] → [<sub>α</sub> β [<sub>α</sub> α [...<del>β</del>...]]]

For head movement however, the trigger is first merged with the mover, and then merged into the tree, with the correspondent derivation illustrated as follows:

[...β...] → [<sub>α</sub> α β] [...<del>β</del>...] → [<sub>α</sub> [<sub>α</sub> α β] [...<del>β</del>...]]

In phrasal movement at each time point there is only one tree in the workspace, while in head movement, the first step results in a two-tree workspace, namely {[<sub>α</sub> α β], [...<del>β</del>...]}.

Note that workspaces with multiple trees are not rare even without head movement. Consider embedded clauses as an example: the embedded clause is first built, and then it may be merged into a half-built tree, possibly resulting in a two-tree workspace. 

What makes thing odd in head movement is it involves information exchange between two trees before they are merged - β is deleted from the first tree and merged into the second tree. If we regard movement as a Copy-Delete-Merge operation and hence an Agree-Merge operation (given that Copy invokes Agree for copying relevant features), we would expect no movement from one tree to another is possible, since Agree can only be licensed by a probe and applied to a licensee c-commanded by the licensor, implying both the licensor and the licensee to be on the same tree.

One possible solution is to suppose Agree can happen *before* Merge, and also *after* Merge. In English, *after* a T is merged into the tree, it probes for a nominal subject because it bears [EPP] feature and must take a nominal specifier; but *before* a head movement licensor is merged into the tree, Agree has already taken place and found the mover.

The cost for such an analysis is we can no longer claim that Agree licensor always c-commands its licensee. Now we have the following comparison:
| Movement Type    | C-commanding Relation between Licensor and Licensee | Whether the Mover is Extractable after Movement |
| :--------------- | :-------------------------------------------------- | :---------------------------------------------- |
| Phrasal Movement | Yes                                                 | Yes                                             |
| Head Movement    | No                                                  | No                                              |

For head movement, after movement the mover and the licensor are combined into a syncretized head, from which nothing can be extracted for further movement.

# 2019.8.25
相反，他疯了。

*他相反疯了。

Rather, he is mad.

---

一个猜测：作通格语言当中，for me<sub>[acc-case]</sub> to do这种类型的小句应该比较少出现

# 2019.8.27
What hope of finding any survivors could there be?

What hope could there be of finding any survivors?

What is tricky here is *what hope* is not a constituent. How could a fronted constituent *what hope of finding any survivors* receive a partial spellout?

Borer-Chomsky conjecture: All parameters of variation are attributable to differences in the features of particular items (e.g. the functional heads) in the Lexicon.

Question: what about phonological constraints (for example, certain type of replication is not allowed phonologically) ?

I don't think economy principle is the best way to deal with syntax. Neural networks are more prominent.